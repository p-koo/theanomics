{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with tSNE visualizations on MNIST Dataset\n",
    "\n",
    "In this notebook, we will explore the t-distributed Stochastic Neighbor Embedding (tSNE) as a visualization tool using the MNIST dataset. tSNE is a powerful dimensionality reduction technique that focuses on local pairwise similarities between the input features. It has been shown to outperform other techniques, including PCA, ISOMAP, locally linear embedding, etc (see van der Maaten and Hinton, JMLR, 2008). The visualizations resulting from tSNE dimensionality reduction is ridiculously cool and has been insightful for understanding the structure of the data. It certainly warrants an exploration.\n",
    "\n",
    "tSNE works by minimizing the Kullback-Leibler divergence between the high dimensional dataset $p_{i,j}$ and the distribution of the low dimensional data projection $q_{i,j}$. This cost allows tSNE to preserve local similarity structure of the high dimensional data on the lower dimensional projection. The student's t-distribution was an important addition as it has a heavy tail to improve how it handles dissimilar points (avoiding the crowding problem).\n",
    "\n",
    "In this notebook, we will explore tSNE on the MNIST dataset with 3 different representations, directly on the raw pixels, via convolutional neural networks, and a convolutional autoencoder network.  Let's get started!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os, sys, gzip\n",
    "import cPickle as pickle\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(247)   # for reproducibility\n",
    "\n",
    "# stochastic neighbor embedding (to install --> pip install tsne)\n",
    "from tsne import bh_sne\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "from scipy.misc import imresize\n",
    "\n",
    "from lasagne import layers, nonlinearities, updates, objectives, init \n",
    "from lasagne.layers import get_output, get_output_shape, get_all_params\n",
    "import theano.tensor as T\n",
    "import theano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the MNIST dataset \n",
    "\n",
    "Note, if you don't have the MNIST dataset, you can download it via:\n",
    "\n",
    "!wget http://deeplearning.net/data/mnist/mnist.pkl.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load mnist data\n",
    "fname = '/home/peter/Data/mnist/mnist.pkl.gz'\n",
    "f = gzip.open(fname, 'rb')\n",
    "train_set, valid_set, test_set = pickle.load(f)\n",
    "f.close()\n",
    "X_train, y_train = train_set\n",
    "X_valid, y_valid = valid_set\n",
    "X_test, y_test = test_set\n",
    "\n",
    "num_labels = 10\n",
    "num_train = len(y_train)\n",
    "num_valid = len(y_valid)\n",
    "num_test = len(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tSNE on MNIST pixels\n",
    "\n",
    "Let's first directly run tSNE with the Barnes-Hut approximation directly on the pixels of the images from the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# perform t-SNE embedding\n",
    "vis_data = bh_sne(X_train.astype(float))\n",
    "vis_x = vis_pixel[:, 0]\n",
    "vis_y = vis_pixel[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tSNE is completely unsupervised. But it is more visually appealing as well as informative if we give the projection coordinates a color based on the labels.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the result\n",
    "plt.figure(figsize = (10,10))\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(50, 50, forward=True)\n",
    "plt.scatter(vis_x, vis_y, c=y_train[:len(X_train)+1], cmap=plt.cm.get_cmap(\"jet\", num_labels),  edgecolor = 'none')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a cool visualization of the data, but we don't know what the structure of the data look like within each class. Evidently, we can learn more about the structure of the data if we embed the acutal images onto each projection coordinate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# setup a colormap\n",
    "colormap = cm.gist_rainbow(np.linspace(0, 255, num_labels).astype(int))\n",
    "\n",
    "# rescale embedding coordinates within [0, 1]\n",
    "vis_pixel -= np.min(vis_pixel)\n",
    "vis_pixel /= np.max(vis_pixel)\n",
    "\n",
    "# create canvas to embed images\n",
    "canvas_size = 5000\n",
    "canvas = np.zeros((canvas_size, canvas_size, 3)).astype(np.uint8)\n",
    "\n",
    "# set image_size, will downsample if smaller than 28\n",
    "image_size = 28\n",
    "scale = canvas_size-image_size\n",
    "\n",
    "# embed images onto canvas\n",
    "num_samples = 50000\n",
    "for i in range(num_samples):\n",
    "    pos = np.ceil(vis_pixel[i,:]*scale).astype(int)\n",
    "    downsample_img =imresize(np.reshape(X_train[i], (28,28)),(image_size, image_size))\n",
    "    for j in range(3):\n",
    "        canvas[pos[0]:pos[0]+image_size,pos[1]:pos[1]+image_size,j] = downsample_img*colormap[y_train[i]][j]\n",
    "            \n",
    "    \n",
    "plt.figure(figsize = (10,10))\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 15, forward=True)\n",
    "plt.imshow(np.invert(canvas), cmap='gray', interpolation='nearest')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is really cool. You can visualize the manifold within each class.  The ones lean one way on the left and lean to the other side on the right.  It shows you the range of representations that tSNE was able to group together.  Interestingly, classes that are similar, like 4 and 9 are close together, while 3 and 8 are also closer together, which makes sense.  \n",
    "\n",
    "But can we do better if we use more informative features? Let's use deep convolutional neural networks to learn new features from the data and then use these as inputs to tSNE. Then we can do a side by side comparison whether deep learning representations are indeed more informative compared to raw pixel values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's setup a simple convolutional neural network with 3 convolutional layers followed by a dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build model\n",
    "input_var = T.tensor4('input')\n",
    "target_var = T.ivector('target')\n",
    "\n",
    "l_in = layers.InputLayer(shape=(None,1,28,28), input_var=input_var, name='input')\n",
    "\n",
    "l_conv1 = layers.Conv2DLayer(l_in, num_filters=50, filter_size=(5,5), W=init.GlorotUniform(), \n",
    "                                   b=init.Constant(0.05), nonlinearity=None, pad='same')\n",
    "#l_norm1 = layers.BatchNormLayer(l_conv1)\n",
    "l_nonlin1 = layers.NonlinearityLayer(l_conv1, nonlinearity=nonlinearities.rectify)\n",
    "l_pool1 = layers.MaxPool2DLayer(l_nonlin1, pool_size=(2,2))\n",
    "\n",
    "l_conv2 = layers.Conv2DLayer(l_pool1, num_filters=35, filter_size=(3,3), W=init.GlorotUniform(), \n",
    "                                   b=init.Constant(0.05), nonlinearity=None, pad='same')\n",
    "#l_norm2 = layers.BatchNormLayer(l_conv2)\n",
    "l_nonlin2 = layers.NonlinearityLayer(l_conv2, nonlinearity=nonlinearities.rectify)\n",
    "l_pool2 = layers.MaxPool2DLayer(l_nonlin2, pool_size=(2,2))\n",
    "\n",
    "l_conv3 = layers.Conv2DLayer(l_pool2, num_filters=20, filter_size=(3,3), W=init.GlorotUniform(), \n",
    "                                   b=init.Constant(0.05), nonlinearity=None, pad='same')\n",
    "#l_norm3 = layers.BatchNormLayer(l_conv3)\n",
    "l_nonlin3 = layers.NonlinearityLayer(l_conv3, nonlinearity=nonlinearities.rectify)\n",
    "\n",
    "l_dense4 = layers.DenseLayer(l_nonlin3,  num_units=200, W=init.GlorotUniform(), \n",
    "                             b=init.Constant(0.0), nonlinearity=None)\n",
    "l_norm4 = layers.BatchNormLayer(l_dense4)\n",
    "l_nonlin4 = layers.NonlinearityLayer(l_norm4, nonlinearity=nonlinearities.rectify)\n",
    "\n",
    "l_out = layers.DenseLayer(l_nonlin4,  num_units=10, W=init.GlorotUniform(), \n",
    "                             b=init.Constant(0.0), nonlinearity=nonlinearities.softmax)\n",
    "\n",
    "# setup loss for training and validation\n",
    "prediction = get_output(l_out)\n",
    "train_loss = objectives.categorical_crossentropy(prediction, target_var)\n",
    "train_loss = train_loss.mean()\n",
    "\n",
    "valid_prediction = get_output(l_out, deterministic=True)\n",
    "valid_loss = objectives.categorical_crossentropy(valid_prediction, target_var)\n",
    "valid_loss = valid_loss.mean()\n",
    "\n",
    "valid_acc = T.mean(T.eq(T.argmax(valid_prediction, axis=1), target_var),\n",
    "                  dtype=theano.config.floatX)\n",
    "\n",
    "# setup adam optimizer\n",
    "params = get_all_params(l_out, trainable=True)\n",
    "update_op = updates.adam(train_loss, params)\n",
    "\n",
    "train_fn = theano.function([input_var, target_var], train_loss, updates=update_op, allow_input_downcast=True)\n",
    "val_fn = theano.function([input_var, target_var], [valid_loss, valid_acc], allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model with mini-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reshape from (50000, 784) to 4D tensor (50000, 1, 28, 28)\n",
    "X_train = np.reshape(X_train, (-1, 1, 28, 28))\n",
    "X_valid = np.reshape(X_valid, (-1, 1, 28, 28))\n",
    "\n",
    "# setup mini-batch generator\n",
    "def batch_generator(X, y, N):\n",
    "    while True:\n",
    "        idx = np.random.choice(len(y), N)\n",
    "        yield X[idx].astype('float32'), y[idx].astype('float32')\n",
    "\n",
    "batch_size = 512\n",
    "num_train_batches = len(X_train) // batch_size\n",
    "train_batches = batch_generator(X_train, y_train, batch_size)\n",
    "\n",
    "num_valid_batches = len(X_valid) // batch_size\n",
    "valid_batches = batch_generator(X_valid, y_valid, batch_size)\n",
    "\n",
    "# train network (for short time -- MNIST is quick to train)\n",
    "n_epochs = 12\n",
    "for e in range(n_epochs):\n",
    "    ave_loss = 0\n",
    "    for index in range(num_train_batches):\n",
    "        X_batch, y_batch = next(train_batches)\n",
    "        train_loss = train_fn(X_batch, y_batch)\n",
    "        ave_loss += train_loss\n",
    "    print(\"train: %f\" % float(ave_loss/num_train_batches))\n",
    "\n",
    "    ave_loss = 0\n",
    "    ave_acc = 0\n",
    "    for index in range(num_valid_batches):\n",
    "        X_batch, y_batch = next(valid_batches)\n",
    "        valid_loss, valid_acc = val_fn(X_batch, y_batch)\n",
    "        ave_loss += valid_loss\n",
    "        ave_acc += valid_acc\n",
    "    print(\"valid: %f\" % float(ave_loss/num_valid_batches))\n",
    "    print(\"accuracy: %f\" % float(ave_acc/num_valid_batches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the weights of the first convolutional layer to see if it's learned diverse features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_conv_weights(layer, figsize=(6, 6)):\n",
    "    W =  np.squeeze(layer.W.get_value())\n",
    "    shape = W.shape\n",
    "    nrows = np.ceil(np.sqrt(shape[0])).astype(int)\n",
    "    ncols = nrows\n",
    "\n",
    "    figs, axes = plt.subplots(nrows, ncols, figsize=figsize,frameon=False)\n",
    "\n",
    "    for ax in axes.flatten():\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.axis('off')\n",
    "\n",
    "    for i, ax in enumerate(axes.ravel()):\n",
    "        if i >= shape[0]:\n",
    "            break\n",
    "        im = ax.imshow(W[i], cmap='gray', interpolation='nearest')\n",
    "    return figs, axes\n",
    "\n",
    "figs, axes = plot_conv_weights(l_conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filters look fine.  I don't see too many redundant features, so let's move on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shape = get_output_shape(l_nonlin3)\n",
    "encode = theano.function([input_var], layers.get_output(l_nonlin3), allow_input_downcast=True)\n",
    "\n",
    "batch_size = 512\n",
    "num_batches = len(X_train) // batch_size\n",
    "val = np.zeros(shape[1]*shape[2]*shape[3])\n",
    "label = np.zeros(1)\n",
    "index = np.zeros(1)\n",
    "for i in range(num_batches):\n",
    "    index = np.vstack([index, np.reshape(np.arange(i*batch_size,(i+1)*batch_size), (batch_size,-1) )])\n",
    "    code = encode(X_train[i*batch_size:(i+1)*batch_size])\n",
    "    val = np.vstack([val, np.reshape(code, (batch_size,-1))])\n",
    "    label = np.vstack([label, np.reshape(y_train[i*batch_size:(i+1)*batch_size], (batch_size,-1))])\n",
    "label = np.squeeze(label).astype(int)\n",
    "index = np.squeeze(index).astype(int)\n",
    "X2 = np.reshape(val, (val.shape[0],-1))\n",
    "X2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform tSNE using the Barnes-Hut approximation (takes about 10 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# perform t-SNE embedding\n",
    "vis_cnn = bh_sne(X2.astype(float))\n",
    "vis_x = vis_cnn[:, 0]\n",
    "vis_y = vis_cnn[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first visualize with datapoints on the 2D projection surface with colors given by their respective labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the result\n",
    "plt.figure(figsize = (10,10))\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(30, 30, forward=True)\n",
    "plt.scatter(vis_x, vis_y, c=label[0:len(X2)+1], cmap=plt.cm.get_cmap(\"jet\", 10),  edgecolor = 'none')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's embed the acutal images onto each projection coordinate to see if we can notice anything neat about the structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# setup a colormap\n",
    "colormap = cm.gist_rainbow(np.linspace(0, 255, num_labels).astype(int))\n",
    "\n",
    "# rescale embedding coordinates within [0, 1]\n",
    "vis_cnn -= np.min(vis_cnn)\n",
    "vis_cnn /= np.max(vis_cnn)\n",
    "\n",
    "# create canvas to embed images\n",
    "canvas_size = 5000\n",
    "canvas = np.zeros((canvas_size, canvas_size, 3)).astype(np.uint8)\n",
    "\n",
    "# set image_size, will downsample if smaller than 28\n",
    "image_size = 28\n",
    "scale = canvas_size-image_size\n",
    "\n",
    "# embed images onto canvas\n",
    "num_samples = len(vis_cnn)\n",
    "for i in range(num_samples):\n",
    "    pos = np.ceil(vis_cnn[i,:]*scale).astype(int)\n",
    "    downsample_img =imresize(np.reshape(X_train[index[i]], (28,28)),(image_size, image_size))\n",
    "    for j in range(3):\n",
    "        canvas[pos[0]:pos[0]+image_size,pos[1]:pos[1]+image_size,j] = downsample_img*colormap[label[i]][j]\n",
    "            \n",
    "    \n",
    "plt.figure(figsize = (10,10))\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 15, forward=True)\n",
    "plt.imshow(np.invert(canvas), cmap='gray', interpolation='nearest')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting thing to try is to see what tSNE learns after each layer. But some of the layers have too many parameters, so tSNE will take too long for this notebook.\n",
    "\n",
    "## Convolutional autoencoder visualization\n",
    "Next, let's try a convolutional autoencoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build model\n",
    "input_var = T.tensor4('input')\n",
    "\n",
    "l_in = layers.InputLayer(shape=(None,1,28,28), input_var=input_var, name='input')\n",
    "\n",
    "l_conv1 = layers.Conv2DLayer(l_in, num_filters=15, filter_size=(5,5), W=init.GlorotUniform(), \n",
    "                             nonlinearity=None, b=None, pad='same')\n",
    "l_bias1 = layers.BiasLayer(l_conv1, b=init.Constant(0.05))\n",
    "l_nonlin1 = layers.NonlinearityLayer(l_bias1, nonlinearity=nonlinearities.rectify)\n",
    "l_pool1 = layers.MaxPool2DLayer(l_nonlin1, pool_size=(2,2))\n",
    "\n",
    "l_conv3 = layers.Conv2DLayer(l_pool1, num_filters=30, filter_size=(5,5), W=init.GlorotUniform(), \n",
    "                             nonlinearity=None, b=None, pad='same')\n",
    "l_bias3 = layers.BiasLayer(l_conv3, b=init.Constant(0.05))\n",
    "l_nonlin3 = layers.NonlinearityLayer(l_bias3, nonlinearity=nonlinearities.rectify)\n",
    "l_pool3 = layers.MaxPool2DLayer(l_nonlin3, pool_size=(2,2))\n",
    "\n",
    "l_dense4 = layers.DenseLayer(l_pool3,  num_units=200, W=init.GlorotUniform(), \n",
    "                             nonlinearity=None, b=None)\n",
    "l_bias4 = layers.BiasLayer(l_dense4, b=init.Constant(0.0))\n",
    "l_nonlin4 = layers.NonlinearityLayer(l_bias4, nonlinearity=nonlinearities.rectify)\n",
    "                                     \n",
    "l_dense5 = layers.DenseLayer(l_nonlin4,  num_units=30, W=init.GlorotUniform(), \n",
    "                             nonlinearity=None, b=None)\n",
    "l_bias5 = layers.BiasLayer(l_dense5, b=init.Constant(0.0))\n",
    "l_nonlin5 = layers.NonlinearityLayer(l_bias5, nonlinearity=nonlinearities.sigmoid)\n",
    "\n",
    "l_enc = layers.NonlinearityLayer(l_nonlin5, nonlinearity=None)\n",
    "\n",
    "l_dec9 = layers.InverseLayer(l_enc, l_dense5)\n",
    "l_bias9 = layers.BiasLayer(l_dec9, init.Constant(0.0))\n",
    "l_nonlin9 = layers.NonlinearityLayer(l_bias9, nonlinearity=nonlinearities.sigmoid)\n",
    "\n",
    "l_dec8 = layers.InverseLayer(l_nonlin9, l_dense4)\n",
    "l_bias8 = layers.BiasLayer(l_dec8, init.Constant(0.0))\n",
    "l_nonlin8 = layers.NonlinearityLayer(l_bias8, nonlinearity=nonlinearities.rectify)\n",
    "\n",
    "l_depool6 = layers.InverseLayer(l_nonlin8, l_pool3)\n",
    "l_dec6 = layers.InverseLayer(l_depool6, l_conv3)\n",
    "l_bias6 = layers.BiasLayer(l_dec6, init.Constant(0.05))\n",
    "l_nonlin6 = layers.NonlinearityLayer(l_bias6, nonlinearity=nonlinearities.rectify)\n",
    "\n",
    "l_depool7 = layers.InverseLayer(l_nonlin6, l_pool1)\n",
    "l_deconv7 = layers.InverseLayer(l_depool7, l_conv1)\n",
    "l_bias7 = layers.BiasLayer(l_deconv7, init.Constant(0.05))\n",
    "l_out = layers.NonlinearityLayer(l_bias7, nonlinearity=nonlinearities.rectify)\n",
    "\n",
    "# train and validation loss\n",
    "prediction = get_output(l_out)\n",
    "train_loss = objectives.squared_error(prediction, input_var)\n",
    "train_loss = train_loss.mean()\n",
    "\n",
    "valid_prediction = get_output(l_out, deterministic=True)\n",
    "valid_loss = objectives.squared_error(valid_prediction, input_var)\n",
    "valid_loss = valid_loss.mean()\n",
    "\n",
    "# optimizer\n",
    "params = get_all_params(l_out, trainable=True)\n",
    "update_op = updates.adam(train_loss, params)\n",
    "\n",
    "# theano function\n",
    "train_function = theano.function([input_var], train_loss, updates=update_op)\n",
    "valid_function = theano.function([input_var], valid_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "num_train_batches = len(X_train) // batch_size\n",
    "train_batches = batch_generator(X_train, y_train, batch_size)\n",
    "\n",
    "num_valid_batches = len(X_valid) // batch_size\n",
    "valid_batches = batch_generator(X_valid, y_valid, batch_size)\n",
    "\n",
    "# train network (for short time -- MNIST is quick to train)\n",
    "n_epochs = 10\n",
    "for e in range(n_epochs):\n",
    "    ave_loss = 0\n",
    "    for index in range(num_train_batches):\n",
    "        X_batch, y_batch = next(train_batches)\n",
    "        train_loss = train_function(X_batch)\n",
    "        ave_loss += train_loss\n",
    "    print(\"train: %f\" % float(ave_loss/num_train_batches))\n",
    "\n",
    "    ave_loss = 0\n",
    "    for index in range(num_valid_batches):\n",
    "        X_batch, y_batch = next(valid_batches)\n",
    "        valid_loss = valid_function(X_batch)\n",
    "        ave_loss += valid_loss\n",
    "    print(\"valid: %f\" % float(ave_loss/num_valid_batches))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the original images and the reconstructed images to see how well we did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full = theano.function([input_var], layers.get_output(l_out), allow_input_downcast=True)\n",
    "encode = theano.function([input_var], layers.get_output(l_enc), allow_input_downcast=True)\n",
    "\n",
    "target_var = T.dmatrix('codes')\n",
    "out_expr = layers.get_output(l_out, {l_enc:target_var})\n",
    "fn = theano.function([target_var, l_in.input_var], out_expr, allow_input_downcast=True)\n",
    "\n",
    "width = 10\n",
    "height = 10\n",
    "offset = 4000\n",
    "\n",
    "index = range(offset, offset+width*height)\n",
    "\n",
    "plt.figure(figsize = (height,width))\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(100, 100, forward=True)\n",
    "gs = mpl.gridspec.GridSpec(height, width)\n",
    "gs.update(wspace=0.1, hspace=0.1, left=0.1, right=0.2, bottom=0.1, top=0.2) \n",
    "for i in range(width*height):\n",
    "    orig_image = np.expand_dims(X_train[index[i]], axis=0) \n",
    "    plt.subplot(gs[i])\n",
    "    plt.imshow(np.squeeze(orig_image), cmap='gray', interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize = (height,width))\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(100, 100, forward=True)\n",
    "gs = mpl.gridspec.GridSpec(height, width)\n",
    "gs.update(wspace=0.1, hspace=0.1, left=0.1, right=0.2, bottom=0.1, top=0.2) \n",
    "for i in range(width*height):\n",
    "    orig_image = np.expand_dims(X_train[index[i]], axis=0) \n",
    "    code = encode(orig_image)\n",
    "    encode_image = fn(code, orig_image)\n",
    "    plt.subplot(gs[i])\n",
    "    plt.imshow(np.squeeze(encode_image), cmap='gray', interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_conv_weights(layer, figsize=(6, 6)):\n",
    "    \"\"\"nolearn's plot the weights of a specific layer\"\"\"\n",
    "\n",
    "    W =  np.squeeze(layer.W.get_value())\n",
    "    shape = W.shape\n",
    "    nrows = np.ceil(np.sqrt(shape[0])).astype(int)\n",
    "    ncols = nrows\n",
    "\n",
    "    figs, axes = plt.subplots(nrows, ncols, figsize=figsize,frameon=False)\n",
    "\n",
    "    for ax in axes.flatten():\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.axis('off')\n",
    "\n",
    "    for i, ax in enumerate(axes.ravel()):\n",
    "        if i >= shape[0]:\n",
    "            break\n",
    "        im = ax.imshow(W[i], cmap='gray', interpolation='nearest')\n",
    "\n",
    "    return figs, axes\n",
    "\n",
    "\n",
    "figs, axes = plot_conv_weights(l_conv1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shape = get_output_shape(l_pool3)\n",
    "encode = theano.function([input_var], layers.get_output(l_pool3), allow_input_downcast=True)\n",
    "\n",
    "batch_size = 512\n",
    "num_batches = len(X_train) // batch_size\n",
    "val = np.zeros(shape[1]*shape[2]*shape[3])\n",
    "label = np.zeros(1)\n",
    "index = np.zeros(1)\n",
    "for i in range(num_batches):\n",
    "    index = np.vstack([index, np.reshape(np.arange(i*batch_size,(i+1)*batch_size), (batch_size,-1) )])\n",
    "    code = encode(X_train[i*batch_size:(i+1)*batch_size])\n",
    "    val = np.vstack([val, np.reshape(code, (batch_size,-1))])\n",
    "    label = np.vstack([label, np.reshape(y_train[i*batch_size:(i+1)*batch_size], (batch_size,-1))])\n",
    "val.shape\n",
    "label = np.squeeze(label)\n",
    "index = np.squeeze(index)\n",
    "X2 = np.reshape(val, (val.shape[0],-1))\n",
    "\n",
    "\n",
    "vis_cae = bh_sne(X2.astype(float))\n",
    "vis_x = vis_cae[:, 0]\n",
    "vis_y = vis_cae[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the result\n",
    "plt.figure(figsize = (10,10))\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(50, 50, forward=True)\n",
    "plt.scatter(vis_x, vis_y, c=label[0:len(X2)+1], cmap=plt.cm.get_cmap(\"jet\", 10),  edgecolor = 'none')\n",
    "plt.axis('off')\n",
    "\n",
    "# setup a colormap\n",
    "colormap = cm.gist_rainbow(np.linspace(0, 255, num_labels).astype(int))\n",
    "\n",
    "# rescale embedding coordinates within [0, 1]\n",
    "vis_cae -= np.min(vis_cae)\n",
    "vis_cae /= np.max(vis_cae)\n",
    "\n",
    "# create canvas to embed images\n",
    "canvas_size = 2000\n",
    "canvas = np.zeros((canvas_size, canvas_size, 3)).astype(np.uint8)\n",
    "\n",
    "# set image_size, will downsample if smaller than 28\n",
    "image_size = 28\n",
    "scale = canvas_size-image_size\n",
    "\n",
    "# embed images onto canvas\n",
    "num_samples = len(vis_cae)\n",
    "for i in range(num_samples):\n",
    "    pos = np.ceil(vis_cae[i,:]*scale).astype(int)\n",
    "    downsample_img =imresize(np.reshape(X_train[index[i]], (28,28)),(image_size, image_size))\n",
    "    for j in range(3):\n",
    "        canvas[pos[0]:pos[0]+image_size,pos[1]:pos[1]+image_size,j] = downsample_img*colormap[label[i]][j]\n",
    "            \n",
    "    \n",
    "plt.figure(figsize = (10,10))\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 15, forward=True)\n",
    "plt.imshow(np.invert(canvas), cmap='gray', interpolation='nearest')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
