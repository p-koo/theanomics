{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sys.path.append('/Users/juliankimura/Desktop/deepomics')\n",
    "import deepomics.neuralnetwork as nn\n",
    "from deepomics import learn, utils\n",
    "from models import vae_model\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from lasagne import layers\n",
    "\n",
    "np.random.seed(247)   # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_dataset(data_path):\n",
    "    import gzip\n",
    "    def load_mnist_images(filename):\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "        data = data.reshape(-1, 28*28)\n",
    "        return data / np.float32(256)\n",
    "\n",
    "    def load_mnist_labels(filename):\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "        return data\n",
    "\n",
    "    # We can now download and read the training and test set images and labels.\n",
    "    X_train = load_mnist_images(os.path.join(data_path,'train-images-idx3-ubyte.gz'))\n",
    "    y_train = load_mnist_labels(os.path.join(data_path,'train-labels-idx1-ubyte.gz'))\n",
    "    X_test = load_mnist_images(os.path.join(data_path,'t10k-images-idx3-ubyte.gz'))\n",
    "    y_test = load_mnist_labels(os.path.join(data_path,'t10k-labels-idx1-ubyte.gz'))\n",
    "\n",
    "    # We reserve the last 10000 training examples for validation.\n",
    "    X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
    "    y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
    "\n",
    "    # We just return all the arrays in order, as expected in main().\n",
    "    # (It doesn't matter how we do this as long as we can read them again.)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "data_path = '/Users/juliankimura/Desktop/data/MNIST'\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shape = (None, X_train.shape[1])\n",
    "network, placeholders, optimization = vae_model.model(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "Network architecture:\n",
      "----------------------------------------------------------------------------\n",
      "layer1: \n",
      "<lasagne.layers.input.InputLayer object at 0x11a68e5c0>\n",
      "shape:(None, 784)\n",
      "layer2: \n",
      "<lasagne.layers.dense.DenseLayer object at 0x11a68e630>\n",
      "shape:(None, 200)\n",
      "parameters: W\n",
      "layer3: \n",
      "<lasagne.layers.normalization.BatchNormLayer object at 0x11a68e5f8>\n",
      "shape:(None, 200)\n",
      "parameters: beta, gamma, mean, inv_std\n",
      "layer4: \n",
      "<lasagne.layers.special.NonlinearityLayer object at 0x11a69e160>\n",
      "shape:(None, 200)\n",
      "layer5: \n",
      "<lasagne.layers.noise.DropoutLayer object at 0x11a69e6d8>\n",
      "shape:(None, 200)\n",
      "layer6: \n",
      "<lasagne.layers.dense.DenseLayer object at 0x11a69e8d0>\n",
      "shape:(None, 100)\n",
      "parameters: W\n",
      "layer7: \n",
      "<lasagne.layers.normalization.BatchNormLayer object at 0x11a69e860>\n",
      "shape:(None, 100)\n",
      "parameters: beta, gamma, mean, inv_std\n",
      "layer8: \n",
      "<lasagne.layers.special.NonlinearityLayer object at 0x11a69ee10>\n",
      "shape:(None, 100)\n",
      "layer9: \n",
      "<lasagne.layers.noise.DropoutLayer object at 0x11a69efd0>\n",
      "shape:(None, 100)\n",
      "layer10: \n",
      "<lasagne.layers.dense.DenseLayer object at 0x12c901080>\n",
      "shape:(None, 4)\n",
      "parameters: W, b\n",
      "layer11: \n",
      "<lasagne.layers.dense.DenseLayer object at 0x12c901240>\n",
      "shape:(None, 4)\n",
      "parameters: W, b\n",
      "layer12: \n",
      "<deepomics.build_network.VariationalSampleLayer object at 0x12c901588>\n",
      "shape:(None, 4)\n",
      "layer13: \n",
      "<lasagne.layers.special.NonlinearityLayer object at 0x12c901780>\n",
      "shape:(None, 4)\n",
      "layer14: \n",
      "<lasagne.layers.dense.DenseLayer object at 0x12c9017f0>\n",
      "shape:(None, 100)\n",
      "parameters: W\n",
      "layer15: \n",
      "<lasagne.layers.special.BiasLayer object at 0x12c901828>\n",
      "shape:(None, 100)\n",
      "parameters: b\n",
      "layer16: \n",
      "<lasagne.layers.special.NonlinearityLayer object at 0x12c9019e8>\n",
      "shape:(None, 100)\n",
      "layer17: \n",
      "<lasagne.layers.dense.DenseLayer object at 0x12c901c18>\n",
      "shape:(None, 200)\n",
      "parameters: W\n",
      "layer18: \n",
      "<lasagne.layers.special.BiasLayer object at 0x12c9017b8>\n",
      "shape:(None, 200)\n",
      "parameters: b\n",
      "layer19: \n",
      "<lasagne.layers.special.NonlinearityLayer object at 0x12c901dd8>\n",
      "shape:(None, 200)\n",
      "layer20: \n",
      "<lasagne.layers.dense.DenseLayer object at 0x12c901fd0>\n",
      "shape:(None, 784)\n",
      "parameters: W\n",
      "layer21: \n",
      "<lasagne.layers.special.BiasLayer object at 0x12c901c50>\n",
      "shape:(None, 784)\n",
      "parameters: b\n",
      "layer22: \n",
      "<lasagne.layers.special.NonlinearityLayer object at 0x12c9231d0>\n",
      "shape:(None, 784)\n",
      "----------------------------------------------------------------------------\n",
      "compiling model\n"
     ]
    }
   ],
   "source": [
    "# build neural network class\n",
    "nnmodel = nn.NeuralNet(network, placeholders)\n",
    "nnmodel.inspect_layers()\n",
    "\n",
    "# set output file paths\n",
    "output_name = 'MNIST_vae'\n",
    "results_path = utils.make_directory(data_path, 'Results')\n",
    "results_path = utils.make_directory(results_path, output_name)\n",
    "file_path = os.path.join(results_path, output_name)\n",
    "nntrainer = nn.NeuralTrainer(nnmodel, optimization, save='best', file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  valid loss:\t\t1262.32289\n",
      "  valid mean squared loss:\t0.13535\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t938.19430\n",
      "  valid mean squared loss:\t0.10467\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t812.76215\n",
      "  valid mean squared loss:\t0.09675\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t730.55875\n",
      "  valid mean squared loss:\t0.09390\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t670.26449\n",
      "  valid mean squared loss:\t0.09145\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t628.55691\n",
      "  valid mean squared loss:\t0.09022\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t604.94115\n",
      "  valid mean squared loss:\t0.08944\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t583.57584\n",
      "  valid mean squared loss:\t0.08882\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t561.68471\n",
      "  valid mean squared loss:\t0.08730\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t548.03346\n",
      "  valid mean squared loss:\t0.08622\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t531.31231\n",
      "  valid mean squared loss:\t0.08522\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t516.93656\n",
      "  valid mean squared loss:\t0.08440\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t509.93669\n",
      "  valid mean squared loss:\t0.08366\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t504.23735\n",
      "  valid mean squared loss:\t0.08320\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t497.24047\n",
      "  valid mean squared loss:\t0.08260\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t490.29464\n",
      "  valid mean squared loss:\t0.08284\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t493.23119\n",
      "  valid mean squared loss:\t0.08316\n",
      "\n",
      "  valid loss:\t\t478.82324\n",
      "  valid mean squared loss:\t0.08243\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t472.72062\n",
      "  valid mean squared loss:\t0.08219\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t461.75575\n",
      "  valid mean squared loss:\t0.08164\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t455.11784\n",
      "  valid mean squared loss:\t0.08081\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t445.91539\n",
      "  valid mean squared loss:\t0.07973\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t438.04495\n",
      "  valid mean squared loss:\t0.07864\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t436.83844\n",
      "  valid mean squared loss:\t0.07840\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t433.10342\n",
      "  valid mean squared loss:\t0.07803\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t430.75465\n",
      "  valid mean squared loss:\t0.07760\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t429.83710\n",
      "  valid mean squared loss:\t0.07728\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t430.80442\n",
      "  valid mean squared loss:\t0.07732\n",
      "\n",
      "  valid loss:\t\t434.10205\n",
      "  valid mean squared loss:\t0.07754\n",
      "\n",
      "  valid loss:\t\t434.48656\n",
      "  valid mean squared loss:\t0.07767\n",
      "\n",
      "  valid loss:\t\t430.17365\n",
      "  valid mean squared loss:\t0.07739\n",
      "\n",
      "  valid loss:\t\t430.31422\n",
      "  valid mean squared loss:\t0.07712\n",
      "\n",
      "  valid loss:\t\t425.02500\n",
      "  valid mean squared loss:\t0.07656\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t423.44422\n",
      "  valid mean squared loss:\t0.07631\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t420.98298\n",
      "  valid mean squared loss:\t0.07598\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t420.44639\n",
      "  valid mean squared loss:\t0.07535\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t419.21643\n",
      "  valid mean squared loss:\t0.07512\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t417.94068\n",
      "  valid mean squared loss:\t0.07485\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t412.85282\n",
      "  valid mean squared loss:\t0.07514\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t410.80626\n",
      "  valid mean squared loss:\t0.07499\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t407.26342\n",
      "  valid mean squared loss:\t0.07541\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t406.83719\n",
      "  valid mean squared loss:\t0.07482\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t405.96621\n",
      "  valid mean squared loss:\t0.07470\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t400.65951\n",
      "  valid mean squared loss:\t0.07454\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t398.08723\n",
      "  valid mean squared loss:\t0.07393\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t397.48433\n",
      "  valid mean squared loss:\t0.07354\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t396.23808\n",
      "  valid mean squared loss:\t0.07331\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t395.09667\n",
      "  valid mean squared loss:\t0.07312\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t393.36392\n",
      "  valid mean squared loss:\t0.07291\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t393.27436\n",
      "  valid mean squared loss:\t0.07317\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t392.49956\n",
      "  valid mean squared loss:\t0.07303\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t386.74252\n",
      "  valid mean squared loss:\t0.07278\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t386.23773\n",
      "  valid mean squared loss:\t0.07264\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t383.79042\n",
      "  valid mean squared loss:\t0.07269\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t383.08037\n",
      "  valid mean squared loss:\t0.07262\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t383.13335\n",
      "  valid mean squared loss:\t0.07252\n",
      "\n",
      "  valid loss:\t\t382.57006\n",
      "  valid mean squared loss:\t0.07239\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t381.43238\n",
      "  valid mean squared loss:\t0.07217\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t374.27167\n",
      "  valid mean squared loss:\t0.07188\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t373.61299\n",
      "  valid mean squared loss:\t0.07179\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t373.63946\n",
      "  valid mean squared loss:\t0.07173\n",
      "\n",
      "  valid loss:\t\t373.42133\n",
      "  valid mean squared loss:\t0.07161\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t373.58253\n",
      "  valid mean squared loss:\t0.07167\n",
      "\n",
      "  valid loss:\t\t373.15020\n",
      "  valid mean squared loss:\t0.07166\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t372.52768\n",
      "  valid mean squared loss:\t0.07152\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t372.08566\n",
      "  valid mean squared loss:\t0.07138\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t372.24727\n",
      "  valid mean squared loss:\t0.07152\n",
      "\n",
      "  valid loss:\t\t371.80973\n",
      "  valid mean squared loss:\t0.07138\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t375.73361\n",
      "  valid mean squared loss:\t0.07269\n",
      "\n",
      "  valid loss:\t\t377.37014\n",
      "  valid mean squared loss:\t0.07250\n",
      "\n",
      "  valid loss:\t\t376.56135\n",
      "  valid mean squared loss:\t0.07224\n",
      "\n",
      "  valid loss:\t\t374.82119\n",
      "  valid mean squared loss:\t0.07201\n",
      "\n",
      "  valid loss:\t\t369.60871\n",
      "  valid mean squared loss:\t0.07198\n",
      "saving model parameters to: /Users/juliankimura/Desktop/data/MNIST/Results/MNIST_vae/MNIST_vae_best.pickle\n",
      "\n",
      "  valid loss:\t\t369.79130\n",
      "  valid mean squared loss:\t0.07221\n",
      "\n",
      "  valid loss:\t\t396.35618\n",
      "  valid mean squared loss:\t0.07644\n",
      "\n",
      "  valid loss:\t\t399.91104\n",
      "  valid mean squared loss:\t0.07713\n",
      "\n",
      "  valid loss:\t\t389.16606\n",
      "  valid mean squared loss:\t0.07615\n",
      "\n",
      "  valid loss:\t\t386.88089\n",
      "  valid mean squared loss:\t0.07574\n",
      "\n",
      "  valid loss:\t\t383.58214\n",
      "  valid mean squared loss:\t0.07500\n",
      "\n",
      "  valid loss:\t\t381.71142\n",
      "  valid mean squared loss:\t0.07441\n",
      "\n",
      "  valid loss:\t\t379.23130\n",
      "  valid mean squared loss:\t0.07409\n",
      "\n",
      "  valid loss:\t\t377.00362\n",
      "  valid mean squared loss:\t0.07373\n",
      "\n",
      "  valid loss:\t\t377.87994\n",
      "  valid mean squared loss:\t0.07410\n",
      "Patience ran out... Early stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<deepomics.neuralnetwork.NeuralTrainer at 0x12c923b38>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "learn.train_minibatch(nntrainer, data={'train': X_train, 'valid': X_val}, \n",
    "                              batch_size=100, num_epochs=500, patience=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('input', <lasagne.layers.input.InputLayer at 0x11a68e5c0>),\n",
       "             ('dense1', <lasagne.layers.dense.DenseLayer at 0x11a68e630>),\n",
       "             ('dense1_batch',\n",
       "              <lasagne.layers.normalization.BatchNormLayer at 0x11a68e5f8>),\n",
       "             ('dense1_active',\n",
       "              <lasagne.layers.special.NonlinearityLayer at 0x11a69e160>),\n",
       "             ('dense1_dropout',\n",
       "              <lasagne.layers.noise.DropoutLayer at 0x11a69e6d8>),\n",
       "             ('dense2', <lasagne.layers.dense.DenseLayer at 0x11a69e8d0>),\n",
       "             ('dense2_batch',\n",
       "              <lasagne.layers.normalization.BatchNormLayer at 0x11a69e860>),\n",
       "             ('dense2_active',\n",
       "              <lasagne.layers.special.NonlinearityLayer at 0x11a69ee10>),\n",
       "             ('dense2_dropout',\n",
       "              <lasagne.layers.noise.DropoutLayer at 0x11a69efd0>),\n",
       "             ('encode_mu', <lasagne.layers.dense.DenseLayer at 0x12c901080>),\n",
       "             ('encode_logsigma',\n",
       "              <lasagne.layers.dense.DenseLayer at 0x12c901240>),\n",
       "             ('Z',\n",
       "              <deepomics.build_network.VariationalSampleLayer at 0x12c901588>),\n",
       "             ('encode_active',\n",
       "              <lasagne.layers.special.NonlinearityLayer at 0x12c901780>),\n",
       "             ('dense3', <lasagne.layers.dense.DenseLayer at 0x12c9017f0>),\n",
       "             ('dense3_bias',\n",
       "              <lasagne.layers.special.BiasLayer at 0x12c901828>),\n",
       "             ('dense3_active',\n",
       "              <lasagne.layers.special.NonlinearityLayer at 0x12c9019e8>),\n",
       "             ('dense4', <lasagne.layers.dense.DenseLayer at 0x12c901c18>),\n",
       "             ('dense4_bias',\n",
       "              <lasagne.layers.special.BiasLayer at 0x12c9017b8>),\n",
       "             ('dense4_active',\n",
       "              <lasagne.layers.special.NonlinearityLayer at 0x12c901dd8>),\n",
       "             ('decode_mu', <lasagne.layers.dense.DenseLayer at 0x12c901fd0>),\n",
       "             ('decode_mu_bias',\n",
       "              <lasagne.layers.special.BiasLayer at 0x12c901c50>),\n",
       "             ('decode_mu_active',\n",
       "              <lasagne.layers.special.NonlinearityLayer at 0x12c9231d0>)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnmodel.network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('You cannot drop a non-broadcastable dimension.', ((False,), ('x',)))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f5c4d93310fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mz_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgenerated_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnnmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'decode_mu_active'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mnnmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encode_mu'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mz_var\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mgen_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz_var\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnum_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/juliankimura/anaconda/lib/python3.5/site-packages/lasagne/layers/helper.py\u001b[0m in \u001b[0;36mget_output\u001b[0;34m(layer_or_layers, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m                                  \u001b[0;34m\"mapping this layer to an input expression.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                                  % layer)\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0mall_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_output_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                 accepted_kwargs |= set(utils.inspect_kwargs(\n",
      "\u001b[0;32m/Users/juliankimura/anaconda/lib/python3.5/site-packages/lasagne/layers/special.py\u001b[0m in \u001b[0;36mget_output_for\u001b[0;34m(self, input, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m                        \u001b[0;32melse\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_axes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                        for input_axis in range(input.ndim)]\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdimshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/juliankimura/anaconda/lib/python3.5/site-packages/theano/tensor/var.py\u001b[0m in \u001b[0;36mdimshuffle\u001b[0;34m(self, *pattern)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         op = theano.tensor.basic.DimShuffle(list(self.type.broadcastable),\n\u001b[0;32m--> 355\u001b[0;31m                                             pattern)\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/juliankimura/anaconda/lib/python3.5/site-packages/theano/tensor/elemwise.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_broadcastable, new_order, inplace)\u001b[0m\n\u001b[1;32m    175\u001b[0m                     raise ValueError(\n\u001b[1;32m    176\u001b[0m                         \u001b[0;34m\"You cannot drop a non-broadcastable dimension.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m                         (input_broadcastable, new_order))\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;31m# this is the list of the original dimensions that we keep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: ('You cannot drop a non-broadcastable dimension.', ((False,), ('x',)))"
     ]
    }
   ],
   "source": [
    "z_var = T.vector()\n",
    "generated_x = layers.get_output(nnmodel.network['decode_mu_active'], {nnmodel.network['encode_mu']: z_var}, deterministic=True)\n",
    "gen_fn = theano.function([z_var], generated_x)\n",
    "\n",
    "num_grid = 20\n",
    "pos = np.linspace(-2, 2, num_grid)\n",
    "samples = []\n",
    "for i in range(num_grid):\n",
    "    for j in range(num_grid):\n",
    "        z = np.asarray([pos[i], pos[j]], dtype=theano.config.floatX)\n",
    "        samples.append(gen_fn(z))\n",
    "samples = np.array(samples)        \n",
    "                \n",
    "plt.imsave(os.path.join(results_path,'MNIST_manifold.png'),\n",
    "           (samples.reshape(num_grid, num_grid, 28, 20).transpose(0, 2, 1, 3)\n",
    "            .reshape(num_grid*28, num_grid*20)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.imshow((samples.reshape(num_grid, num_grid, 28, 20)\n",
    "                   .transpose(0, 2, 1, 3)\n",
    "                   .reshape(num_grid*28, num_grid*20)), cmap='gray')\n",
    "plt.axis('off')\n",
    "fig.set_size_inches(20,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 10\n",
    "samples = X_train[:num_steps*num_steps]\n",
    "plt.imsave(os.path.join(results_path,'MNIST_raw.png'), \n",
    "           (samples.reshape(num_steps, num_steps, 28, 20).transpose(0, 2, 1, 3)\n",
    "            .reshape(num_steps*28, num_steps*20)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
