{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating synthetic sequences with embedded regulatory grammars for RNA-binding proteins\n",
    "\n",
    "### Motivation\n",
    "\n",
    "RNA-binding proteins physically interact with RNAs primarily through direct nucleotide interactions via sequence binding motifs or binding to secondary structure motifs.  Here, I will focus on the former using a deep convolutional neural network. However, assessing the performance on actual data is difficult as the \"ground truth\" is usually not known. \n",
    "\n",
    "To test the performance of CNNs, I am going to simulate idealistic data where the sequences contain combinations of \"known\" binding motifs implanted in specific \"known\" locations while the rest of the sequence is generated randomly. This idealistic dataset can therefore gauge whether or not CNNs can indeed recover the motifs and their regulatory grammars in the  simplest scenario. It will also provide a baseline understanding the limitations of this approach, such as how many sequences are needed to recover different levels of regulatory grammar complexities.   \n",
    "\n",
    "Note that the regulatory grammars generated here are unrealistic -- there is no such databases with accurate annotations as far as I am aware of.  Here, only the binding motifs are derived from actual experimental data; while the regulatory grammars are arbitrary, generated from a probabilisitic framework with a user-defined level of complexity, i.e. how many motifs interact with each other.\n",
    "\n",
    "### Simulation model\n",
    "\n",
    "To generate regulatory grammars, we need to create a framework for the interactions of specific motifs across distinct spatial distances. \n",
    "\n",
    "First, we will assume there are only P proteins. Of these P proteins, we can generate G regulatory grammars, which sample combinations of the M motifs.  We can then generate arbitrary distances, sampled from an exponential distribution, between each motif. Once the motif distances and combinations have been set, these constitute the set of regulatory grammars.  \n",
    "\n",
    "We can also simulate negative results by simulating different motifs with the same distances or the same motifs with different distance or incomplete grammars.  First, we'll just assume a perfect dataset and see how it performs.  Then, we will systematically increase the complexity to see when exactly this model fails.  \n",
    "\n",
    "\n",
    "#### Create a motif database for drosophila melanogaster\n",
    "\n",
    "The data comes from Ray et al. \"A compendium of RNA-binding motifs for decoding gene regulation\" (http://www.nature.com/nature/journal/v499/n7457/abs/nature12311.html). The link to the motifs I downloaded is here: \n",
    "\n",
    "\\$ wget http://hugheslab.ccbr.utoronto.ca/supplementary-data/RNAcompete_eukarya/top10align_motifs.tar.gz\n",
    "\n",
    "\\$ tar xzvf top10align_motifs.tar.gz\n",
    "\n",
    "Here, each file is a different RBP motif as a position frequency matrix.  So, the first step is to compile all of these files into a suitable database.  In particular, we can parse each motifs (position frequency matrix) from each file in motifpath (downloaded top10align_motifs folder), create a database (list of arrays), and save as binary format (motif.list):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from six.moves import cPickle\n",
    "\n",
    "motifpath = 'top10align_motifs/'   # directory where motif files are located\n",
    "motiflist = 'motif.pickle'         # output filename\n",
    "\n",
    "# get all motif files in motifpath directory\n",
    "listdir = os.listdir(motifpath)\n",
    "\n",
    "# parse motifs\n",
    "motif_set = []\n",
    "for files in listdir:\n",
    "    df = pd.read_table(os.path.join(motifpath,files))\n",
    "    motif_set.append(df.iloc[0::,1::].transpose())\n",
    "\n",
    "# save motifs    \n",
    "f = open(motiflist, 'wb')\n",
    "cPickle.dump(motif_set, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simulate some sequences with some specs... Note, there are 244 motifs here. So, for simplicity, we will downsample this to create different levels of dataset complexity.  \n",
    "\n",
    "$N$: total number of sequences \n",
    "\n",
    "$M$: total number of motifs ($M \\leq 244$) \n",
    "\n",
    "$S$: size of each sequence \n",
    "\n",
    "$w$: population fraction of each regulatory grammar \n",
    "\n",
    "To make the sequence super clean, we will use a uniform distribution for the PWFs of the 'non-motif sequences'. We will first set up a hierarchical interaction of the motifs, a so-called \"regulatory grammar\". \n",
    "\n",
    "So, first, we will sample a regulatory grammar.  This will give us which motifs are present and how far apart they are separated with respect to each other.  Then, we can create a postiion frequency matrix of this regulatory grammar and simulate a set number based on the population fraction $w$. \n",
    "\n",
    "Now that we generated some simulated data, we should shuffle the data, then split the data into training set, cross-validation set, and test set, while converting the sequence data into one-hot representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating motif grammars\n",
      "Generating synthetic data\n",
      "Splitting dataset into train, cross-validation, and test\n",
      "Saving dataset\n",
      "Saving model\n"
     ]
    }
   ],
   "source": [
    "from six.moves import cPickle\n",
    "import numpy as np\n",
    "\n",
    "def generate_grammar_model(options):\n",
    "    \"\"\"generate a regulatory grammar model: various numbers of motifs with \n",
    "    distinct separations.\"\"\"\n",
    "\n",
    "    # input options\n",
    "    motif_set = options[0]   # set of all motifs\n",
    "    num_motif = options[1]           # number of motifs for data set (to sample from motif_set)\n",
    "    num_grammar = options[2]           # number of regulatory grammars (combinations of motifs)\n",
    "    interaction_rate = options[3]  # exponential rate of number of motifs for each grammar\n",
    "    distance_scale = options[4]    # exponential rate of distance between motifs\n",
    "    distance_offset = options[5]   # offset addition between motif distances\n",
    "    max_motif = options[6]         # maximum number of motifs in a grammar\n",
    "\n",
    "    # select M random motifs from the complete list of motif_set\n",
    "    motifIndex = np.random.permutation(len(motif_set))[0:num_motif]\n",
    "\n",
    "    # build subset of motifs relevant for dataset\n",
    "    motifs = []\n",
    "    for index in motifIndex:\n",
    "        motifs.append(motif_set[index])\n",
    "    \n",
    "    # generate G regulatory grammars (combinations of motifs + distance between motifs)\n",
    "    Z = np.ceil(np.random.exponential(scale=interaction_rate, size=num_grammar)).astype(int)\n",
    "    num_interactions = np.minimum(Z, max_motif)\n",
    "    grammar = []\n",
    "    distance = []\n",
    "    for num in num_interactions:\n",
    "        index = motifIndex[np.random.randint(num_motif, size=num)]\n",
    "        grammar.append(index)\n",
    "        separation = np.ceil(np.random.exponential(scale=distance_scale, size=num)).astype(int) + distance_offset\n",
    "        distance.append(separation)\n",
    "\n",
    "    return [motifs, grammar, distance]\n",
    "\n",
    "\n",
    "def generate_sequence_pwm(seq_length, gram, dist, motifs):\n",
    "    \"\"\"generates the position weight matrix (pwm) for a given regulatory grammar \n",
    "    with a string length S\"\"\"\n",
    "    \n",
    "    # figure out offset after centering grammar\n",
    "    offset = np.round(np.random.uniform(1,(seq_length - np.sum(dist) - len(dist)*8 - 20)))\n",
    "\n",
    "    # build position weight matrix\n",
    "    sequence_pwm = np.ones((4,offset))/4\n",
    "    for i in xrange(len(gram)):\n",
    "        sequence_pwm = np.hstack((sequence_pwm, motifs[i]))\n",
    "        if i < len(dist):\n",
    "            sequence_pwm = np.hstack((sequence_pwm, np.ones((4,dist[i]))/4))\n",
    "\n",
    "    # fill in the rest of the sequence with uniform distribution to have length seq_length\n",
    "    sequence_pwm = np.hstack((sequence_pwm, np.ones((4,seq_length - sequence_pwm.shape[1]))/4))\n",
    "    \n",
    "    return sequence_pwm\n",
    "\n",
    "\n",
    "def generate_sequence_model(seq_length, model):\n",
    "    \"\"\"generate the sequence models (PWMs) for each regulatory grammars. \"\"\"\n",
    "    \n",
    "    motifs = model[0]      # set of motifs\n",
    "    grammar = model[1]     # set of combinations of motifs\n",
    "    distance = model[2]    # set of distances between motifs\n",
    "\n",
    "    # build a PWM for each regulatory grammar\n",
    "    seq_model = []\n",
    "    for j in xrange(len(grammar)):\n",
    "        seq_model.append(generate_sequence_pwm(seq_length, grammar[j], distance[j], motifs)) \n",
    "        \n",
    "    return seq_model\n",
    "\n",
    "\n",
    "def simulate_sequence(sequence_pwm):\n",
    "    \"\"\"simulate a sequence given a sequence model\"\"\"\n",
    "    \n",
    "    nucleotide = 'ACGU'\n",
    "\n",
    "    # sequence length\n",
    "    seq_length = sequence_pwm.shape[1]\n",
    "\n",
    "    # generate uniform random number for each nucleotide in sequence\n",
    "    Z = np.random.uniform(0,1,seq_length)\n",
    "    \n",
    "    # calculate cumulative sum of the probabilities\n",
    "    cum_prob = sequence_pwm.cumsum(axis=0)\n",
    "\n",
    "    # go through sequence and find bin where random number falls in cumulative \n",
    "    # probabilities for each nucleotide\n",
    "    sequence = ''\n",
    "    for i in xrange(seq_length):\n",
    "        index=[j for j in xrange(4) if Z[i] < cum_prob[j,i]][0]\n",
    "        sequence += nucleotide[index]\n",
    "\n",
    "    return sequence\n",
    "\n",
    "\n",
    "def simulate_data(seq_model, num_seq):\n",
    "    \"\"\"simulates N sequences with random population fractions for each sequence \n",
    "    model (PWM) of each regulatory grammar \"\"\"\n",
    "\n",
    "    # simulate random population fractions and scale to N sequences\n",
    "    w = np.random.uniform(0, 1, size=len(seq_model))\n",
    "    w = np.round(w/sum(w)*num_seq)\n",
    "    popFrac = w.astype(int)\n",
    "    \n",
    "    # create a popFrac weighted number of simulation for each regulatory grammar\n",
    "    label = []\n",
    "    data = []\n",
    "    for i in xrange(len(popFrac)):\n",
    "        for j in xrange(popFrac[i]):\n",
    "            sequence = simulate_sequence(seq_model[i])\n",
    "            data.append(sequence)\n",
    "            label.append(i)\n",
    "            \n",
    "    return data, label\n",
    "\n",
    "\n",
    "def convert_one_hot(seq):\n",
    "    \"\"\"convert a sequence into a 1-hot representation\"\"\"\n",
    "    \n",
    "    nucleotide = 'ACGU'\n",
    "    N = len(seq)\n",
    "    one_hot_seq = np.zeros((4,N))\n",
    "    for i in xrange(200):         \n",
    "        #for j in range(4):\n",
    "        #    if seq[i] == nucleotide[j]:\n",
    "        #        one_hot_seq[j,i] = 1\n",
    "        index = [j for j in xrange(4) if seq[i] == nucleotide[j]]\n",
    "        one_hot_seq[index,i] = 1\n",
    "        \n",
    "    return one_hot_seq\n",
    "\n",
    "\n",
    "def subset_data(data, label, sub_index):\n",
    "    \"\"\"returns a subset of the data and labels based on sub_index\"\"\"\n",
    "    \n",
    "    num_labels = len(np.unique(label))\n",
    "    num_sub = len(sub_index)\n",
    "    \n",
    "    sub_set = np.zeros((num_sub, 4, len(data[0])))\n",
    "    sub_set_label = np.zeros((num_sub, num_labels))\n",
    "    \n",
    "    k = 0;\n",
    "    for index in sub_index:\n",
    "        sub_set[k] = convert_one_hot(data[index])\n",
    "        sub_set_label[k,label[index]] = 1\n",
    "        k += 1\n",
    "\n",
    "    sub_set_label = sub_set_label.astype(np.uint8)\n",
    "    \n",
    "    return (sub_set, sub_set_label)\n",
    "\n",
    "\n",
    "def split_data(data, label, split_size):\n",
    "    \"\"\"split data into train set, cross-validation set, and test set\"\"\"\n",
    "    \n",
    "    # number of labels\n",
    "    num_labels = len(np.unique(label))\n",
    "\n",
    "    # determine indices of each dataset\n",
    "    N = len(data)\n",
    "    cum_index = np.cumsum(np.multiply([0, split_size[0], split_size[1], split_size[2]],N)).astype(int) \n",
    "\n",
    "    # shuffle data\n",
    "    shuffle = np.random.permutation(N)\n",
    "\n",
    "    # training dataset\n",
    "    train_index = shuffle[range(cum_index[0], cum_index[1])]\n",
    "    cross_validation_index = shuffle[range(cum_index[1], cum_index[2])]\n",
    "    test_index = shuffle[range(cum_index[2], cum_index[3])]\n",
    "\n",
    "    # create subsets of data based on indices \n",
    "    train = subset_data(data, label, train_index)\n",
    "    cross_validation = subset_data(data, label, cross_validation_index)\n",
    "    test = subset_data(data, label, test_index)\n",
    "    \n",
    "    return train, cross_validation, test\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # dataset parameters\n",
    "    num_seq = 100000       # number of sequences\n",
    "    seq_length = 200      # length of sequence\n",
    "    num_motif = 10        # number of motifs\n",
    "    num_grammar = 20      # number of regulatory grammars\n",
    "    filename =  'N=' + str(num_seq) + \\\n",
    "                '_S=' + str(seq_length) + \\\n",
    "                '_M=' + str(num_motif) + \\\n",
    "                '_G=' + str(num_grammar) # output filename\n",
    "                \n",
    "    # motif interaction parameters (grammars)\n",
    "    interaction_rate = 1.5       # exponential rate of number of motifs for each grammar\n",
    "    distance_scale = seq_length/15        # exponential rate of distance between motifs\n",
    "    offset = 5                   # offset addition between motif distances\n",
    "    maxMotif = 5                 # maximum number of motifs in a grammar\n",
    "\n",
    "    # percentage for each dataset\n",
    "    train_size = 0.7\n",
    "    cross_validation_size = 0.15\n",
    "    test_size = 0.15\n",
    "\n",
    "    # load motif list from file\n",
    "    motiflist = 'motif.pickle'\n",
    "    f = open(motiflist, 'rb')\n",
    "    motif_set = cPickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    # generate regulatory grammar model\n",
    "    print \"Generating motif grammars\"\n",
    "    options = [motif_set, num_motif, num_grammar, \n",
    "            interaction_rate, distance_scale, offset, maxMotif]\n",
    "    model = generate_grammar_model(options)\n",
    "\n",
    "    # convert this to a sequence position weight matrix for each model\n",
    "    seq_model = generate_sequence_model(seq_length, model)\n",
    "\n",
    "    # simulate N sequences based on the position weight matrices\n",
    "    print \"Generating synthetic data\"\n",
    "    data, label = simulate_data(seq_model, num_seq)\n",
    "\n",
    "    # get indices for each dataset\n",
    "    print \"Splitting dataset into train, cross-validation, and test\"\n",
    "    split_size = [train_size, cross_validation_size, test_size]\n",
    "    train, cross_validation, test = split_data(data, label, split_size)\n",
    "\n",
    "    # save training dataset in one-hot representation\n",
    "    print \"Saving dataset\"\n",
    "    f = open(filename+'_data.pickle', 'wb')\n",
    "    cPickle.dump(train, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "    cPickle.dump(cross_validation, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "    cPickle.dump(test, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "\n",
    "    # save training dataset in one-hot representation\n",
    "    print \"Saving model\"\n",
    "    f = open(filename+'_model.pickle', 'wb')\n",
    "    cPickle.dump(options, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "    cPickle.dump(model, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "    cPickle.dump(seq_model, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# For TensorFlow, need to reorganize data\n",
    "    # sub_set = munge_data(sub_set):\n",
    "        \n",
    "def munge_data(data):\n",
    "    num_data, dim, seq_length = np.shape(data)\n",
    "    data = np.transpose(data, (0,2,1))\n",
    "    return np.reshape(data, np.shape(data)+(1,))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now, we are ready to build and test our deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from six.moves import cPickle\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# load training data\n",
    "filename = 'train_data_10000_200_10_20.pickle'\n",
    "f = open(filename, 'rb')\n",
    "train = cPickle.load(f)\n",
    "cross_validation = cPickle.load(f)\n",
    "test = cPickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# separate data\n",
    "train_set, train_set_label = train\n",
    "cross_validation_set, cross_validation_set_label = cross_validation\n",
    "test_set, test_set_label = test\n",
    "\n",
    "# munge data for deep learning\n",
    "train_set = munge_data(train_set)\n",
    "cross_validation_set = munge_data(cross_validation_set)\n",
    "test_set = munge_data(test_set)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making directory data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "outdir = 'data'\n",
    "if not os.path.isdir(outdir):\n",
    "    os.mkdir(outdir)\n",
    "    print \"making directory: \" + outdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implanting_sequence_motifs.ipynb  process_dataset.py  tensorflow_train.py\r\n",
      "keras_train.py                    scrape_motifs.py    \u001b[0m\u001b[01;34mtop10align_motifs\u001b[0m/\r\n",
      "motif.pickle                      simulate_data.py\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from six.moves import cPickle\n",
    "\n",
    "# load motif list from file\n",
    "filename = 'model_10000_200_10_20.pickle'\n",
    "f = open(filename, 'rb')\n",
    "model = cPickle.load(f)\n",
    "options = cPickle.load(f)\n",
    "seq_model = cPickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# options = [motif_set, M, G, interaction_rate, distance_scale, offset, maxMotif]\n",
    "# model [motifs, grammar, distance]\n",
    "# seq_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
