{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 980 (CNMeM is disabled, CuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "import os, sys, gzip\n",
    "import cPickle as pickle\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1004)   # for reproducibility\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "from scipy.misc import imresize\n",
    "\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "from lasagne import layers, nonlinearities, updates, objectives, init, regularization\n",
    "from lasagne.layers import get_output, get_output_shape, get_all_params\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "#from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "datapath='/home/peter/Data/CMAP/training.csv'\n",
    "savepath='/home/peter/Data/CMAP/dataset.hd5f'\n",
    "\n",
    "batch_size = 20000\n",
    "shuffle_index = np.random.permutation(100000)\n",
    "for i in range(1,5):\n",
    "    batch_index =range(i*batch_size, (i+1)*batch_size)\n",
    "    data = pd.read_csv(datapath, usecols=shuffle_index[batch_index], header=None, dtype=np.float32)\n",
    "    genes = data.as_matrix()\n",
    "    num_landmark = 970\n",
    "    num_nonlandmark = 11350\n",
    "    num_samples = genes.shape[1]\n",
    "    landmark = genes[:970,:]\n",
    "    nonlandmark = genes[970:,:]\n",
    "    del genes\n",
    "\n",
    "    f = h5py.File(savepath, \"a\")\n",
    "    dset = f.create_dataset(\"landmark\"+str(i), data=landmark)\n",
    "    dset = f.create_dataset(\"nonlandmark\"+str(i), data=nonlandmark)\n",
    "    del landmark\n",
    "    del nonlandmark\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "def normalize_data(landmark, nonlandmark, mean_landmark, std_landmark, mean_nonlandmark, std_nonlandmark, num_samples):\n",
    "    landmark = (landmark - np.outer(mean_landmark,np.ones(num_samples)))/np.outer(std_landmark,np.ones(num_samples))\n",
    "    nonlandmark = (nonlandmark - np.outer(mean_nonlandmark,np.ones(num_samples)))/np.outer(std_nonlandmark,np.ones(num_samples))\n",
    "    landmark = landmark.transpose([1,0])\n",
    "    nonlandmark = nonlandmark.transpose([1,0])\n",
    "\n",
    "    return landmark, nonlandmark\n",
    "\n",
    "\n",
    "filepath='/home/peter/Data/CMAP/dataset.hd5f'\n",
    "trainmat = h5py.File(filepath, 'r')\n",
    "landmark= np.array(trainmat['landmark1']).astype(np.float32)\n",
    "nonlandmark = np.array(trainmat['nonlandmark1']).astype(np.float32)\n",
    "\n",
    "num_files = 5\n",
    "mean_landmark = 0\n",
    "std_landmark = 0\n",
    "mean_nonlandmark = 0\n",
    "std_nonlandmark = 0\n",
    "for i in range(num_files):\n",
    "    landmark= np.array(trainmat['landmark'+str(i)]).astype(np.float32)\n",
    "    nonlandmark = np.array(trainmat['nonlandmark'+str(i)]).astype(np.float32)\n",
    "    mean_landmark += np.mean(landmark, axis=1)\n",
    "    std_landmark += np.std(landmark,axis=1)\n",
    "    mean_nonlandmark += np.mean(nonlandmark, axis=1)\n",
    "    std_nonlandmark += np.std(nonlandmark, axis=1)\n",
    "\n",
    "mean_landmark /= num_files\n",
    "std_landmark /= num_files\n",
    "mean_nonlandmark /= num_files\n",
    "std_nonlandmark /= num_files\n",
    "\n",
    "\n",
    "\n",
    "savepath='/home/peter/Data/CMAP/dataset_norm.hd5f'\n",
    "f = h5py.File(savepath, \"a\")\n",
    "batch_size = 20000\n",
    "for i in range(0,5):\n",
    "    landmark = np.array(trainmat['landmark'+str(i)]).astype(np.float32)\n",
    "    nonlandmark = np.array(trainmat['nonlandmark'+str(i)]).astype(np.float32)\n",
    "    landmark, nonlandmark = normalize_data(landmark, nonlandmark, mean_landmark, std_landmark, mean_nonlandmark, std_nonlandmark, num_samples)\n",
    "    dset = f.create_dataset(\"landmark\"+str(i), data=landmark)\n",
    "    dset = f.create_dataset(\"nonlandmark\"+str(i), data=nonlandmark)\n",
    "dset = f.create_dataset(\"mean_landmark\", data=mean_landmark)\n",
    "dset = f.create_dataset(\"std_landmark\", data=std_landmark)\n",
    "dset = f.create_dataset(\"mean_nonlandmark\", data=mean_nonlandmark)\n",
    "dset = f.create_dataset(\"std_nonlandmark\", data=std_nonlandmark)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filepath='/home/peter/Data/CMAP/dataset_norm.hd5f'\n",
    "trainmat = h5py.File(filepath, 'r')\n",
    "landmark= np.array(trainmat['landmark4']).astype(np.float32)\n",
    "nonlandmark = np.array(trainmat['nonlandmark4']).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nO+3AlsO4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f46c218d150>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 2\n",
    "plt.hist(landmark[:,index]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_encode=100\n",
    "\n",
    "input_var = T.dmatrix('inputs')\n",
    "shape = (None, landmark.shape[1])\n",
    "net = {}\n",
    "net['input'] = layers.InputLayer(shape=shape, input_var=input_var)\n",
    "net['dense1'] = layers.DenseLayer(net['input'], num_units=nonlandmark.shape[1], W=init.GlorotUniform(), \n",
    "                                  b=init.Constant(.0), nonlinearity=None)\n",
    "net['dense1_active'] = layers.NonlinearityLayer(net['dense1'], nonlinearity=nonlinearities.linear)\n",
    "net['output'] = net['dense1_active']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_var = T.dmatrix('inputs')\n",
    "prediction = get_output(net['output'], deterministic=False )\n",
    "loss = objectives.squared_error(target_var, prediction)\n",
    "loss = objectives.aggregate(loss, mode='mean')\n",
    "\n",
    "# ADAM updates\n",
    "params = get_all_params(net['output'], trainable=True)\n",
    "update_op = updates.adam(loss, params, learning_rate=1e-3)\n",
    "\n",
    "train_fun = theano.function([input_var, target_var], loss , updates=update_op, allow_input_downcast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size=128):\n",
    "    for start_idx in range(0, len(X)-batch_size+1, batch_size):\n",
    "        excerpt = slice(start_idx, start_idx+batch_size)\n",
    "        yield X[excerpt], y[excerpt]\n",
    "\n",
    "batch_size = 100        \n",
    "num_samples = 20000\n",
    "num_files = 5        \n",
    "num_epochs = 60    \n",
    "num_batches = num_samples // batch_size\n",
    "for epoch in range(num_epochs):\n",
    "    sys.stdout.write(\"\\rEpoch %d \\n\"%(epoch+1))\n",
    "\n",
    "    train_loss = 0\n",
    "    for i in range(num_files):\n",
    "        sys.stdout.write(\"\\r  File %d \\n\"%(i+1))\n",
    "        landmark= np.array(trainmat['landmark'+str(i)]).astype(np.float32)\n",
    "        nonlandmark = np.array(trainmat['nonlandmark'+str(i)]).astype(np.float32)\n",
    "        batches = batch_generator(landmark, nonlandmark, batch_size)\n",
    "        \n",
    "        loss = 0\n",
    "        for i in range(num_batches):\n",
    "            X, y = next(batches)\n",
    "            loss += train_fun(X, y)\n",
    "        print(\"    training loss:\\t\\t{:.6f}\".format(loss/num_batches))    \n",
    "        train_loss += loss/num_batches              \n",
    "    train_loss /= num_files\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 1 \n",
    "  File 1 \n",
    "    training loss:\t\t0.588965\n",
    "  File 2 \n",
    "    training loss:\t\t0.599399\n",
    "  File 3 \n",
    "    training loss:\t\t0.509065\n",
    "  File 4 \n",
    "    training loss:\t\t0.578905\n",
    "  File 5 \n",
    "    training loss:\t\t0.566761\n",
    "  training loss:\t\t0.568619\n",
    "Epoch 2 \n",
    "  File 1 \n",
    "    training loss:\t\t0.464421\n",
    "  File 2 \n",
    "    training loss:\t\t0.486734\n",
    "  File 3 \n",
    "    training loss:\t\t0.432946\n",
    "  File 4 \n",
    "    training loss:\t\t0.476401\n",
    "  File 5 \n",
    "    training loss:\t\t0.494727\n",
    "  training loss:\t\t0.471046\n",
    "Epoch 3 \n",
    "  File 1 \n",
    "    training loss:\t\t0.448270\n",
    "  File 2 \n",
    "    training loss:\t\t0.484055\n",
    "  File 3 \n",
    "    training loss:\t\t0.431769\n",
    "  File 4 \n",
    "    training loss:\t\t0.487850\n",
    "  File 5 \n",
    "    training loss:\t\t0.504944\n",
    "  training loss:\t\t0.471378"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed forward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_encode=100\n",
    "\n",
    "input_var = T.dmatrix('inputs')\n",
    "shape = (None, landmark.shape[1])\n",
    "net = {}\n",
    "net['input'] = layers.InputLayer(shape=shape, input_var=input_var)\n",
    "net['dense1'] = layers.DenseLayer(net['input'], num_units=5000, W=init.GlorotUniform(), \n",
    "                                  b=init.Constant(.0), nonlinearity=None)\n",
    "net['dense1_active'] = layers.NonlinearityLayer(net['dense1'], nonlinearity=nonlinearities.rectify)\n",
    "#net['dense1_active'] = layers.ParametricRectifierLayer(net['dense1_norm'])\n",
    "#net['dense1_droput'] = layers.DropoutLayer(net['dense1_active'], p=0.5)\n",
    "\n",
    "\n",
    "net['dense2'] = layers.DenseLayer(net['dense1_active'], num_units=nonlandmark.shape[1], W=init.GlorotUniform(), \n",
    "                                  b=init.Constant(.0), nonlinearity=None)\n",
    "net['dense2_active'] = layers.NonlinearityLayer(net['dense2'], nonlinearity=nonlinearities.linear)\n",
    "#net['dense2_active'] = layers.ParametricRectifierLayer(net['dense2_norm'])\n",
    "#net['dense2_droput'] = layers.DropoutLayer(net['dense2_active'], p=0.5)\n",
    "\n",
    "net['output'] = net['dense2_active']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_var = T.dmatrix('inputs')\n",
    "prediction = get_output(net['output'], deterministic=False )\n",
    "loss = objectives.squared_error(target_var, prediction)\n",
    "loss = objectives.aggregate(loss, mode='mean')\n",
    "\n",
    "#l1_penalty = regularization.regularize_network_params(net, regularization.l1) * 1e-5\n",
    "#loss += l1_penalty\n",
    "#l2_penalty = regularization.regularize_network_params(net, regularization.l2) * 1e-6    \n",
    "#loss += l2_penalty \n",
    "    \n",
    "# ADAM updates\n",
    "params = get_all_params(net['output'], trainable=True)\n",
    "update_op = updates.adam(loss, params, learning_rate=1e-3)\n",
    "\n",
    "train_fun = theano.function([input_var, target_var], loss , updates=update_op, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \n",
      "  File 1 \n",
      "    training loss:\t\t1.121686\n",
      "  File 2 \n",
      "    training loss:\t\t2.193375\n",
      "  File 3 \n",
      "    training loss:\t\t11.844916\n",
      "  File 4 \n",
      "    training loss:\t\t4.190112\n",
      "  File 5 \n",
      "    training loss:\t\t1.010566\n",
      "  training loss:\t\t4.072131\n",
      "Epoch 2 \n",
      "  File 1 \n",
      "    training loss:\t\t203.215020\n",
      "  File 2 \n"
     ]
    }
   ],
   "source": [
    "def batch_generator(X, y, batch_size=128):\n",
    "    for start_idx in range(0, len(X)-batch_size+1, batch_size):\n",
    "        excerpt = slice(start_idx, start_idx+batch_size)\n",
    "        yield X[excerpt], y[excerpt]\n",
    "\n",
    "batch_size = 100        \n",
    "num_samples = 20000\n",
    "num_files = 5        \n",
    "num_epochs = 60    \n",
    "num_batches = num_samples // batch_size\n",
    "for epoch in range(num_epochs):\n",
    "    sys.stdout.write(\"\\rEpoch %d \\n\"%(epoch+1))\n",
    "\n",
    "    train_loss = 0\n",
    "    for i in range(num_files):\n",
    "        sys.stdout.write(\"\\r  File %d \\n\"%(i+1))\n",
    "        landmark= np.array(trainmat['landmark'+str(i)]).astype(np.float32)\n",
    "        nonlandmark = np.array(trainmat['nonlandmark'+str(i)]).astype(np.float32)\n",
    "        batches = batch_generator(landmark, nonlandmark, batch_size)\n",
    "        \n",
    "        loss = 0\n",
    "        for i in range(num_batches):\n",
    "            X, y = next(batches)\n",
    "            loss += train_fun(X, y)\n",
    "        print(\"    training loss:\\t\\t{:.6f}\".format(loss/num_batches))    \n",
    "        train_loss += loss/num_batches              \n",
    "    train_loss /= num_files\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VariationalSampleLayer(layers.MergeLayer):\n",
    "    def __init__(self, incoming_mu, incoming_logsigma, **kwargs):\n",
    "        super(VariationalSampleLayer, self).__init__(incomings=[incoming_mu, incoming_logsigma], **kwargs)\n",
    "        self.srng = RandomStreams(seed=234)\n",
    "\n",
    "    def get_output_shape_for(self, input_shapes):\n",
    "        return input_shapes[0]\n",
    "\n",
    "    def get_output_for(self, inputs, deterministic=False, **kwargs):\n",
    "        mu, logsigma = inputs\n",
    "        shape=(self.input_shapes[0][0] or inputs[0].shape[0],\n",
    "                self.input_shapes[0][1] or inputs[0].shape[1])\n",
    "        if deterministic:\n",
    "            return mu\n",
    "        return mu + T.exp(logsigma) * self.srng.normal(shape, avg=0.0, std=1).astype(theano.config.floatX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_encode=100\n",
    "\n",
    "input_var = T.dmatrix('inputs')\n",
    "shape = (None, landmark.shape[1])\n",
    "net = {}\n",
    "net['input'] = layers.InputLayer(shape=shape, input_var=input_var)\n",
    "net['encode1'] = layers.DenseLayer(net['input'], num_units=600, W=init.GlorotUniform(), \n",
    "                                  b=init.Constant(.0), nonlinearity=None)\n",
    "net['encode1_active'] = layers.NonlinearityLayer(net['encode1'], nonlinearity=nonlinearities.rectify)\n",
    "#net['encode1_active'] = layers.ParametricRectifierLayer(net['encode1_norm'])\n",
    "net['encode1_dropout'] = layers.DropoutLayer(net['encode1_active'], p=0.5)\n",
    "\n",
    "net['encode2'] = layers.DenseLayer(net['encode1_dropout'], num_units=300, W=init.GlorotUniform(), \n",
    "                                  b=init.Constant(.0), nonlinearity=None)\n",
    "#net['encode2_norm'] = layers.BatchNormLayer(net['encode2'])\n",
    "net['encode2_active'] = layers.NonlinearityLayer(net['encode2'], nonlinearity=nonlinearities.rectify)\n",
    "#net['encode2_active'] = layers.ParametricRectifierLayer(net['encode2_norm'])\n",
    "net['encode2_dropout'] = layers.DropoutLayer(net['encode2_active'], p=0.5)\n",
    "\n",
    "\n",
    "net['encode_mu'] = layers.DenseLayer(net['encode2_dropout'], num_units=num_encode, W=init.GlorotUniform(), \n",
    "                                  b=init.Constant(.0), nonlinearity=nonlinearities.linear)\n",
    "net['encode_logsigma'] = layers.DenseLayer(net['encode2_dropout'], num_units=num_encode, W=init.GlorotUniform(), \n",
    "                                  b=init.Constant(.0), nonlinearity=nonlinearities.linear)\n",
    "net['Z'] = VariationalSampleLayer(net['encode_mu'], net['encode_logsigma'])\n",
    "\n",
    "\n",
    "net['dencode1'] = layers.DenseLayer(net['Z'], num_units=300, W=init.GlorotUniform(), \n",
    "                                  b=init.Constant(.0), nonlinearity=None)\n",
    "#net['dencode1_norm'] = layers.BatchNormLayer(net['dencode1'])\n",
    "net['dencode1_active'] = layers.NonlinearityLayer(net['dencode1'], nonlinearity=nonlinearities.rectify)\n",
    "#net['dencode1_active'] = layers.ParametricRectifierLayer(net['dencode1_norm'])\n",
    "net['dencode1_dropout'] = layers.DropoutLayer(net['dencode1_active'], p=0.5)\n",
    "\n",
    "\n",
    "net['dencode2'] = layers.DenseLayer(net['dencode1_dropout'], num_units=300, W=init.GlorotUniform(), \n",
    "                                  b=init.Constant(.0), nonlinearity=None)\n",
    "#net['dencode2_norm'] = layers.BatchNormLayer(net['dencode2'])\n",
    "net['dencode2_active'] = layers.NonlinearityLayer(net['dencode2'], nonlinearity=nonlinearities.rectify)\n",
    "#net['dencode2_active'] = layers.ParametricRectifierLayer(net['dencode2_norm'])\n",
    "net['dencode2_dropout'] = layers.DropoutLayer(net['dencode2_active'], p=0.5)\n",
    "\n",
    "\n",
    "net['X'] = layers.DenseLayer(net['dencode2_dropout'], num_units=nonlandmark.shape[1],  W=init.GlorotUniform(), \n",
    "                                  b=init.Constant(.0), nonlinearity=nonlinearities.linear)\n",
    "#net['decode_logsigma'] = layers.DenseLayer(net['decode2'], num_units=x_dim, nonlinearity=nonlinearities.linear)\n",
    "#net['X'] = VariationalSampleLayer(net['decode_mu'], net['decode_logsigma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_loss(net, target_var, deterministic):\n",
    "    z_mu = get_output(net['encode_mu'], deterministic=deterministic)\n",
    "    z_logsigma = get_output(net['encode_logsigma'], deterministic=deterministic)\n",
    "    x_mu = get_output(net['X'], deterministic=deterministic)\n",
    "    #x_mu = T.clip(x_mu, 1e-7, 1-1e-7)\n",
    "    x_logsigma = 1.# use real std   \n",
    "    kl_divergence = 0.5*T.sum(1 + 2*z_logsigma - T.sqr(z_mu) - T.exp(2*z_logsigma), axis=1)\n",
    "    log_likelihood = T.sum(-0.5*T.log(2*np.float32(np.pi))- x_logsigma - 0.5*T.sqr(target_var-x_mu)/T.exp(2*x_logsigma),axis=1)\n",
    "    variational_lower_bound = -log_likelihood - kl_divergence\n",
    "    prediction = x_mu\n",
    "    return variational_lower_bound.mean()\n",
    "\n",
    "target_var = T.dmatrix('inputs')\n",
    "train_loss = build_loss(net, target_var, deterministic=False)\n",
    "test_loss = build_loss(net, target_var, deterministic=True)\n",
    "\n",
    "# ADAM updates\n",
    "params = get_all_params(net['X'], trainable=True)\n",
    "update_op = updates.adam(train_loss, params, learning_rate=1e-3)\n",
    "\n",
    "train_fun = theano.function([input_var, target_var], train_loss , updates=update_op, allow_input_downcast=True)\n",
    "valid_fun = theano.function([input_var, target_var], test_loss, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size=128):\n",
    "    for start_idx in range(0, len(X)-batch_size+1, batch_size):\n",
    "        excerpt = slice(start_idx, start_idx+batch_size)\n",
    "        yield X[excerpt], y[excerpt]\n",
    "\n",
    "batch_size = 100        \n",
    "num_samples = 20000\n",
    "num_files = 5        \n",
    "num_epochs = 60    \n",
    "num_batches = num_samples // batch_size\n",
    "for epoch in range(num_epochs):\n",
    "    sys.stdout.write(\"\\rEpoch %d \\n\"%(epoch+1))\n",
    "\n",
    "    train_loss = 0\n",
    "    for i in range(num_files):\n",
    "        sys.stdout.write(\"\\r  File %d \\n\"%(i+1))\n",
    "        landmark= np.array(trainmat['landmark'+str(i)]).astype(np.float32)\n",
    "        nonlandmark = np.array(trainmat['nonlandmark'+str(i)]).astype(np.float32)\n",
    "        landmark, nonlandmark = normalize_data(landmark, nonlandmark, mean_landmark, std_landmark, mean_nonlandmark, std_nonlandmark, num_samples)\n",
    "        batches = batch_generator(landmark, nonlandmark, batch_size)\n",
    "        \n",
    "        loss = 0\n",
    "        for i in range(num_batches):\n",
    "            X, y = next(batches)\n",
    "            loss += train_fun(X, y)\n",
    "        print(\"    training loss:\\t\\t{:.6f}\".format(loss/num_batches))    \n",
    "        train_loss += loss/num_batches              \n",
    "    train_loss /= num_files\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def statistics(net, target_var):\n",
    "    z_mu = get_output(net['encode_mu'], deterministic=True)\n",
    "    z_logsigma = get_output(net['encode_logsigma'], deterministic=True)\n",
    "    x_mu = get_output(net['X'], deterministic=True)\n",
    "    x_mu = T.clip(x_mu, 1e-7, 1-1e-7)\n",
    "    x_logsigma = T.log(T.sqrt(x_mu*(1-x_mu)))\n",
    "    kl_divergence = 0.5*(1 + 2*z_logsigma - T.sqr(z_mu) - T.exp(2*z_logsigma))\n",
    "    log_likelihood = -0.5*T.log(2*np.float32(np.pi))- x_logsigma - 0.5*T.sqr(target_var-x_mu)/T.exp(2*x_logsigma)\n",
    "    return kl_divergence, log_likelihood\n",
    "\n",
    "kl_divergence, log_likelihood = statistics(net, input_var)\n",
    "statistics_fun = theano.function([input_var],[kl_divergence, log_likelihood])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 25\n",
    "num_test_batches = X_valid.shape[0] // batch_size\n",
    "test_batches = batch_generator(X_valid, batch_size, shuffle=False)\n",
    "\n",
    "kldiv = []\n",
    "logL = []\n",
    "for index in range(num_test_batches):\n",
    "    stats = statistics_fun(next(test_batches))\n",
    "    kldiv.append(stats[0])\n",
    "    logL.append(stats[1])\n",
    "\n",
    "kldiv = np.array(kldiv).reshape([-1,stats[0].shape[1]])\n",
    "logL = np.array(logL).reshape([-1,stats[1].shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure\n",
    "plt.errorbar(range(num_encode),-np.mean(kldiv,axis=0),np.std(kldiv,axis=0), fmt='o', markersize=12)\n",
    "plt.xlim([-.3, num_encode-.7])\n",
    "plt.xlabel('z', fontsize=18)\n",
    "plt.ylabel('-KLD', fontsize=18)\n",
    "#plt.yticks([0, 1, 2, 3], fontsize = 18)\n",
    "plt.xticks(fontsize = 18)\n",
    "plt.savefig('Genes_vae_kldivergence_'+ str(num_encode) +'.eps', format='eps', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# denoising autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_encode=100\n",
    "\n",
    "input_var = T.dmatrix('inputs')\n",
    "shape = (None, num_landmark)\n",
    "net = {}\n",
    "net['input'] = layers.InputLayer(shape=shape, input_var=input_var)\n",
    "net['corrupt1'] = layers.GaussianNoiseLayer(net['input'], sigma=0.1)\n",
    "net['encode1'] = layers.DenseLayer(net['corrupt1'], num_units=600, W=init.GlorotUniform(), \n",
    "                                  b=init.Constant(.0), nonlinearity=None)\n",
    "net['encode1_active'] = layers.NonlinearityLayer(net['encode1'], nonlinearity=nonlinearities.rectify)\n",
    "#net['encode1_active'] = layers.ParametricRectifierLayer(net['encode1_norm'])\n",
    "\n",
    "\n",
    "net['corrupt2'] = layers.GaussianNoiseLayer(net['encode1_active'], sigma=0.1)\n",
    "net['encode2'] = layers.DenseLayer(net['corrupt2'], num_units=300, W=init.GlorotUniform(), \n",
    "                                  b=init.Constant(.0), nonlinearity=None)\n",
    "net['encode2_active'] = layers.NonlinearityLayer(net['encode2'], nonlinearity=nonlinearities.rectify)\n",
    "#net['encode2_active'] = layers.ParametricRectifierLayer(net['encode2_norm'])\n",
    "\n",
    "net['encode_mu'] = layers.DenseLayer(net['encode2_dropout'], num_units=num_encode, W=init.GlorotUniform(), \n",
    "                                  b=init.Constant(.0), nonlinearity=nonlinearities.linear)\n",
    "net['encode_logsigma'] = layers.DenseLayer(net['encode2_dropout'], num_units=num_encode, W=init.GlorotUniform(), \n",
    "                                  b=init.Constant(.0), nonlinearity=nonlinearities.linear)\n",
    "net['Z'] = VariationalSampleLayer(net['encode_mu'], net['encode_logsigma'])\n",
    "\n",
    "\n",
    "net['corrupt3'] = layers.GaussianNoiseLayer(net['Z'], sigma=0.1)\n",
    "net['dencode1'] = layers.DenseLayer(net['corrupt3'], num_units=300, W=init.GlorotUniform(), \n",
    "                                  b=init.Constant(.0), nonlinearity=None)\n",
    "net['dencode1_active'] = layers.NonlinearityLayer(net['dencode1'], nonlinearity=nonlinearities.rectify)\n",
    "#net['dencode1_active'] = layers.ParametricRectifierLayer(net['dencode1_norm'])\n",
    "\n",
    "\n",
    "net['corrupt4'] = layers.GaussianNoiseLayer(net['dencode1_active'], sigma=0.1)\n",
    "net['dencode2'] = layers.DenseLayer(net['corrupt4'], num_units=300, W=init.GlorotUniform(), \n",
    "                                  b=init.Constant(.0), nonlinearity=None)\n",
    "net['dencode2_active'] = layers.NonlinearityLayer(net['dencode2'], nonlinearity=nonlinearities.rectify)\n",
    "#net['dencode2_active'] = layers.ParametricRectifierLayer(net['dencode2_norm'])\n",
    "\n",
    "\n",
    "net['X'] = layers.DenseLayer(net['dencode2_dropout'], num_units=num_nonlandmark,  W=init.GlorotUniform(), \n",
    "                                  b=init.Constant(.0), nonlinearity=nonlinearities.linear)\n",
    "#net['decode_logsigma'] = layers.DenseLayer(net['decode2'], num_units=x_dim, nonlinearity=nonlinearities.linear)\n",
    "#net['X'] = VariationalSampleLayer(net['decode_mu'], net['decode_logsigma'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
