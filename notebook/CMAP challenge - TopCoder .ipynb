{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 980 (CNMeM is disabled, cuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "import os, sys, gzip\n",
    "import cPickle as pickle\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1004)   # for reproducibility\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "from scipy.misc import imresize\n",
    "\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "from lasagne import layers, nonlinearities, updates, objectives, init, regularization\n",
    "from lasagne.layers import get_output, get_output_shape, get_all_params\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "#from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datapath='/home/peter/Data/CMAP/training.csv'\n",
    "savepath='/home/peter/Data/CMAP/data.hd5f'\n",
    "\n",
    "f = h5py.File(savepath, \"w\")\n",
    "batch_size = 30000\n",
    "shuffle_index = np.random.permutation(100000)\n",
    "for i in range(3):\n",
    "    batch_index =range(i*batch_size, (i+1)*batch_size)\n",
    "    data = pd.read_csv(datapath, usecols=shuffle_index[batch_index], header=None, dtype=np.float32)\n",
    "    genes = data.as_matrix()\n",
    "    num_landmark = 970\n",
    "    num_nonlandmark = 11350\n",
    "    num_samples = genes.shape[1]\n",
    "    landmark = genes[:970,:]\n",
    "    nonlandmark = genes[970:,:]\n",
    "    del genes\n",
    "\n",
    "    dset = f.create_dataset(\"landmark\"+str(i), data=landmark)\n",
    "    dset = f.create_dataset(\"nonlandmark\"+str(i), data=nonlandmark)\n",
    "    del landmark\n",
    "    del nonlandmark\n",
    "\n",
    "batch_index =range(3*batch_size, 100000)\n",
    "data = pd.read_csv(datapath, usecols=shuffle_index[batch_index], header=None, dtype=np.float32)\n",
    "genes = data.as_matrix()\n",
    "num_landmark = 970\n",
    "num_nonlandmark = 11350\n",
    "num_samples = genes.shape[1]\n",
    "landmark = genes[:970,:]\n",
    "nonlandmark = genes[970:,:]\n",
    "del genes\n",
    "\n",
    "dset = f.create_dataset(\"landmark3\", data=landmark)\n",
    "dset = f.create_dataset(\"nonlandmark3\", data=nonlandmark)\n",
    "del landmark\n",
    "del nonlandmark\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def normalize_data(landmark, mean_landmark, std_landmark, num_samples):\n",
    "    landmark = (landmark - np.outer(mean_landmark,np.ones(num_samples)))/np.outer(std_landmark,np.ones(num_samples))\n",
    "    landmark = landmark.transpose([1,0])\n",
    "    \n",
    "    return landmark\n",
    "\n",
    "\n",
    "filepath='/home/peter/Data/CMAP/data.hd5f'\n",
    "trainmat = h5py.File(filepath, 'r')\n",
    "landmark= np.array(trainmat['landmark1']).astype(np.float32)\n",
    "nonlandmark = np.array(trainmat['nonlandmark1']).astype(np.float32)\n",
    "\n",
    "num_files = 4\n",
    "mean_landmark = 0\n",
    "std_landmark = 0\n",
    "mean_nonlandmark = 0\n",
    "std_nonlandmark = 0\n",
    "for i in range(num_files):\n",
    "    landmark= np.array(trainmat['landmark'+str(i)]).astype(np.float32)\n",
    "    nonlandmark = np.array(trainmat['nonlandmark'+str(i)]).astype(np.float32)\n",
    "    mean_landmark += np.mean(landmark, axis=1)\n",
    "    std_landmark += np.std(landmark,axis=1)\n",
    "    mean_nonlandmark += np.mean(nonlandmark, axis=1)\n",
    "    std_nonlandmark += np.std(nonlandmark, axis=1)\n",
    "\n",
    "mean_landmark /= num_files\n",
    "std_landmark /= num_files\n",
    "mean_nonlandmark /= num_files\n",
    "std_nonlandmark /= num_files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_landmark = (mean_landmark + np.mean(landmark,axis=1))/2\n",
    "std_landmark = (std_landmark + np.std(landmark, axis=1))/2\n",
    "\n",
    "savepath='/home/peter/Data/CMAP/data_norm.hd5f'\n",
    "f = h5py.File(savepath, \"w\")\n",
    "for i in range(4):\n",
    "    landmark = np.array(trainmat['landmark'+str(i)]).astype(np.float32)\n",
    "    nonlandmark = np.array(trainmat['nonlandmark'+str(i)]).astype(np.float32)\n",
    "    landmark = normalize_data(landmark, mean_landmark, std_landmark, landmark.shape[1])\n",
    "    dset = f.create_dataset(\"landmark\"+str(i), data=landmark)\n",
    "    dset = f.create_dataset(\"nonlandmark\"+str(i), data=nonlandmark)\n",
    "dset = f.create_dataset(\"mean_landmark\", data=mean_landmark)\n",
    "dset = f.create_dataset(\"std_landmark\", data=std_landmark)\n",
    "dset = f.create_dataset(\"mean_nonlandmark\", data=mean_nonlandmark)\n",
    "dset = f.create_dataset(\"std_nonlandmark\", data=std_nonlandmark)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filepath='/home/peter/Data/CMAP/dataset_norm.hd5f'\n",
    "trainmat = h5py.File(filepath, 'r')\n",
    "landmark= np.array(trainmat['landmark4']).astype(np.float32)\n",
    "nonlandmark = np.array(trainmat['nonlandmark4']).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = 2\n",
    "plt.hist(landmark[:,index]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C = np.cov(nonlandmark.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(C)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_var = T.dmatrix('inputs')\n",
    "shape = (None, landmark.shape[1])\n",
    "net = {}\n",
    "net['input'] = layers.InputLayer(shape=shape, input_var=input_var)\n",
    "net['dense1'] = layers.DenseLayer(net['input'], num_units=nonlandmark.shape[1], W=init.GlorotUniform(), \n",
    "                                  b=init.Constant(.0), nonlinearity=None)\n",
    "net['dense1_active'] = layers.NonlinearityLayer(net['dense1'], nonlinearity=nonlinearities.linear)\n",
    "net['output'] = net['dense1_active']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_var = T.dmatrix('inputs')\n",
    "prediction = get_output(net['output'], deterministic=False )\n",
    "loss = objectives.squared_error(target_var, prediction)\n",
    "loss = objectives.aggregate(loss, mode='mean')\n",
    "\n",
    "# ADAM updates\n",
    "params = get_all_params(net['output'], trainable=True)\n",
    "update_op = updates.adam(loss, params, learning_rate=1e-3)\n",
    "\n",
    "train_fun = theano.function([input_var, target_var], loss , updates=update_op, allow_input_downcast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size=128):\n",
    "    for start_idx in range(0, len(X)-batch_size+1, batch_size):\n",
    "        excerpt = slice(start_idx, start_idx+batch_size)\n",
    "        yield X[excerpt], y[excerpt]\n",
    "\n",
    "batch_size = 100        \n",
    "num_samples = 20000\n",
    "num_files = 5        \n",
    "num_epochs = 60    \n",
    "num_batches = num_samples // batch_size\n",
    "for epoch in range(num_epochs):\n",
    "    sys.stdout.write(\"\\rEpoch %d \\n\"%(epoch+1))\n",
    "\n",
    "    train_loss = 0\n",
    "    for i in range(num_files):\n",
    "        sys.stdout.write(\"\\r  File %d \\n\"%(i+1))\n",
    "        landmark= np.array(trainmat['landmark'+str(i)]).astype(np.float32)\n",
    "        nonlandmark = np.array(trainmat['nonlandmark'+str(i)]).astype(np.float32)\n",
    "        batches = batch_generator(landmark, nonlandmark, batch_size)\n",
    "        \n",
    "        loss = 0\n",
    "        for i in range(num_batches):\n",
    "            X, y = next(batches)\n",
    "            loss += train_fun(X, y)\n",
    "        print(\"    training loss:\\t\\t{:.6f}\".format(loss/num_batches))    \n",
    "        train_loss += loss/num_batches              \n",
    "    train_loss /= num_files\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 1 \n",
    "  File 1 \n",
    "    training loss:\t\t0.588965\n",
    "  File 2 \n",
    "    training loss:\t\t0.599399\n",
    "  File 3 \n",
    "    training loss:\t\t0.509065\n",
    "  File 4 \n",
    "    training loss:\t\t0.578905\n",
    "  File 5 \n",
    "    training loss:\t\t0.566761\n",
    "  training loss:\t\t0.568619\n",
    "Epoch 2 \n",
    "  File 1 \n",
    "    training loss:\t\t0.464421\n",
    "  File 2 \n",
    "    training loss:\t\t0.486734\n",
    "  File 3 \n",
    "    training loss:\t\t0.432946\n",
    "  File 4 \n",
    "    training loss:\t\t0.476401\n",
    "  File 5 \n",
    "    training loss:\t\t0.494727\n",
    "  training loss:\t\t0.471046\n",
    "Epoch 3 \n",
    "  File 1 \n",
    "    training loss:\t\t0.448270\n",
    "  File 2 \n",
    "    training loss:\t\t0.484055\n",
    "  File 3 \n",
    "    training loss:\t\t0.431769\n",
    "  File 4 \n",
    "    training loss:\t\t0.487850\n",
    "  File 5 \n",
    "    training loss:\t\t0.504944\n",
    "  training loss:\t\t0.471378"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed forward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_encode=100\n",
    "\n",
    "input_var = T.dmatrix('inputs')\n",
    "shape = (None, landmark.shape[1])\n",
    "net = {}\n",
    "net['input'] = layers.InputLayer(shape=shape, input_var=input_var)\n",
    "net['dense1'] = layers.DenseLayer(net['input'], num_units=5000, W=init.GlorotUniform(), \n",
    "                                  b=init.Constant(.0), nonlinearity=None)\n",
    "net['dense1_active'] = layers.NonlinearityLayer(net['dense1'], nonlinearity=nonlinearities.rectify)\n",
    "#net['dense1_active'] = layers.ParametricRectifierLayer(net['dense1_norm'])\n",
    "#net['dense1_droput'] = layers.DropoutLayer(net['dense1_active'], p=0.5)\n",
    "\n",
    "\n",
    "net['dense2'] = layers.DenseLayer(net['dense1_active'], num_units=nonlandmark.shape[1], W=init.GlorotUniform(), \n",
    "                                  b=init.Constant(.0), nonlinearity=None)\n",
    "net['dense2_active'] = layers.NonlinearityLayer(net['dense2'], nonlinearity=nonlinearities.linear)\n",
    "#net['dense2_active'] = layers.ParametricRectifierLayer(net['dense2_norm'])\n",
    "#net['dense2_droput'] = layers.DropoutLayer(net['dense2_active'], p=0.5)\n",
    "\n",
    "net['output'] = net['dense2_active']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_var = T.dmatrix('inputs')\n",
    "prediction = get_output(net['output'], deterministic=False )\n",
    "loss = objectives.squared_error(target_var, prediction)\n",
    "loss = objectives.aggregate(loss, mode='mean')\n",
    "\n",
    "#l1_penalty = regularization.regularize_network_params(net, regularization.l1) * 1e-5\n",
    "#loss += l1_penalty\n",
    "#l2_penalty = regularization.regularize_network_params(net, regularization.l2) * 1e-6    \n",
    "#loss += l2_penalty \n",
    "    \n",
    "# ADAM updates\n",
    "params = get_all_params(net['output'], trainable=True)\n",
    "update_op = updates.adam(loss, params, learning_rate=1e-3)\n",
    "\n",
    "train_fun = theano.function([input_var, target_var], loss , updates=update_op, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size=128):\n",
    "    for start_idx in range(0, len(X)-batch_size+1, batch_size):\n",
    "        excerpt = slice(start_idx, start_idx+batch_size)\n",
    "        yield X[excerpt], y[excerpt]\n",
    "\n",
    "batch_size = 100        \n",
    "num_samples = 20000\n",
    "num_files = 5        \n",
    "num_epochs = 60    \n",
    "num_batches = num_samples // batch_size\n",
    "for epoch in range(num_epochs):\n",
    "    sys.stdout.write(\"\\rEpoch %d \\n\"%(epoch+1))\n",
    "\n",
    "    train_loss = 0\n",
    "    for i in range(num_files):\n",
    "        sys.stdout.write(\"\\r  File %d \\n\"%(i+1))\n",
    "        landmark= np.array(trainmat['landmark'+str(i)]).astype(np.float32)\n",
    "        nonlandmark = np.array(trainmat['nonlandmark'+str(i)]).astype(np.float32)\n",
    "        batches = batch_generator(landmark, nonlandmark, batch_size)\n",
    "        \n",
    "        loss = 0\n",
    "        for i in range(num_batches):\n",
    "            X, y = next(batches)\n",
    "            loss += train_fun(X, y)\n",
    "        print(\"    training loss:\\t\\t{:.6f}\".format(loss/num_batches))    \n",
    "        train_loss += loss/num_batches              \n",
    "    train_loss /= num_files\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# denoising autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_var = T.dmatrix('inputs')\n",
    "shape = (None, landmark.shape[1])\n",
    "net = {}\n",
    "net['input'] = layers.InputLayer(shape=shape, input_var=input_var)\n",
    "\n",
    "# encode layer 1\n",
    "net['corrupt1'] = layers.GaussianNoiseLayer(net['input'], sigma=0.1)\n",
    "net['encode1'] = layers.DenseLayer(net['corrupt1'], num_units=2000, W=init.GlorotUniform(), \n",
    "                                  b=init.Constant(.0), nonlinearity=None)\n",
    "net['encode1_active'] = layers.NonlinearityLayer(net['encode1'], nonlinearity=nonlinearities.rectify)\n",
    "\n",
    "# encode layer 2\n",
    "net['corrupt2'] = layers.GaussianNoiseLayer(net['encode1_active'], sigma=0.1)\n",
    "net['encode2'] = layers.DenseLayer(net['corrupt2'], num_units=4000, W=init.GlorotUniform(), \n",
    "                                  b=init.Constant(.0), nonlinearity=None)\n",
    "net['encode2_active'] = layers.NonlinearityLayer(net['encode2'], nonlinearity=nonlinearities.rectify)\n",
    "\n",
    "# encode layer\n",
    "net['encode'] = layers.DenseLayer(net['encode2_active'], num_units=nonlandmark.shape[1], W=init.GlorotUniform(), \n",
    "                                  b=init.Constant(.0), nonlinearity=None)\n",
    "net['encode_active'] = layers.NonlinearityLayer(net['encode'], nonlinearity=nonlinearities.linear)\n",
    "\n",
    "# decode layer\n",
    "net['decode'] = layers.DenseLayer(net['encode_active'], num_units=4000, W=net['encode'].W.T, \n",
    "                                  b=init.Constant(.0), nonlinearity=None)\n",
    "net['decode_active'] = layers.NonlinearityLayer(net['decode'], nonlinearity=nonlinearities.rectify)\n",
    "\n",
    "# decode layer 1\n",
    "net['decode1'] = layers.DenseLayer(net['decode_active'], num_units=2000, W=net['encode2'].W.T, \n",
    "                                  b=init.Constant(.0), nonlinearity=None)\n",
    "net['decode1_active'] = layers.NonlinearityLayer(net['decode1'], nonlinearity=nonlinearities.rectify)\n",
    "\n",
    "# decode layer 2\n",
    "net['decode2'] = layers.DenseLayer(net['decode1_active'], num_units=landmark.shape[1], W=net['encode1'].W.T, \n",
    "                                  b=init.Constant(.0), nonlinearity=None)\n",
    "net['decode2_active'] = layers.NonlinearityLayer(net['decode2'], nonlinearity=nonlinearities.linear)\n",
    "net['output'] = net['decode2_active']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = get_output(net['output'], deterministic=False )\n",
    "loss1 = objectives.squared_error(input_var, prediction)\n",
    "loss1 = objectives.aggregate(loss1, mode='mean')\n",
    "\n",
    "target_var = T.dmatrix('inputs')\n",
    "prediction = get_output(net['encode_active'], deterministic=False )\n",
    "loss2 = objectives.squared_error(target_var, prediction)\n",
    "loss2 = objectives.aggregate(loss2, mode='mean')\n",
    "\n",
    "loss = loss1 + loss2\n",
    "\n",
    "#l1_penalty = regularization.regularize_network_params(net, regularization.l1) * 1e-5\n",
    "#loss += l1_penalty\n",
    "#l2_penalty = regularization.regularize_network_params(net, regularization.l2) * 1e-6    \n",
    "#loss += l2_penalty \n",
    "    \n",
    "# ADAM updates\n",
    "params = get_all_params(net['output'], trainable=True)\n",
    "update_op = updates.adam(loss, params, learning_rate=1e-3)\n",
    "\n",
    "train_fun = theano.function([input_var, target_var], loss , updates=update_op, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size=128):\n",
    "    for start_idx in range(0, len(X)-batch_size+1, batch_size):\n",
    "        excerpt = slice(start_idx, start_idx+batch_size)\n",
    "        yield X[excerpt], y[excerpt]\n",
    "\n",
    "batch_size = 100        \n",
    "num_samples = 20000\n",
    "num_files = 5        \n",
    "num_epochs = 60    \n",
    "num_batches = num_samples // batch_size\n",
    "for epoch in range(num_epochs):\n",
    "    sys.stdout.write(\"\\rEpoch %d \\n\"%(epoch+1))\n",
    "\n",
    "    train_loss = 0\n",
    "    for i in range(num_files):\n",
    "        sys.stdout.write(\"\\r  File %d \\n\"%(i+1))\n",
    "        landmark= np.array(trainmat['landmark'+str(i)]).astype(np.float32)\n",
    "        nonlandmark = np.array(trainmat['nonlandmark'+str(i)]).astype(np.float32)\n",
    "        batches = batch_generator(landmark, nonlandmark, batch_size)\n",
    "        \n",
    "        loss = 0\n",
    "        for j in range(num_batches):\n",
    "            X, y = next(batches)\n",
    "            loss += train_fun(X, y)\n",
    "        print(\"    training loss:\\t\\t{:.6f}\".format(loss/num_batches))    \n",
    "        train_loss += loss/num_batches              \n",
    "    train_loss /= num_files\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 980 (CNMeM is disabled, cuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "import os, sys, gzip\n",
    "import cPickle as pickle\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1004)   # for reproducibility\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "from scipy.misc import imresize\n",
    "\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "from lasagne import layers, nonlinearities, updates, objectives, init, regularization\n",
    "from lasagne.layers import get_output, get_output_shape, get_all_params\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "#from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "savepath='/home/peter/Data/CMAP/dataset_norm.hd5f'\n",
    "trainmat = h5py.File(savepath, 'r')\n",
    "mean_landmark = np.array(trainmat['mean_landmark']).astype(np.float32)\n",
    "std_landmark = np.array(trainmat['std_landmark']).astype(np.float32)\n",
    "mean_nonlandmark = np.array(trainmat['mean_nonlandmark']).astype(np.float32)\n",
    "std_nonlandmark = np.array(trainmat['std_nonlandmark']).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(970, 1000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landmarkpath='/home/peter/Data/CMAP/test/landmarks.csv'\n",
    "data = pd.read_csv(landmarkpath, header=None, dtype=np.float32)\n",
    "landmark = data.as_matrix()\n",
    "landmark.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11350, 1000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonlandmarkpath='/home/peter/Data/CMAP/test/truth.csv'\n",
    "data = pd.read_csv(nonlandmarkpath, header=None, dtype=np.float32)\n",
    "nonlandmark = data.as_matrix()\n",
    "nonlandmark.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def normalize_data(landmark, nonlandmark, mean_landmark, std_landmark, mean_nonlandmark, std_nonlandmark, num_samples):\n",
    "    landmark = (landmark - np.outer(mean_landmark,np.ones(num_samples)))/np.outer(std_landmark,np.ones(num_samples))\n",
    "    nonlandmark = (nonlandmark - np.outer(mean_nonlandmark,np.ones(num_samples)))/np.outer(std_nonlandmark,np.ones(num_samples))\n",
    "    landmark = landmark.transpose([1,0])\n",
    "    nonlandmark = nonlandmark.transpose([1,0])\n",
    "\n",
    "    return landmark, nonlandmark\n",
    "\n",
    "landmark, nonlandmark = normalize_data(landmark, nonlandmark, mean_landmark, std_landmark, mean_nonlandmark, std_nonlandmark, landmark.shape[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "sys.path.append('..')\n",
    "from src import NeuralNet\n",
    "\n",
    "# build model\n",
    "model_name = \"CMAP_model\"\n",
    "nnmodel = NeuralNet(model_name, shape=[], num_labels=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weightpath = '/home/peter/Data/CMAP/Results/feedforward_epoch_4.pickle'\n",
    "nnmodel.set_parameters_from_file(weightpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encode': <lasagne.layers.dense.DenseLayer at 0x7fe8efabced0>,\n",
       " 'encode1': <lasagne.layers.dense.DenseLayer at 0x7fe8efab26d0>,\n",
       " 'encode1_active': <lasagne.layers.special.NonlinearityLayer at 0x7fe8efabc490>,\n",
       " 'encode1_norm': <models.CMAP_model.BatchNormLayer at 0x7fe8efab2650>,\n",
       " 'encode2': <lasagne.layers.dense.DenseLayer at 0x7fe8efabc550>,\n",
       " 'encode2_active': <lasagne.layers.special.NonlinearityLayer at 0x7fe8efabce10>,\n",
       " 'encode2_norm': <models.CMAP_model.BatchNormLayer at 0x7fe8f01326d0>,\n",
       " 'input': <lasagne.layers.input.InputLayer at 0x7fe8efab2550>,\n",
       " 'output': None}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = nnmodel.network\n",
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_var = nnmodel.input_var\n",
    "prediction = get_output(network['encode'], deterministic=True)\n",
    "\n",
    "get_prediction = theano.function([input_var], prediction, allow_input_downcast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = get_prediction(landmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71553492575064892"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = []\n",
    "for i in range(prediction.shape[1]):\n",
    "    R.append(stats.spearmanr(prediction[:,i], nonlandmark[:,i])[0])\n",
    "    \n",
    "np.mean(R)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEACAYAAAC+gnFaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCKwTQAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe8ee43ea10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(R,bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 11350)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = prediction.transpose([1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_samples = prediction.shape[1]\n",
    "prediction = (prediction*np.outer(std_nonlandmark,np.ones(num_samples))) + np.outer(mean_nonlandmark,np.ones(num_samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2.to_csv('/home/peter/Data/CMAP/test/prediction_ff.csv', header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Rscript cmap_scoring_function.R --inf_ds prediction_ff.csv \\\n",
    "    --truth_ds truth.csv \\\n",
    "    --reference_scores refScores.csv \\\n",
    "    --out here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11350, 1000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path='/home/peter/Data/CMAP/test/prediction_ff.csv'\n",
    "data = pd.read_csv(path, header=None, dtype=np.float32)\n",
    "test = data.as_matrix()\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11350, 1000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonlandmarkpath='/home/peter/Data/CMAP/test/truth.csv'\n",
    "data = pd.read_csv(nonlandmarkpath, header=None, dtype=np.float32)\n",
    "truth = data.as_matrix()\n",
    "truth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73756263297012703"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = []\n",
    "for i in range(prediction.shape[1]):\n",
    "    R.append(stats.spearmanr(prediction[i,:], truth[i,:])[0])\n",
    "np.mean(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
