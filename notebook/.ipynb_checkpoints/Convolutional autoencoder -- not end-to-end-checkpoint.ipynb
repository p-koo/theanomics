{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "sys.path.append('..')\n",
    "from src import NeuralNet\n",
    "from src import train as fit\n",
    "from src import make_directory \n",
    "from models import load_model\n",
    "from data import load_data\n",
    "from six.moves import cPickle\n",
    "np.random.seed(247) # for reproducibility\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'figure.autolayout': True})\n",
    "%matplotlib inline\n",
    "from scipy.misc import imresize\n",
    "\n",
    "from lasagne import layers, nonlinearities, updates, objectives, init \n",
    "from lasagne.layers import Conv2DLayer, TransposedConv2DLayer, DenseLayer, InputLayer, ExpressionLayer, BiasLayer\n",
    "\n",
    "from lasagne.layers import get_output, get_output_shape, get_all_params\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "np.random.seed(247) # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datapath = '/home/peter/Data/SequenceMotif'\n",
    "#name = 'MotifSimulation_binary'\n",
    "#filepath = os.path.join(datapath, 'N=300000_S=200_M=20_G=20_data.pickle')\n",
    "name = 'MotifSimulation_correlated'\n",
    "filepath = os.path.join(datapath, 'synthetic_random_motifs_100000_4.hdf5')\n",
    "\n",
    "train, valid, test = load_data(name, filepath)\n",
    "shape = (None, train[0].shape[1], train[0].shape[2], train[0].shape[3])\n",
    "num_labels = train[1].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = \"test_motif_model\"\n",
    "nnmodel = NeuralNet(model_name, shape, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=1.38121 -- accuracy=89.52%  \n",
      "  valid loss:\t\t0.93397\n",
      "  valid accuracy:\t0.91647+/-0.00386\n",
      "  valid auc-roc:\t0.83580+/-0.01029\n",
      "  valid auc-pr:\t\t0.56233+/-0.01864\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/synthetic_random_motifs_100000_4.hdf5_epoch_0.pickle\n",
      "Epoch 2 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.72383 -- accuracy=92.36%  \n",
      "  valid loss:\t\t0.53191\n",
      "  valid accuracy:\t0.93131+/-0.00633\n",
      "  valid auc-roc:\t0.84621+/-0.00995\n",
      "  valid auc-pr:\t\t0.58582+/-0.01760\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/synthetic_random_motifs_100000_4.hdf5_epoch_1.pickle\n",
      "Epoch 3 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.45094 -- accuracy=93.32%  \n",
      "  valid loss:\t\t0.36468\n",
      "  valid accuracy:\t0.93719+/-0.00245\n",
      "  valid auc-roc:\t0.85284+/-0.00983\n",
      "  valid auc-pr:\t\t0.59969+/-0.01800\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/synthetic_random_motifs_100000_4.hdf5_epoch_2.pickle\n",
      "Epoch 4 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.34120 -- accuracy=93.59%  \n",
      "  valid loss:\t\t0.30052\n",
      "  valid accuracy:\t0.94117+/-0.00237\n",
      "  valid auc-roc:\t0.85597+/-0.00992\n",
      "  valid auc-pr:\t\t0.60610+/-0.01817\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/synthetic_random_motifs_100000_4.hdf5_epoch_3.pickle\n",
      "Epoch 5 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.29638 -- accuracy=93.69%  \n",
      "  valid loss:\t\t0.26875\n",
      "  valid accuracy:\t0.94032+/-0.00230\n",
      "  valid auc-roc:\t0.85897+/-0.00989\n",
      "  valid auc-pr:\t\t0.61176+/-0.01845\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/synthetic_random_motifs_100000_4.hdf5_epoch_4.pickle\n",
      "Epoch 6 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.27466 -- accuracy=93.73%  \n",
      "  valid loss:\t\t0.25495\n",
      "  valid accuracy:\t0.94014+/-0.00237\n",
      "  valid auc-roc:\t0.86136+/-0.00990\n",
      "  valid auc-pr:\t\t0.61744+/-0.01835\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/synthetic_random_motifs_100000_4.hdf5_epoch_5.pickle\n",
      "Epoch 7 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.25954 -- accuracy=93.79%  \n",
      "  valid loss:\t\t0.24122\n",
      "  valid accuracy:\t0.94097+/-0.00227\n",
      "  valid auc-roc:\t0.86428+/-0.00989\n",
      "  valid auc-pr:\t\t0.62400+/-0.01814\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/synthetic_random_motifs_100000_4.hdf5_epoch_6.pickle\n",
      "Epoch 8 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.24955 -- accuracy=93.84%  \n",
      "  valid loss:\t\t0.23323\n",
      "  valid accuracy:\t0.94153+/-0.00220\n",
      "  valid auc-roc:\t0.86656+/-0.00947\n",
      "  valid auc-pr:\t\t0.63009+/-0.01739\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/synthetic_random_motifs_100000_4.hdf5_epoch_7.pickle\n",
      "Epoch 9 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.24187 -- accuracy=93.91%  \n",
      "  valid loss:\t\t0.22771\n",
      "  valid accuracy:\t0.94206+/-0.00210\n",
      "  valid auc-roc:\t0.86849+/-0.00931\n",
      "  valid auc-pr:\t\t0.63509+/-0.01679\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/synthetic_random_motifs_100000_4.hdf5_epoch_8.pickle\n",
      "Epoch 10 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.23618 -- accuracy=93.96%  \n",
      "  valid loss:\t\t0.22363\n",
      "  valid accuracy:\t0.94155+/-0.00212\n",
      "  valid auc-roc:\t0.87069+/-0.00928\n",
      "  valid auc-pr:\t\t0.63887+/-0.01645\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/synthetic_random_motifs_100000_4.hdf5_epoch_9.pickle\n",
      "Epoch 11 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.23295 -- accuracy=93.98%  \n",
      "  valid loss:\t\t0.22142\n",
      "  valid accuracy:\t0.94167+/-0.00210\n",
      "  valid auc-roc:\t0.87132+/-0.00896\n",
      "  valid auc-pr:\t\t0.64033+/-0.01618\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/synthetic_random_motifs_100000_4.hdf5_epoch_10.pickle\n",
      "Epoch 12 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.22926 -- accuracy=94.00%  \n",
      "  valid loss:\t\t0.21936\n",
      "  valid accuracy:\t0.94160+/-0.00202\n",
      "  valid auc-roc:\t0.87248+/-0.00918\n",
      "  valid auc-pr:\t\t0.64294+/-0.01623\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/synthetic_random_motifs_100000_4.hdf5_epoch_11.pickle\n",
      "Epoch 13 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.22689 -- accuracy=94.03%  \n",
      "  valid loss:\t\t0.21911\n",
      "  valid accuracy:\t0.94152+/-0.00205\n",
      "  valid auc-roc:\t0.87319+/-0.00882\n",
      "  valid auc-pr:\t\t0.64446+/-0.01582\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/synthetic_random_motifs_100000_4.hdf5_epoch_12.pickle\n",
      "Epoch 14 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.22529 -- accuracy=94.04%  \n",
      "  valid loss:\t\t0.22452\n",
      "  valid accuracy:\t0.94305+/-0.00200\n",
      "  valid auc-roc:\t0.87287+/-0.00872\n",
      "  valid auc-pr:\t\t0.64500+/-0.01524\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/synthetic_random_motifs_100000_4.hdf5_epoch_13.pickle\n",
      "Patience ran out... Early stopping.\n"
     ]
    }
   ],
   "source": [
    "nnmodel = fit.train_minibatch(nnmodel, train, valid, batch_size=128, num_epochs=500, \n",
    "                        patience=0, verbose=1, filepath=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#epoch = 111\n",
    "#savepath = filepath + \"_epoch_\" + str(epoch) + \".pickle\"\n",
    "#nnmodel.set_parameters_from_file(savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq_logo(pwm, height=100, nt_width=20, norm=0, rna=1, filepath='.'):\n",
    "    \"\"\"generate a sequence logo from a pwm\"\"\"\n",
    "    \n",
    "    def load_alphabet(filepath, rna):\n",
    "        \"\"\"load images of nucleotide alphabet \"\"\"\n",
    "        df = pd.read_table(os.path.join(filepath, 'A.txt'), header=None);\n",
    "        A_img = df.as_matrix()\n",
    "        A_img = np.reshape(A_img, [72, 65, 3], order=\"F\").astype(np.uint8)\n",
    "\n",
    "        df = pd.read_table(os.path.join(filepath, 'C.txt'), header=None);\n",
    "        C_img = df.as_matrix()\n",
    "        C_img = np.reshape(C_img, [76, 64, 3], order=\"F\").astype(np.uint8)\n",
    "\n",
    "        df = pd.read_table(os.path.join(filepath, 'G.txt'), header=None);\n",
    "        G_img = df.as_matrix()\n",
    "        G_img = np.reshape(G_img, [76, 67, 3], order=\"F\").astype(np.uint8)\n",
    "\n",
    "        if rna == 1:\n",
    "            df = pd.read_table(os.path.join(filepath, 'U.txt'), header=None);\n",
    "            T_img = df.as_matrix()\n",
    "            T_img = np.reshape(T_img, [74, 57, 3], order=\"F\").astype(np.uint8)\n",
    "        else:\n",
    "            df = pd.read_table(os.path.join(filepath, 'T.txt'), header=None);\n",
    "            T_img = df.as_matrix()\n",
    "            T_img = np.reshape(T_img, [72, 59, 3], order=\"F\").astype(np.uint8)\n",
    "\n",
    "        return A_img, C_img, G_img, T_img\n",
    "\n",
    "\n",
    "    def get_nt_height(pwm, height, norm):\n",
    "        \"\"\"get the heights of each nucleotide\"\"\"\n",
    "\n",
    "        def entropy(p):\n",
    "            \"\"\"calculate entropy of each nucleotide\"\"\"\n",
    "            s = 0\n",
    "            for i in range(4):\n",
    "                if p[i] > 0:\n",
    "                    s -= p[i]*np.log2(p[i])\n",
    "            return s\n",
    "\n",
    "        num_nt, num_seq = pwm.shape\n",
    "        heights = np.zeros((num_nt,num_seq));\n",
    "        for i in range(num_seq):\n",
    "            if norm == 1:\n",
    "                total_height = height\n",
    "            else:\n",
    "                total_height = (np.log2(4) - entropy(pwm[:, i]))*height;\n",
    "            heights[:,i] = np.floor(pwm[:,i]*total_height);\n",
    "        return heights.astype(int)\n",
    "\n",
    "    \n",
    "    # get the alphabet images of each nucleotide\n",
    "    A_img, C_img, G_img, T_img = load_alphabet(filepath='.', rna=1)\n",
    "    \n",
    "    \n",
    "    # get the heights of each nucleotide\n",
    "    heights = get_nt_height(pwm, height, norm)\n",
    "    \n",
    "    # resize nucleotide images for each base of sequence and stack\n",
    "    num_nt, num_seq = pwm.shape\n",
    "    width = np.ceil(nt_width*num_seq).astype(int)\n",
    "    \n",
    "    total_height = np.sum(heights,axis=0)\n",
    "    max_height = np.max(total_height)\n",
    "    logo = np.ones((height*2, width, 3)).astype(int)*255;\n",
    "    for i in range(num_seq):\n",
    "        remaining_height = total_height[i];\n",
    "        offset = max_height-remaining_height\n",
    "        nt_height = np.sort(heights[:,i]);\n",
    "        index = np.argsort(heights[:,i])\n",
    "\n",
    "        for j in range(num_nt):\n",
    "            # resized dimensions of image\n",
    "            resize = (nt_height[j], nt_width)\n",
    "            if index[j] == 0:\n",
    "                nt_img = imresize(A_img, resize)\n",
    "            elif index[j] == 1:\n",
    "                nt_img = imresize(C_img, resize)\n",
    "            elif index[j] == 2:\n",
    "                nt_img = imresize(G_img, resize)\n",
    "            elif index[j] == 3:\n",
    "                nt_img = imresize(T_img, resize)\n",
    "                \n",
    "            # determine location of image\n",
    "            height_range = range(remaining_height-nt_height[j], remaining_height)\n",
    "            width_range = range(i*nt_width, i*nt_width+nt_width)\n",
    "\n",
    "            # 'annoying' way to broadcast resized nucleotide image\n",
    "            if height_range:\n",
    "                for k in range(3):\n",
    "                    for m in range(len(width_range)):\n",
    "                        logo[height_range+offset, width_range[m],k] = nt_img[:,m,k];\n",
    "\n",
    "            remaining_height -= nt_height[j]\n",
    "\n",
    "    return logo.astype(np.uint8), heights\n",
    "\n",
    "\n",
    "\n",
    "def plot_conv_filter(plt, pwm, height=200, bp_width=100, norm=0, rna=1, adjust=-1, filepath='.', showbar=0):\n",
    "    num_seq = pwm.shape[1]\n",
    "    width = bp_width*num_seq\n",
    "\n",
    "    logo = seq_logo(pwm, height, width, norm, rna, filepath)\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1)\n",
    "    axes[0].imshow(logo, extent=[bp_width*2, width+bp_width, 0, height])\n",
    "    axes[0].set_axis_off()\n",
    "    im = axes[1].imshow(pwm, cmap='jet', vmin=0, vmax=1, interpolation='none') \n",
    "    axes[1].set_axis_off()\n",
    "    fig.subplots_adjust(bottom=adjust)\n",
    "    if showbar == 1:\n",
    "        cbar_ax = fig.add_axes([.85, 0.05, 0.05, 0.45])\n",
    "        cb = fig.colorbar(im, cax=cbar_ax, ticks=[0, 0.5, 1])\n",
    "        cb.ax.tick_params(labelsize=16)\n",
    "    return fig\n",
    "\n",
    "def fig_options(plt, options):\n",
    "    if 'figsize' in options:\n",
    "        fig = plt.gcf()\n",
    "        fig.set_size_inches(options['figsize'][0], options['figsize'][1], forward=True)\n",
    "    if 'ylim' in options:\n",
    "        plt.ylim(options['ylim'][0],options['ylim'][1])\n",
    "    if 'yticks' in options:\n",
    "        plt.yticks(options['yticks'])\n",
    "    if 'xticks' in options:\n",
    "        plt.xticks(options['xticks'])\n",
    "    if 'labelsize' in options:        \n",
    "        ax = plt.gca()\n",
    "        ax.tick_params(axis='x', labelsize=options['labelsize'])\n",
    "        ax.tick_params(axis='y', labelsize=options['labelsize'])\n",
    "    if 'axis' in options:\n",
    "        plt.axis(options['axis'])\n",
    "    if 'xlabel' in options:\n",
    "        plt.xlabel(options['xlabel'], fontsize=options['fontsize'])\n",
    "    if 'ylabel' in options:\n",
    "        plt.ylabel(options['ylabel'], fontsize=options['fontsize'])\n",
    "    if 'linewidth' in options:\n",
    "        plt.rc('axes', linewidth=options['linewidth'])\n",
    "        \n",
    "\n",
    "def subplot_grid(nrows, ncols):\n",
    "    grid= mpl.gridspec.GridSpec(nrows, ncols)\n",
    "    grid.update(wspace=0.2, hspace=0.2, left=0.1, right=0.2, bottom=0.1, top=0.2) \n",
    "    return grid\n",
    "\n",
    "\n",
    "def get_weights(layer, convert_pwm=0):\n",
    "    W =  np.squeeze(layer.W.get_value())\n",
    "    W_norm = W\n",
    "    if convert_pwm == 1:\n",
    "        for i in range(len(W)):\n",
    "            #weights = np.exp(W[i])\n",
    "            MIN = np.min(W[i])\n",
    "            weights = W[i] - MIN\n",
    "            Z = np.sum(weights, axis=0)\n",
    "            weights /= np.tile(Z, (W[i].shape[0],1))\n",
    "            W_norm[i] = weights\n",
    "    return W_norm\n",
    "\n",
    "def plot_conv_weights(W, options):\n",
    "    num_filters = W.shape[0]\n",
    "    nrows = np.ceil(np.sqrt(num_filters)).astype(int)\n",
    "    ncols = nrows\n",
    "    plt.figure()\n",
    "    grid = subplot_grid(nrows, ncols)\n",
    "    for i in range(num_filters):\n",
    "        plt.subplot(grid[i])\n",
    "        plt.imshow(W[i], cmap='hot_r', interpolation='nearest')\n",
    "        fig_options(plt, options)\n",
    "    return plt\n",
    "\n",
    "def batch_generator(X, y, batch_size=128, shuffle=True):\n",
    "    \"\"\"python generator to get a randomized minibatch\"\"\"\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(X))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(X)-batch_size+1, batch_size):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx+batch_size]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx+batch_size)\n",
    "        yield X[excerpt].astype('float32'), y[excerpt].astype('int32')\n",
    "\n",
    "\n",
    "def get_feature_map(layer, input_var, X, index):\n",
    "    feature_maps = theano.function([input_var], layers.get_output(layer), allow_input_downcast=True)\n",
    "    \n",
    "    # get feature maps in batches for speed (large batches may be too much memory for GPU)\n",
    "    return feature_maps(X[index])\n",
    "\n",
    "def pseudoinverse_filter2(W1):\n",
    "    #W1 = W1.transpose([2,0,1,3])\n",
    "    weight = np.squeeze(W1)    \n",
    "    W1_inv = []\n",
    "    for i in range(len(weight)):\n",
    "        W1_inv.append(np.linalg.pinv(weight[i]))\n",
    "    W1_inv = np.array(W1_inv, dtype=theano.config.floatX)\n",
    "    W1_inv = np.expand_dims(W1_inv,3).transpose([2,0,1,3])\n",
    "    W1_inv = theano.shared(W1_inv)\n",
    "    return W1_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X2, labels = reconstruct_layer2(network, train)\n",
    "map_index = range(10000)\n",
    "X = train[0][map_index]\n",
    "X = np.expand_dims(X,0)\n",
    "y = np.argmax(train[1][map_index])\n",
    "\n",
    "def get_class_pwm(X, class_index, norm=0):\n",
    "    class_pwm = 0\n",
    "    for i in class_index:\n",
    "        x = np.squeeze(X[i])\n",
    "        if norm == 1:\n",
    "            for i in range(5):\n",
    "                x[:,:10]=0\n",
    "                x[:,-10:]=0\n",
    "                MEAN = np.nanmean(x,axis=1)\n",
    "                x -= np.outer(MEAN, np.ones(x.shape[1]))\n",
    "            sumX = np.sum(x,axis=0)\n",
    "            x /= np.outer(np.ones(4),sumX)\n",
    "        class_pwm += x\n",
    "    class_pwm /= len(class_index)\n",
    "    return class_pwm\n",
    "\n",
    "model = []\n",
    "for class_plot in range(20):\n",
    "    y = np.argmax(train[1], axis=1)\n",
    "    y = y[map_index]\n",
    "    class_index = np.where(y == class_plot)[0]\n",
    "    model.append(get_class_pwm(np.squeeze(X), class_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv1': <lasagne.layers.dnn.Conv2DDNNLayer at 0x7fef6f817cd0>,\n",
       " 'conv1_active': <lasagne.layers.special.NonlinearityLayer at 0x7fef6f5f9890>,\n",
       " 'conv1_batch': <lasagne.layers.normalization.BatchNormLayer at 0x7fef6f5e73d0>,\n",
       " 'conv1_bias': <lasagne.layers.special.BiasLayer at 0x7fef6f6efd50>,\n",
       " 'conv1_pool': <lasagne.layers.pool.MaxPool2DLayer at 0x7fef6f609550>,\n",
       " 'conv2': <lasagne.layers.dnn.Conv2DDNNLayer at 0x7fef6f5e7b90>,\n",
       " 'conv2_active': <lasagne.layers.special.NonlinearityLayer at 0x7fef6f609810>,\n",
       " 'conv2_batch': <lasagne.layers.normalization.BatchNormLayer at 0x7fef6f609690>,\n",
       " 'conv2_bias': <lasagne.layers.special.BiasLayer at 0x7fef6f5e7c10>,\n",
       " 'conv2_pool': <lasagne.layers.pool.MaxPool2DLayer at 0x7fef6f609d50>,\n",
       " 'dense1': <lasagne.layers.dense.DenseLayer at 0x7fef6f609d90>,\n",
       " 'dense1_active': <lasagne.layers.special.NonlinearityLayer at 0x7fef6f617250>,\n",
       " 'dense1_bias': <lasagne.layers.special.BiasLayer at 0x7fef6f808d50>,\n",
       " 'dense1_dropout': <lasagne.layers.noise.DropoutLayer at 0x7fef6f6173d0>,\n",
       " 'dense2': <lasagne.layers.dense.DenseLayer at 0x7fef6f808e10>,\n",
       " 'dense2_active': <lasagne.layers.special.NonlinearityLayer at 0x7fef6f617590>,\n",
       " 'dense2_bias': <lasagne.layers.special.BiasLayer at 0x7fef6f617450>,\n",
       " 'input': <lasagne.layers.input.InputLayer at 0x7fef6f817c90>,\n",
       " 'output': <lasagne.layers.special.NonlinearityLayer at 0x7fef6f617590>}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = nnmodel.network\n",
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_output(X,layer):\n",
    "    num_data = len(X)\n",
    "    feature_maps = theano.function([nnmodel.input_var], layers.get_output(layer,deterministic=True), allow_input_downcast=True)\n",
    "    map_shape = get_output_shape(layer)\n",
    "\n",
    "    # get feature maps in batches for speed (large batches may be too much memory for GPU)\n",
    "    batch_size=500\n",
    "    num_batches = num_data // batch_size\n",
    "    shape = list(map_shape)\n",
    "    shape[0] = num_data\n",
    "    output = np.empty(tuple(shape))\n",
    "    for i in range(num_batches):\n",
    "        index = range(i*batch_size, (i+1)*batch_size)    \n",
    "        output[index] = feature_maps(X[index])\n",
    "\n",
    "    # get the rest of the feature maps\n",
    "    excess = num_data-num_batches*batch_size\n",
    "    if excess:\n",
    "        index = range(num_data-excess, num_data)  \n",
    "        output[index] = feature_maps(X[index])\n",
    "    return output\n",
    "\n",
    "\n",
    "X = train[0]\n",
    "layer = network['output']\n",
    "output = get_output(X,layer)\n",
    "output2 = get_output(test[0],layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01799067,  0.02015203,  0.01981791, ...,  0.02109863,\n",
       "         0.02291781,  0.0191187 ],\n",
       "       [ 0.17051034,  0.17405045,  0.1624075 , ...,  0.17112423,\n",
       "         0.18133062,  0.16678168],\n",
       "       [ 0.0240998 ,  0.02605957,  0.02544425, ...,  0.02700753,\n",
       "         0.02919181,  0.02455413],\n",
       "       ..., \n",
       "       [ 0.02071388,  0.02282716,  0.02220435, ...,  0.02359214,\n",
       "         0.02581214,  0.021242  ],\n",
       "       [ 0.00889643,  0.00987821,  0.00978408, ...,  0.0104605 ,\n",
       "         0.01154824,  0.00908871],\n",
       "       [ 0.02013096,  0.02237908,  0.02215598, ...,  0.02359683,\n",
       "         0.0256871 ,  0.02148707]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shape2 = list(output.shape)\n",
    "shape2[0] = None\n",
    "input_var2 = T.dmatrix('input')\n",
    "input_var2 = T.cast(input_var2, 'float32')\n",
    "\n",
    "\n",
    "deconv = {}\n",
    "deconv['input'] = InputLayer(tuple(shape2), input_var=input_var2)\n",
    "num_units = list(get_output_shape(network['dense1']))[1]\n",
    "deconv['dense2'] = layers.DenseLayer(deconv['input'], num_units=num_units, W=network['dense2'].W.dimshuffle([1,0]), \n",
    "                                     b=init.Constant(0.05), nonlinearity=nonlinearities.rectify)\n",
    "num_units = np.prod(list(get_output_shape(network['conv2_pool']))[1:])\n",
    "deconv['dense1'] = layers.DenseLayer(deconv['dense2'], num_units=num_units, W=network['dense1'].W.dimshuffle([1,0]), \n",
    "                                     b=init.Constant(0.05), nonlinearity=nonlinearities.rectify)\n",
    "\n",
    "shape = list(get_output_shape(network['conv2_pool']))\n",
    "shape[0] = -1\n",
    "deconv['reshape'] = layers.ReshapeLayer(deconv['dense1'], shape=tuple(shape))\n",
    "deconv['pool2'] = layers.Upscale2DLayer(deconv['reshape'], (2,1))\n",
    "deconv['conv2']  = Conv2DLayer(deconv['pool2'], num_filters=network['conv2'].input_shape[1],\n",
    "                                          filter_size=network['conv2'].filter_size,\n",
    "                                          W=network['conv2'].W.dimshuffle([1,0,2,3]), #W2_inv, #\n",
    "                                          b=init.Constant(0.05), \n",
    "                                          pad='same',\n",
    "                                          nonlinearity=nonlinearities.rectify, flip_filters=True)\n",
    "\n",
    "deconv['pool1'] = layers.Upscale2DLayer(deconv['conv2'], (4,1))\n",
    "deconv['conv1']  = Conv2DLayer(deconv['pool1'], num_filters=network['conv1'].input_shape[1],\n",
    "                                          filter_size=network['conv1'].filter_size,\n",
    "                                          W=network['conv1'].W.dimshuffle([1,0,2,3]), #W2_inv, #\n",
    "                                          b=init.Constant(0.05), \n",
    "                                          pad='same',\n",
    "                                          nonlinearity=nonlinearities.sigmoid, flip_filters=True)\n",
    "deconv['output'] = deconv['conv1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_var = T.tensor4('targets')\n",
    "prediction = layers.get_output(deconv['output'], deterministic=False)\n",
    "\n",
    "loss = (target_var - prediction) ** 2\n",
    "loss = loss.mean()\n",
    "\n",
    "params = layers.get_all_params(deconv['output'], trainable=True)    \n",
    "grad = T.grad(loss, params)\n",
    "\n",
    "update_op = updates.adam(grad, params, learning_rate=0.001)\n",
    "train_fun = theano.function([input_var2, target_var], [loss, prediction], updates=update_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \n",
      "  loss = 0.188757 \n",
      "Epoch 2 \n",
      "  loss = 0.187327 \n",
      "Epoch 3 \n",
      "  loss = 0.187311 \n",
      "Epoch 4 \n",
      "  loss = 0.187301 \n",
      "Epoch 5 \n",
      "  loss = 0.187294 \n",
      "Epoch 6 \n",
      "  loss = 0.187287 \n",
      "Epoch 7 \n",
      "  loss = 0.187277 \n",
      "Epoch 8 \n",
      "  loss = 0.187265 \n",
      "Epoch 9 \n",
      "  loss = 0.187253 \n",
      "Epoch 10 \n",
      "  loss = 0.187242 \n",
      "Epoch 11 \n",
      "  loss = 0.187236 \n",
      "Epoch 12 \n",
      "  loss = 0.187227 \n",
      "Epoch 13 \n",
      "  loss = 0.187219 \n",
      "Epoch 14 \n",
      "  loss = 0.187211 \n",
      "Epoch 15 \n",
      "  loss = 0.187203 \n",
      "Epoch 16 \n",
      "  loss = 0.187197 \n",
      "Epoch 17 \n",
      "  loss = 0.187192 \n",
      "Epoch 18 \n",
      "  loss = 0.187184 \n",
      "Epoch 19 \n",
      "  loss = 0.187177 \n",
      "Epoch 20 \n",
      "  loss = 0.187169 \n",
      "Epoch 21 \n",
      "  loss = 0.187163 \n",
      "Epoch 22 \n",
      "  loss = 0.187157 \n",
      "Epoch 23 \n",
      "  loss = 0.187152 \n",
      "Epoch 24 \n",
      "  loss = 0.187147 \n",
      "Epoch 25 \n",
      "  loss = 0.187143 \n",
      "Epoch 26 \n",
      "  loss = 0.187138 \n",
      "Epoch 27 \n",
      "  loss = 0.187136 \n",
      "Epoch 28 \n",
      "  loss = 0.187132 \n",
      "Epoch 29 \n",
      "  loss = 0.187129 \n",
      "Epoch 30 \n",
      "  loss = 0.187126 \n",
      "Epoch 31 \n",
      "  loss = 0.187120 \n",
      "Epoch 32 \n",
      "  loss = 0.187117 \n",
      "Epoch 33 \n",
      "  loss = 0.187113 \n",
      "Epoch 34 \n",
      "  loss = 0.187111 \n",
      "Epoch 35 \n",
      "  loss = 0.187108 \n",
      "Epoch 36 \n",
      "  loss = 0.187106 \n",
      "Epoch 37 \n",
      "  loss = 0.187100 \n",
      "Epoch 38 \n",
      "  loss = 0.187097 \n",
      "Epoch 39 \n",
      "  loss = 0.187090 \n",
      "Epoch 40 \n",
      "  loss = 0.187083 \n",
      "Epoch 41 \n",
      "  loss = 0.187076 \n",
      "Epoch 42 \n",
      "  loss = 0.187071 \n",
      "Epoch 43 \n",
      "  loss = 0.187066 \n",
      "Epoch 44 \n",
      "  loss = 0.187064 \n",
      "Epoch 45 \n",
      "  loss = 0.187059 \n",
      "Epoch 46 \n",
      "  loss = 0.187057 \n",
      "Epoch 47 \n",
      "  loss = 0.187055 \n",
      "Epoch 48 \n",
      "  loss = 0.187054 \n",
      "Epoch 49 \n",
      "  loss = 0.187051 \n",
      "Epoch 50 \n",
      "  loss = 0.187049 \n"
     ]
    }
   ],
   "source": [
    "def batch_generator(X, y, batch_size=128):\n",
    "    for start_idx in range(0, len(X)-batch_size+1, batch_size):\n",
    "        excerpt = slice(start_idx, start_idx+batch_size)\n",
    "        yield X[excerpt].astype(np.float32), y[excerpt].astype(np.float32)\n",
    "\n",
    "batch_size = 128        \n",
    "for epoch in range(50):\n",
    "    sys.stdout.write(\"\\rEpoch %d \\n\"%(epoch+1))\n",
    "\n",
    "    num_batches = output.shape[0] // batch_size\n",
    "    batches = batch_generator(output, train[0], batch_size)\n",
    "    value = 0\n",
    "    for i in range(num_batches):\n",
    "        X,y = next(batches)\n",
    "        loss, prediction = train_fun(X, y)\n",
    "        value += np.mean(loss)\n",
    "    sys.stdout.write(\"\\r  loss = %f \\n\"%(value/num_batches))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X2, labels = reconstruct_layer2(network, train)\n",
    "map_index = range(10000)\n",
    "X = train[0][map_index]\n",
    "X = np.expand_dims(X,0)\n",
    "y = np.argmax(train[1][map_index])\n",
    "\n",
    "def get_class_pwm(X, class_index, norm=0):\n",
    "    class_pwm = 0\n",
    "    for i in class_index:\n",
    "        x = np.squeeze(X[i])\n",
    "        if norm == 1:\n",
    "            for i in range(5):\n",
    "                x[:,:10]=0\n",
    "                x[:,-10:]=0\n",
    "                MEAN = np.nanmean(x,axis=1)\n",
    "                x -= np.outer(MEAN, np.ones(x.shape[1]))\n",
    "            sumX = np.sum(x,axis=0)\n",
    "            x /= np.outer(np.ones(4),sumX)\n",
    "        class_pwm += x\n",
    "    class_pwm /= len(class_index)\n",
    "    return class_pwm\n",
    "\n",
    "model = []\n",
    "for class_plot in range(20):\n",
    "    y = np.argmax(train[1], axis=1)\n",
    "    y = y[map_index]\n",
    "    class_index = np.where(y == class_plot)[0]\n",
    "    model.append(get_class_pwm(np.squeeze(X), class_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv1': <lasagne.layers.dnn.Conv2DDNNLayer at 0x7fef6f817cd0>,\n",
       " 'conv1_active': <lasagne.layers.special.NonlinearityLayer at 0x7fef6f5f9890>,\n",
       " 'conv1_batch': <lasagne.layers.normalization.BatchNormLayer at 0x7fef6f5e73d0>,\n",
       " 'conv1_bias': <lasagne.layers.special.BiasLayer at 0x7fef6f6efd50>,\n",
       " 'conv1_pool': <lasagne.layers.pool.MaxPool2DLayer at 0x7fef6f609550>,\n",
       " 'conv2': <lasagne.layers.dnn.Conv2DDNNLayer at 0x7fef6f5e7b90>,\n",
       " 'conv2_active': <lasagne.layers.special.NonlinearityLayer at 0x7fef6f609810>,\n",
       " 'conv2_batch': <lasagne.layers.normalization.BatchNormLayer at 0x7fef6f609690>,\n",
       " 'conv2_bias': <lasagne.layers.special.BiasLayer at 0x7fef6f5e7c10>,\n",
       " 'conv2_pool': <lasagne.layers.pool.MaxPool2DLayer at 0x7fef6f609d50>,\n",
       " 'dense1': <lasagne.layers.dense.DenseLayer at 0x7fef6f609d90>,\n",
       " 'dense1_active': <lasagne.layers.special.NonlinearityLayer at 0x7fef6f617250>,\n",
       " 'dense1_bias': <lasagne.layers.special.BiasLayer at 0x7fef6f808d50>,\n",
       " 'dense1_dropout': <lasagne.layers.noise.DropoutLayer at 0x7fef6f6173d0>,\n",
       " 'dense2': <lasagne.layers.dense.DenseLayer at 0x7fef6f808e10>,\n",
       " 'dense2_active': <lasagne.layers.special.NonlinearityLayer at 0x7fef6f617590>,\n",
       " 'dense2_bias': <lasagne.layers.special.BiasLayer at 0x7fef6f617450>,\n",
       " 'input': <lasagne.layers.input.InputLayer at 0x7fef6f817c90>,\n",
       " 'output': <lasagne.layers.special.NonlinearityLayer at 0x7fef6f617590>}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_fun' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-9b53f1c801a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mclass_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_fun' is not defined"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "y = np.expand_dims(output2[index,:],0)\n",
    "prediction = test_fun(y.astype(np.float32))\n",
    "class_index = np.argmax(test[1][index,:])\n",
    "\n",
    "\n",
    "height=100\n",
    "bp_width=20\n",
    "size = (25.,10.0)\n",
    "\n",
    "logo = seq_logo(np.squeeze(test[0][index]), height, bp_width, norm=0, rna=1, filepath='.')\n",
    "fig = plt.figure(figsize=size);\n",
    "plt.imshow(logo, interpolation='none');\n",
    "plt.axis('off');\n",
    "\n",
    "logo = seq_logo(np.squeeze(model[class_index]), height, bp_width, norm=0, rna=1, filepath='.')\n",
    "fig = plt.figure(figsize=size);\n",
    "plt.imshow(logo, interpolation='none');\n",
    "plt.axis('off');\n",
    "plt.title(str(class_index))\n",
    "\n",
    "logo = seq_logo(np.squeeze(prediction[0]), height, bp_width, norm=0, rna=1, filepath='.')\n",
    "fig = plt.figure(figsize=size);\n",
    "plt.imshow(logo, interpolation='none');\n",
    "plt.axis('off');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-1c4a97140690>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m25.\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m25.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mlogo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseq_logo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrna\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'none'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-923bc3ce1d21>\u001b[0m in \u001b[0;36mseq_logo\u001b[1;34m(pwm, height, nt_width, norm, rna, filepath)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[0mtotal_height\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[0mmax_height\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_height\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m     \u001b[0mlogo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[0mremaining_height\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_height\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prediction = layers.get_output(deconv['output'], deterministic=False)\n",
    "test_fun = theano.function([input_var2], prediction)\n",
    "\n",
    "\n",
    "for index in range(270,280):\n",
    "\n",
    "    y = np.expand_dims(output2[index,:],0)\n",
    "    prediction = test_fun(y.astype(np.float32))\n",
    "    class_index = np.argmax(test[1][index,:])\n",
    "\n",
    "    height=100\n",
    "    bp_width=20\n",
    "    size = (25.,10.0)\n",
    "\n",
    "    logo = seq_logo(np.squeeze(test[0][index]), height, bp_width, norm=0, rna=1, filepath='.')\n",
    "    fig = plt.figure(figsize=size);\n",
    "    plt.imshow(logo, interpolation='none');\n",
    "    plt.axis('off');\n",
    "\n",
    "    logo = seq_logo(np.squeeze(model[class_index]), height, bp_width, norm=0, rna=1, filepath='.')\n",
    "    fig = plt.figure(figsize=size);\n",
    "    plt.imshow(logo, interpolation='none');\n",
    "    plt.axis('off');\n",
    "    plt.title(str(class_index))\n",
    "\n",
    "    logo = seq_logo(np.squeeze(prediction[0]), height, bp_width, norm=0, rna=1, filepath='.')\n",
    "    fig = plt.figure(figsize=size);\n",
    "    plt.imshow(logo, interpolation='none');\n",
    "    plt.axis('off');\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W =  np.squeeze(l_conv1.W.get_value())\n",
    "num_filters = W.shape[0]\n",
    "for i in range(1):#num_filters):\n",
    "    MIN = np.min(W[i])\n",
    "    MAX = np.max(W[i])\n",
    "    pwm = (W[i] - MIN)/(MAX-MIN)\n",
    "    norm = np.outer(np.ones(4), np.sum(pwm, axis=0))\n",
    "    pwm = pwm/norm\n",
    "    \n",
    "    seq_length = pwm.shape[1]\n",
    "    plt.figure(figsize = (seq_length/8,1))\n",
    "    fig = plot_conv_filter(plt, pwm, height=300, bp_width=75, norm=0, rna=1, adjust=-2, filepath='.', showbar=1)\n",
    "    #fig.set_size_inches(seq_length/20,1, forward=True)\n",
    "    #plt.savefig('categorical_l1_activation.eps', format='eps', dpi=1000)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = get_weights(l_conv1, convert_pwm=1)\n",
    "\n",
    "options = {'figsize':(50,50), 'axis':'off'}\n",
    "plt = plot_conv_weights(W, options)\n",
    "plt.savefig('categorical_layer1_weights.tiff', format='tiff', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fmaps = get_feature_maps(l_nonlin1, train[0]) \n",
    "mean_activation, std_activation = get_class_activation(fmaps, train[1])\n",
    "options = { 'ylim': [0, 4],\n",
    "            'xticks': [0, 50, 100, 150],\n",
    "            'yticks': [0.5, 1.5, 2.5, 3.5], \n",
    "            'labelsize': 18,\n",
    "            'figsize': (150,100)}\n",
    "plt = plot_mean_activations(mean_activation, options)\n",
    "plt.savefig('categorical_l1_activation.eps', format='eps', dpi=1000)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
