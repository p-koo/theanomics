{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating synthetic sequences with embedded regulatory grammars for RNA-binding proteins\n",
    "\n",
    "### Motivation\n",
    "\n",
    "RNA-binding proteins physically interact with RNAs primarily through direct nucleotide interactions via sequence binding motifs or binding to secondary structure motifs.  Here, I will focus on the former using a deep convolutional neural network. However, assessing the performance on actual data is difficult as the \"ground truth\" is usually not known. \n",
    "\n",
    "To test the performance of CNNs, I am going to simulate idealistic data where the sequences contain combinations of \"known\" binding motifs implanted in specific \"known\" locations while the rest of the sequence is generated randomly. This idealistic dataset can therefore gauge whether or not CNNs can indeed recover the motifs and their regulatory grammars in the  simplest scenario. It will also provide a baseline understanding the limitations of this approach, such as how many sequences are needed to recover different levels of regulatory grammar complexities.   \n",
    "\n",
    "Note that the regulatory grammars generated here are unrealistic -- there is no such databases with accurate annotations as far as I am aware of.  Here, only the binding motifs are derived from actual experimental data; while the regulatory grammars are arbitrary, generated from a probabilisitic framework with a user-defined level of complexity, i.e. how many motifs interact with each other.\n",
    "\n",
    "### Simulation model\n",
    "\n",
    "To generate regulatory grammars, we need to create a framework for the interactions of specific motifs across distinct spatial distances. \n",
    "\n",
    "First, we will assume there are only P proteins. Of these P proteins, we can generate G regulatory grammars, which sample combinations of the M motifs.  We can then generate arbitrary distances, sampled from an exponential distribution, between each motif. Once the motif distances and combinations have been set, these constitute the set of regulatory grammars.  \n",
    "\n",
    "We can also simulate negative results by simulating different motifs with the same distances or the same motifs with different distance or incomplete grammars.  First, we'll just assume a perfect dataset and see how it performs.  Then, we will systematically increase the complexity to see when exactly this model fails.  \n",
    "\n",
    "\n",
    "#### Create a motif database for drosophila melanogaster\n",
    "\n",
    "The data comes from Ray et al. \"A compendium of RNA-binding motifs for decoding gene regulation\" (http://www.nature.com/nature/journal/v499/n7457/abs/nature12311.html). The link to the motifs I downloaded is here: \n",
    "\n",
    "\\$ wget http://hugheslab.ccbr.utoronto.ca/supplementary-data/RNAcompete_eukarya/top10align_motifs.tar.gz\n",
    "\n",
    "\\$ tar xzvf top10align_motifs.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wget http://hugheslab.ccbr.utoronto.ca/supplementary-data/RNAcompete_eukarya/top10align_motifs.tar.gz\n",
    "!tar xzf top10align_motifs.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, each file is a different RBP motif as a position frequency matrix.  So, the first step is to compile all of these files into a suitable database.  In particular, we can parse each motifs (position frequency matrix) from each file in motifpath (downloaded top10align_motifs folder), create a database (list of arrays), and save as binary format (motif.list):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from six.moves import cPickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "motifpath = 'top10align_motifs/'   # directory where motif files are located\n",
    "motiflist = 'motif.pickle'         # output filename\n",
    "\n",
    "# get all motif files in motifpath directory\n",
    "listdir = os.listdir(motifpath)\n",
    "\n",
    "# parse motifs\n",
    "motif_set = []\n",
    "for files in listdir:\n",
    "    df = pd.read_table(os.path.join(motifpath,files))\n",
    "    motif_set.append(df.iloc[0::,1::].transpose())\n",
    "\n",
    "# save motifs    \n",
    "f = open(motiflist, 'wb')\n",
    "cPickle.dump(motif_set, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simulate some sequences with some specs... Note, there are 244 motifs here. So, for simplicity, we will downsample this to create different levels of dataset complexity.  \n",
    "\n",
    "$N$: total number of sequences \n",
    "\n",
    "$M$: total number of motifs ($M \\leq 244$) \n",
    "\n",
    "$S$: size of each sequence \n",
    "\n",
    "$w$: population fraction of each regulatory grammar \n",
    "\n",
    "To make the sequence super clean, we will use a uniform distribution for the PWFs of the 'non-motif sequences'. We will first set up a hierarchical interaction of the motifs, a so-called \"regulatory grammar\". \n",
    "\n",
    "So, first, we will sample a regulatory grammar.  This will give us which motifs are present and how far apart they are separated with respect to each other.  Then, we can create a postiion frequency matrix of this regulatory grammar and simulate a set number based on the population fraction $w$. \n",
    "\n",
    "Now that we generated some simulated data, we should shuffle the data, then split the data into training set, cross-validation set, and test set, while converting the sequence data into one-hot representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# setup motif code \n",
    "\n",
    "max_motifs_exon = 7\n",
    "num_models = 10      # number of regulatory grammars\n",
    "distance_rate = 10\n",
    "interaction_rate = 10\n",
    "distance_offset = 3\n",
    "motif_range = [0, 40]\n",
    "exon_size = 100\n",
    "intron_size = 150\n",
    "region_size = np.array([exon_size, intron_size, \n",
    "                 intron_size, exon_size, \n",
    "                 exon_size, intron_size, \n",
    "                 intron_size, exon_size])\n",
    "\n",
    "motif_model = []\n",
    "for i in range(num_models):\n",
    "    exon_model = []\n",
    "    for k in range(8):\n",
    "        status = True\n",
    "        while status:\n",
    "            # simulate number of motifs for each region\n",
    "            num_motifs = np.ceil(np.random.randint(1, max_motifs_exon)).astype(int)\n",
    "\n",
    "            # simulate position offset from border\n",
    "            start_offset = np.ceil(np.random.exponential(scale=distance_rate)).astype(int) + distance_offset\n",
    "\n",
    "            # exponential separation rate between motifs\n",
    "            distance_scale = (region_size[k]-start_offset)/num_motifs/10\n",
    "\n",
    "            # simulate distance between motifs \n",
    "            separation = np.ceil(np.random.exponential(scale=interaction_rate, size=num_motifs)) + distance_offset\n",
    "\n",
    "            # randomly select motifs from top10align\n",
    "            motif_index = np.random.randint(motif_range[0], motif_range[1], size=num_motifs)\n",
    "\n",
    "            # build position weight matrix model\n",
    "            if (k%2) == 0:\n",
    "                reverse = True\n",
    "            else:\n",
    "                reverse = False\n",
    "                \n",
    "            pwm = np.ones((4, start_offset))/4\n",
    "            for j in range(num_motifs):\n",
    "                if reverse:\n",
    "                    pwm = np.hstack([np.ones((4,separation[j]))/4, motif_set[motif_index[j]], pwm])\n",
    "                else:\n",
    "                    pwm = np.hstack([pwm, motif_set[motif_index[j]], np.ones((4,separation[j]))/4])\n",
    "            remainder = region_size[k]-pwm.shape[1]\n",
    "            if remainder > 0:\n",
    "                if reverse:\n",
    "                    pwm = np.hstack((np.ones((4,remainder))/4, pwm))\n",
    "                else:\n",
    "                    pwm = np.hstack((pwm, np.ones((4,remainder))/4))\n",
    "                status = False\n",
    "\n",
    "        model = {'pwm': pwm,\n",
    "                 'num_motifs': num_motifs, \n",
    "                 'start_offset': start_offset, \n",
    "                 'distance_scale': distance_scale, \n",
    "                 'separation': separation, \n",
    "                 'motif_index': motif_index}\n",
    "        exon_model.append(model)\n",
    "    motif_model.append(exon_model)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# setup cell type gene expression profiles\n",
    "num_cell_types = 200\n",
    "num_genes = 5000\n",
    "\n",
    "# random gene states\n",
    "on_state = 4\n",
    "on_std = 1.5\n",
    "off_state = 0\n",
    "off_std = 1\n",
    "pop_frac = .85\n",
    "\n",
    "gene_expression_model = []\n",
    "for i in range(num_cell_types):\n",
    "    gene_state = np.random.binomial(1, pop_frac, num_genes)\n",
    "\n",
    "    on_index = np.where(gene_state==1)[0]\n",
    "    off_index = np.where(gene_state==0)[0]\n",
    "    num_on = len(on_index)\n",
    "    num_off = len(off_index)\n",
    "    \n",
    "    Z_off = np.random.normal(off_state, off_std, num_off)\n",
    "    Z_on = np.random.normal(on_state, on_std, num_on)\n",
    "\n",
    "    Z = np.zeros((num_genes))\n",
    "    Z[on_index] = Z_on\n",
    "    Z[off_index] = Z_off\n",
    "\n",
    "    model = {'gene_expression': Z,\n",
    "             'gene_state': gene_state,\n",
    "             'on_index': on_index,\n",
    "             'off_index': off_index,\n",
    "             'num_off': num_off,\n",
    "             'num_on': num_on\n",
    "             }\n",
    "    gene_expression_model.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get list of unique motifs\n",
    "motif_index = []\n",
    "for exon_model in motif_model:\n",
    "    for model in exon_model:\n",
    "        motif_index.append(model['motif_index'])\n",
    "        \n",
    "motifs = np.hstack(motif_index)\n",
    "unique_motifs = np.unique(motifs)\n",
    "num_unique = len(unique_motifs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get motif count for each exon model\n",
    "motif_count = np.zeros((num_models, num_unique))\n",
    "for i in range(num_models):\n",
    "    exon_model = motif_model[i]\n",
    "    for model in exon_model:\n",
    "        motif_count[i, unique_motifs[model['motif_index']]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup rbp interaction code - associate rbps to multiple motifs\n",
    "num_rbp = 100\n",
    "interaction_rate = 6\n",
    "rbp_code = []\n",
    "for i in range(num_rbp):\n",
    "    num_interactions = min(np.ceil(np.random.exponential(interaction_rate)).astype(int), num_unique)\n",
    "    rbp_interaction = np.random.permutation(range(num_unique))[:num_interactions]\n",
    "    interaction_state = np.random.randint(0,2,num_interactions)\n",
    "    model = {'rbp_interaction': rbp_interaction,\n",
    "             'num_interactions': num_interactions,\n",
    "             'interaction_state': interaction_state,\n",
    "             }\n",
    "    rbp_code.append(model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get catalogue of rbp interactions with each motif\n",
    "motif_interactions = []\n",
    "for motif in unique_motifs:\n",
    "    \n",
    "    # loop through each rbp to see if it regulates the motif\n",
    "    rbp_players = []\n",
    "    for i in range(num_rbp):\n",
    "        code = rbp_code[i]\n",
    "        rbp_interaction = code['rbp_interaction'] # <-- list of motifs that rbp interacts with\n",
    "        match_index = np.where(motif == rbp_interaction)[0] # <-- find which rbp regulate current motif\n",
    "\n",
    "        if len(match_index) > 0: \n",
    "            # rbp interaction type (enhancer or silencer)\n",
    "            interaction_type = code['interaction_state'][match_index] \n",
    "            rbp_players.append([i, interaction_type])\n",
    "    motif_interactions.append(np.array(rbp_players))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# figure out psi for each motif_model based on gene_expression of each rbp and their effect on state\n",
    "psi = np.zeros((num_cell_types, num_models))\n",
    "for i in range(num_cell_types):\n",
    "    gene_expression = gene_expression_model[i]['gene_expression']\n",
    "\n",
    "    for j in range(num_models):\n",
    "        motifs = unique_motifs[motif_count[j] > 1]\n",
    "        count = motif_count[j][motif_count[j] > 1]\n",
    "\n",
    "        rbp = []\n",
    "        state = []\n",
    "        for motif in motifs:\n",
    "            rbp.append(motif_interactions[motif][:,0])\n",
    "            state.append(motif_interactions[motif][:,1])\n",
    "\n",
    "        rbp = np.hstack(rbp)\n",
    "        state = np.hstack(state)\n",
    "        tpm = np.exp(gene_expression[rbp])\n",
    "        tpm /= sum(tpm)\n",
    "        psi[i,j] =sum(tpm*state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 2, 1, 0, 2, 0, 0, 0, 1, 2, 2, 0, 0, 1, 3, 0, 2, 2, 0, 0, 3,\n",
       "       2, 0, 2, 0, 1, 0, 3, 1, 2, 0, 1, 0, 1, 3, 1, 1, 0, 2, 1, 1, 0, 0, 2,\n",
       "       2, 2, 2, 0, 0, 1, 2, 2, 0, 0, 0, 2, 1, 1, 2, 3, 0, 0, 1, 2, 2, 0, 0,\n",
       "       1, 0, 0, 2, 2, 1, 1, 0, 1, 0, 3, 2, 0, 0, 2, 0, 3, 1, 2, 0, 0, 3, 0,\n",
       "       2, 0, 0, 2, 0, 0, 0, 1, 2, 1, 0, 0, 1, 0, 2, 0, 1, 1, 0, 1, 1, 0, 2,\n",
       "       1, 0, 2, 2, 1, 2, 1, 3, 1, 3, 2, 3, 1, 0, 0, 0, 2, 1, 0, 1, 2, 3, 2,\n",
       "       2, 1, 0, 0, 0, 1, 3, 3, 1, 0, 0, 0, 2, 3, 3, 2, 1, 0, 1, 3, 1, 2, 2,\n",
       "       0, 1, 2, 1, 2, 0, 2, 3, 0, 0, 1, 2, 3, 0, 0, 2, 1, 0, 1, 0, 3, 3, 2,\n",
       "       0, 0, 1, 2, 2, 0, 0, 1, 3, 1, 2, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "       1, 1, 0, 1, 1, 2, 2, 3, 0, 2, 0, 0, 1, 0, 2, 0, 2, 0, 1, 2, 0, 2, 0,\n",
       "       1, 0, 0, 3, 2, 1, 0, 2, 0, 0, 0, 0, 2, 2, 3, 3, 1, 0, 2, 2])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simulate_sequences(pwm, num_sequenes):\n",
    "    cum_pwm = pwm.cumsum(axis=0)\n",
    "    sequence = []\n",
    "    for i in range(num_sequences):\n",
    "        Z = np.random.uniform(0, 1, cum_pwm.shape[1])\n",
    "        sequence.append(np.argmin((cum_pwm - Z)**2, axis=0))\n",
    "    return sequence\n",
    "\n",
    "def simulate_skipped_exon_sequences(exon_model, num_sequences):\n",
    "    pwm_up_exon = np.hstack([exon_model[0]['pwm'], exon_model[1]['pwm']])\n",
    "    pwm_ae_exon_1 = np.hstack([exon_model[1]['pwm'], exon_model[2]['pwm']])\n",
    "    pwm_ae_exon_2 = np.hstack([exon_model[2]['pwm'], exon_model[3]['pwm']])\n",
    "    pwm_down_exon = np.hstack([exon_model[3]['pwm'], exon_model[4]['pwm']])\n",
    "\n",
    "    up_exon_seq = simulate_sequences(pwm_up_exon, num_sequenes)\n",
    "    ae_exon_seq_1 = simulate_sequences(pwm_ae_exon_1, num_sequenes)\n",
    "    ae_exon_seq_2 = simulate_sequences(pwm_ae_exon_2, num_sequenes)\n",
    "    down_exon_seq = simulate_sequences(pwm_down_exon, num_sequenes)\n",
    "\n",
    "    return up_exon_seq, ae_exon_seq_1, ae_exon_seq_2, down_exon_seq\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 300)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_exons = 3000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Split data and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def convert_one_hot(seq):\n",
    "    \"\"\"convert a sequence into a 1-hot representation\"\"\"\n",
    "    \n",
    "    nucleotide = 'ACGU'\n",
    "    N = len(seq)\n",
    "    one_hot_seq = np.zeros((4,N))\n",
    "    for i in xrange(200):         \n",
    "        #for j in range(4):\n",
    "        #    if seq[i] == nucleotide[j]:\n",
    "        #        one_hot_seq[j,i] = 1\n",
    "        index = [j for j in xrange(4) if seq[i] == nucleotide[j]]\n",
    "        one_hot_seq[index,i] = 1\n",
    "        \n",
    "    return one_hot_seq\n",
    "\n",
    "\n",
    "def subset_data(data, label, sub_index):\n",
    "    \"\"\"returns a subset of the data and labels based on sub_index\"\"\"\n",
    "    \n",
    "    num_labels = len(np.unique(label))\n",
    "    num_sub = len(sub_index)\n",
    "    \n",
    "    sub_set = np.zeros((num_sub, 4, len(data[0])))\n",
    "    sub_set_label = np.zeros((num_sub, num_labels))\n",
    "    \n",
    "    k = 0;\n",
    "    for index in sub_index:\n",
    "        sub_set[k] = convert_one_hot(data[index])\n",
    "        sub_set_label[k,label[index]] = 1\n",
    "        k += 1\n",
    "\n",
    "    sub_set_label = sub_set_label.astype(np.uint8)\n",
    "    \n",
    "    return (sub_set, sub_set_label)\n",
    "\n",
    "def split_data(data, label, split_size):\n",
    "    \"\"\"split data into train set, cross-validation set, and test set\"\"\"\n",
    "    \n",
    "    # number of labels\n",
    "    num_labels = len(np.unique(label))\n",
    "\n",
    "    # determine indices of each dataset\n",
    "    N = len(data)\n",
    "    cum_index = np.cumsum(np.multiply([0, split_size[0], split_size[1], split_size[2]],N)).astype(int) \n",
    "\n",
    "    # shuffle data\n",
    "    shuffle = np.random.permutation(N)\n",
    "\n",
    "    # training dataset\n",
    "    train_index = shuffle[range(cum_index[0], cum_index[1])]\n",
    "    cross_validation_index = shuffle[range(cum_index[1], cum_index[2])]\n",
    "    test_index = shuffle[range(cum_index[2], cum_index[3])]\n",
    "\n",
    "    # create subsets of data based on indices \n",
    "    train = subset_data(data, label, train_index)\n",
    "    cross_validation = subset_data(data, label, cross_validation_index)\n",
    "    test = subset_data(data, label, test_index)\n",
    "    \n",
    "    return train, cross_validation, test\n",
    "\n",
    "# percentage for each dataset\n",
    "train_size = 0.7\n",
    "cross_validation_size = 0.15\n",
    "test_size = 0.15\n",
    "\n",
    "\n",
    "# get indices for each dataset\n",
    "print \"Splitting dataset into train, cross-validation, and test\"\n",
    "split_size = [train_size, cross_validation_size, test_size]\n",
    "train, cross_validation, test = split_data(data, label, split_size)\n",
    "\n",
    "# save training dataset in one-hot representation\n",
    "print \"Saving dataset\"\n",
    "f = open(filename+'_data.pickle', 'wb')\n",
    "cPickle.dump(train, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "cPickle.dump(cross_validation, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "cPickle.dump(test, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "f.close()\n",
    "\n",
    "# save training dataset in one-hot representation\n",
    "print \"Saving model\"\n",
    "f = open(filename+'_model.pickle', 'wb')\n",
    "cPickle.dump(options, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "cPickle.dump(model, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "cPickle.dump(seq_model, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now, we are ready to build and test our deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from six.moves import cPickle\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# load training data\n",
    "filename = 'train_data_10000_200_10_20.pickle'\n",
    "f = open(filename, 'rb')\n",
    "train = cPickle.load(f)\n",
    "cross_validation = cPickle.load(f)\n",
    "test = cPickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# separate data\n",
    "train_set, train_set_label = train\n",
    "cross_validation_set, cross_validation_set_label = cross_validation\n",
    "test_set, test_set_label = test\n",
    "\n",
    "# munge data for deep learning\n",
    "train_set = munge_data(train_set)\n",
    "cross_validation_set = munge_data(cross_validation_set)\n",
    "test_set = munge_data(test_set)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "outdir = 'data'\n",
    "if not os.path.isdir(outdir):\n",
    "    os.mkdir(outdir)\n",
    "    print \"making directory: \" + outdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from six.moves import cPickle\n",
    "\n",
    "# load motif list from file\n",
    "filename = 'model_10000_200_10_20.pickle'\n",
    "f = open(filename, 'rb')\n",
    "model = cPickle.load(f)\n",
    "options = cPickle.load(f)\n",
    "seq_model = cPickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# options = [motif_set, M, G, interaction_rate, distance_scale, offset, maxMotif]\n",
    "# model [motifs, grammar, distance]\n",
    "# seq_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
