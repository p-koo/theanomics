{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 980 (CNMeM is disabled, cuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "import os, sys, gzip\n",
    "import cPickle as pickle\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1004)   # for reproducibility\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "from scipy.misc import imresize\n",
    "\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from scipy import stats\n",
    "from lasagne import layers, nonlinearities, updates, objectives, init, regularization\n",
    "from lasagne.layers import get_output, get_output_shape, get_all_params\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "\n",
    "from six.moves import cPickle\n",
    "\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "import lasagne as nn\n",
    "from lasagne import layers, init, nonlinearities, utils, regularization, objectives, updates\n",
    "from lasagne.layers.base import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BatchNormLayer(Layer):\n",
    "    def __init__(self, incoming, axes='auto', epsilon=1e-4, alpha=0.1,\n",
    "                 beta=init.Constant(0), gamma=init.Constant(1),\n",
    "                 mean=init.Constant(0), inv_std=init.Constant(1), **kwargs):\n",
    "        super(BatchNormLayer, self).__init__(incoming, **kwargs)\n",
    "\n",
    "        if axes == 'auto':\n",
    "            # default: normalize over all but the second axis\n",
    "            axes = (0,) + tuple(range(2, len(self.input_shape)))\n",
    "        elif isinstance(axes, int):\n",
    "            axes = (axes,)\n",
    "        self.axes = axes\n",
    "\n",
    "        self.epsilon = utils.floatX(epsilon)\n",
    "        self.alpha = utils.floatX(alpha)\n",
    "\n",
    "        # create parameters, ignoring all dimensions in axes\n",
    "        shape = [size for axis, size in enumerate(self.input_shape)\n",
    "                 if axis not in self.axes]\n",
    "        if any(size is None for size in shape):\n",
    "            raise ValueError(\"BatchNormLayer needs specified input sizes for \"\n",
    "                             \"all axes not normalized over.\")\n",
    "        if beta is None:\n",
    "            self.beta = None\n",
    "        else:\n",
    "            self.beta = self.add_param(beta, shape, 'beta',\n",
    "                                       trainable=True, regularizable=False)\n",
    "        if gamma is None:\n",
    "            self.gamma = None\n",
    "        else:\n",
    "            self.gamma = self.add_param(gamma, shape, 'gamma',\n",
    "                                        trainable=True, regularizable=False)\n",
    "        self.mean = self.add_param(mean, shape, 'mean',\n",
    "                                   trainable=False, regularizable=False)\n",
    "        self.inv_std = self.add_param(inv_std, shape, 'inv_std',\n",
    "                                      trainable=False, regularizable=False)\n",
    "\n",
    "        self.beta = T.cast(self.beta, dtype='floatX')\n",
    "        self.gamma = T.cast(self.gamma, dtype='floatX')\n",
    "        self.mean = T.cast(self.mean, dtype='floatX')\n",
    "        self.inv_std = T.cast(self.inv_std, dtype='floatX')\n",
    "\n",
    "    def get_output_for(self, input, deterministic=False,\n",
    "                       batch_norm_use_averages=None,\n",
    "                       batch_norm_update_averages=None, **kwargs):\n",
    "        input_mean = input.mean(self.axes)\n",
    "        input_inv_std = T.inv(T.sqrt(input.var(self.axes) + self.epsilon))\n",
    "\n",
    "        # Decide whether to use the stored averages or mini-batch statistics\n",
    "        if batch_norm_use_averages is None:\n",
    "            batch_norm_use_averages = deterministic\n",
    "        use_averages = batch_norm_use_averages\n",
    "\n",
    "        if use_averages:\n",
    "            mean = self.mean\n",
    "            inv_std = self.inv_std\n",
    "        else:\n",
    "            mean = input_mean\n",
    "            inv_std = input_inv_std\n",
    "\n",
    "        # Decide whether to update the stored averages\n",
    "        if batch_norm_update_averages is None:\n",
    "            batch_norm_update_averages = not deterministic\n",
    "        update_averages = batch_norm_update_averages\n",
    "\n",
    "        if update_averages:\n",
    "            # Trick: To update the stored statistics, we create memory-aliased\n",
    "            # clones of the stored statistics:\n",
    "            running_mean = theano.clone(self.mean, share_inputs=False)\n",
    "            running_inv_std = theano.clone(self.inv_std, share_inputs=False)\n",
    "            # set a default update for them:\n",
    "            running_mean.default_update = ((1 - self.alpha) * running_mean +\n",
    "                                           self.alpha * input_mean)\n",
    "            running_inv_std.default_update = ((1 - self.alpha) *\n",
    "                                              running_inv_std +\n",
    "                                              self.alpha * input_inv_std)\n",
    "            # and make sure they end up in the graph without participating in\n",
    "            # the computation (this way their default_update will be collected\n",
    "            # and applied, but the computation will be optimized away):\n",
    "            mean += 0 * running_mean\n",
    "            inv_std += 0 * running_inv_std\n",
    "\n",
    "        # prepare dimshuffle pattern inserting broadcastable axes as needed\n",
    "        param_axes = iter(range(input.ndim - len(self.axes)))\n",
    "        pattern = ['x' if input_axis in self.axes\n",
    "                   else next(param_axes)\n",
    "                   for input_axis in range(input.ndim)]\n",
    "\n",
    "        # apply dimshuffle pattern to all parameters\n",
    "        beta = 0 if self.beta is None else self.beta.dimshuffle(pattern)\n",
    "        gamma = 1 if self.gamma is None else self.gamma.dimshuffle(pattern)\n",
    "        mean = mean.dimshuffle(pattern)\n",
    "        inv_std = inv_std.dimshuffle(pattern)\n",
    "\n",
    "        # normalize\n",
    "        normalized = (input - mean) * (gamma * inv_std) + beta\n",
    "        return normalized\n",
    "\n",
    "class GaussianSampleLayer(layers.MergeLayer):\n",
    "    def __init__(self, incoming_mu, incoming_logsigma, **kwargs):\n",
    "        super(GaussianSampleLayer, self).__init__(incomings=[incoming_mu, incoming_logsigma], **kwargs)\n",
    "        self.srng = RandomStreams(seed=234)\n",
    "\n",
    "    def get_output_shape_for(self, input_shapes):\n",
    "        return input_shapes[0]\n",
    "\n",
    "    def get_output_for(self, inputs, deterministic=False, **kwargs):\n",
    "        mu, logsigma = inputs\n",
    "        shape=(self.input_shapes[0][0] or inputs[0].shape[0],\n",
    "                self.input_shapes[0][1] or inputs[0].shape[1])\n",
    "        if deterministic:\n",
    "            return mu\n",
    "        return mu + T.exp(logsigma) * self.srng.normal(shape, avg=0.0, std=1).astype(theano.config.floatX)\n",
    "\n",
    "\n",
    "def vae_model(input_var):\n",
    "    net = {}\n",
    "    net['input'] = layers.InputLayer(shape=(None, 970), input_var=input_var)\n",
    "\n",
    "    # encode layer 1\n",
    "    net['encode1'] = layers.DenseLayer(net['input'], num_units=1000, W=init.GlorotUniform(), \n",
    "                                      b=init.Constant(0.05), nonlinearity=None)\n",
    "    net['encode1_norm'] = BatchNormLayer(net['encode1'])\n",
    "    net['encode1_active'] = layers.NonlinearityLayer(net['encode1_norm'], nonlinearity=nonlinearities.rectify)\n",
    "    net['encode1_dropout'] = layers.DropoutLayer(net['encode1_active'],p=0.5)\n",
    "\n",
    "    # encode layer 2\n",
    "    net['encode2'] = layers.DenseLayer(net['encode1_dropout'], num_units=3000, W=init.GlorotUniform(), \n",
    "                                      b=init.Constant(0.05), nonlinearity=None)\n",
    "    net['encode2_norm'] = BatchNormLayer(net['encode2'])\n",
    "    net['encode2_active'] = layers.NonlinearityLayer(net['encode2_norm'], nonlinearity=nonlinearities.rectify)\n",
    "    net['encode2_dropout'] = layers.DropoutLayer(net['encode2_active'],p=0.5)\n",
    "\n",
    "    # encode layer\n",
    "    net['Z_mu'] = layers.DenseLayer(net['encode2_dropout'], num_units=11350, W=init.GlorotUniform(), \n",
    "                                      b=init.Constant(0.05), nonlinearity=nonlinearities.linear)\n",
    "    \n",
    "    net['Z_logsigma'] = layers.DenseLayer(net['encode2_dropout'], num_units=11350, W=init.GlorotUniform(), \n",
    "                                      b=init.Constant(0.05), nonlinearity=nonlinearities.linear)\n",
    "    net['Z'] = GaussianSampleLayer(net['Z_mu'], net['Z_logsigma'])\n",
    "\n",
    "    # encode layer 2\n",
    "    net['decode2'] = layers.DenseLayer(net['Z'], num_units=3000, W=init.GlorotUniform(), \n",
    "                                  b=init.Constant(0.05), nonlinearity=None)\n",
    "    net['decode2_norm'] = BatchNormLayer(net['decode2'])\n",
    "    net['decode2_active'] = layers.NonlinearityLayer(net['decode2_norm'], nonlinearity=nonlinearities.rectify)\n",
    "    net['decode2_dropout'] = layers.DropoutLayer(net['decode2_active'],p=0.5)\n",
    "\n",
    "\n",
    "    # encode layer 1\n",
    "    net['decode1'] = layers.DenseLayer(net['decode2_dropout'], num_units=1000, W=init.GlorotUniform(), \n",
    "                                      b=init.Constant(0.05), nonlinearity=None)\n",
    "    net['decode1_norm'] = BatchNormLayer(net['decode1'])\n",
    "    net['decode1_active'] = layers.NonlinearityLayer(net['decode1_norm'], nonlinearity=nonlinearities.rectify)\n",
    "    net['decode1_dropout'] = layers.DropoutLayer(net['decode1_active'],p=0.5)\n",
    "\n",
    "\n",
    "    # encode layer\n",
    "    net['X_mu'] = layers.DenseLayer(net['decode1_dropout'], num_units=970, W=init.GlorotUniform(), \n",
    "                                      b=init.Constant(0.05), nonlinearity=nonlinearities.linear)\n",
    "    net['X_logsigma'] = layers.DenseLayer(net['decode1_dropout'], num_units=970, W=init.GlorotUniform(), \n",
    "                                      b=init.Constant(0.05), nonlinearity=nonlinearities.linear)\n",
    "    net['X'] = GaussianSampleLayer(net['X_mu'], net['X_logsigma'])\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model_prediction(best_path, landmark, mean_nonlandmark, std_nonlandmark):\n",
    "    class MultiplicativeGatingLayer(nn.layers.MergeLayer):\n",
    "        \"\"\"\n",
    "        Generic layer that combines its 3 inputs t, h1, h2 as follows:\n",
    "        y = t * h1 + (1 - t) * h2\n",
    "        \"\"\"\n",
    "        def __init__(self, gate, input1, input2, **kwargs):\n",
    "            incomings = [gate, input1, input2]\n",
    "            super(MultiplicativeGatingLayer, self).__init__(incomings, **kwargs)\n",
    "            assert gate.output_shape == input1.output_shape == input2.output_shape\n",
    "\n",
    "        def get_output_shape_for(self, input_shapes):\n",
    "            return input_shapes[0]\n",
    "\n",
    "        def get_output_for(self, inputs, **kwargs):\n",
    "            return inputs[0] * inputs[1] + (1 - inputs[0]) * inputs[2]\n",
    "    def mlp_model(input_var):\n",
    "        net = {}\n",
    "        net['input'] = layers.InputLayer(shape=(None, 970), input_var=input_var)\n",
    "\n",
    "        # encode layer 1\n",
    "        net['encode1'] = layers.DenseLayer(net['input'], num_units=1000, W=init.GlorotUniform(), \n",
    "                                          b=init.Constant(0.05), nonlinearity=None)\n",
    "        net['encode1_norm'] = BatchNormLayer(net['encode1'])\n",
    "        net['encode1_active'] = layers.NonlinearityLayer(net['encode1_norm'], nonlinearity=nonlinearities.rectify)\n",
    "        net['encode1_dropout'] = layers.DropoutLayer(net['encode1_active'],p=0.5)\n",
    "\n",
    "        # encode layer 2\n",
    "        net['encode2'] = layers.DenseLayer(net['encode1_dropout'], num_units=2000, W=init.GlorotUniform(), \n",
    "                                          b=init.Constant(0.05), nonlinearity=None)\n",
    "        net['encode2_norm'] = BatchNormLayer(net['encode2'])\n",
    "        net['encode2_active'] = layers.NonlinearityLayer(net['encode2_norm'], nonlinearity=nonlinearities.rectify)\n",
    "        net['encode2_dropout'] = layers.DropoutLayer(net['encode2_active'],p=0.5)\n",
    "\n",
    "        # encode layer 2\n",
    "        #net['encode3'] = layers.DenseLayer(net['encode2_dropout'], num_units=3000, W=init.GlorotUniform(), \n",
    "        #\t\t\t\t\t\t\t\t  b=init.Constant(0.05), nonlinearity=None)\n",
    "        #net['encode3_norm'] = BatchNormLayer(net['encode3'])\n",
    "        #net['encode3_active'] = layers.NonlinearityLayer(net['encode3_norm'], nonlinearity=nonlinearities.rectify)\n",
    "        #net['encode3_dropout'] = layers.DropoutLayer(net['encode3_active'],p=0.5)\n",
    "\n",
    "        # encode layer\n",
    "        net['output'] = layers.DenseLayer(net['encode2_dropout'], num_units=11350, W=init.GlorotUniform(), \n",
    "                                          b=init.Constant(0.05), nonlinearity=nonlinearities.linear)\n",
    "        return net\n",
    "    def dae_model(input_var):\n",
    "        net = {}\n",
    "        net['input'] = layers.InputLayer(shape=(None, 970), input_var=input_var)\n",
    "\n",
    "        # encode layer 1\n",
    "        net['encode1'] = layers.DenseLayer(net['input'], num_units=1000, W=init.GlorotUniform(), \n",
    "                                          b=init.Constant(0.05), nonlinearity=None)\n",
    "        net['encode1_norm'] = BatchNormLayer(net['encode1'])\n",
    "        net['encode1_active'] = layers.NonlinearityLayer(net['encode1_norm'], nonlinearity=nonlinearities.rectify)\n",
    "        net['encode1_dropout'] = layers.DropoutLayer(net['encode1_active'],p=0.5)\n",
    "\n",
    "        # encode layer 2\n",
    "        net['encode2'] = layers.DenseLayer(net['encode1_dropout'], num_units=2000, W=init.GlorotUniform(), \n",
    "                                          b=init.Constant(0.05), nonlinearity=None)\n",
    "        net['encode2_norm'] = BatchNormLayer(net['encode2'])\n",
    "        net['encode2_active'] = layers.NonlinearityLayer(net['encode2_norm'], nonlinearity=nonlinearities.rectify)\n",
    "        net['encode2_dropout'] = layers.DropoutLayer(net['encode2_active'],p=0.5)\n",
    "\n",
    "        # encode layer 2\n",
    "        net['encode3'] = layers.DenseLayer(net['encode2_dropout'], num_units=3000, W=init.GlorotUniform(), \n",
    "                                          b=init.Constant(0.05), nonlinearity=None)\n",
    "        net['encode3_norm'] = BatchNormLayer(net['encode3'])\n",
    "        net['encode3_active'] = layers.NonlinearityLayer(net['encode3_norm'], nonlinearity=nonlinearities.rectify)\n",
    "        net['encode3_dropout'] = layers.DropoutLayer(net['encode3_active'],p=0.5)\n",
    "\n",
    "        # encode layer\n",
    "        net['encode'] = layers.DenseLayer(net['encode3_dropout'], num_units=11350, W=init.GlorotUniform(), \n",
    "                                          b=init.Constant(0.05), nonlinearity=None)\n",
    "        net['output'] = layers.NonlinearityLayer(net['encode'], nonlinearity=nonlinearities.linear)\n",
    "        return net\n",
    "\n",
    "    def highway_dense(incoming, Wh=init.Orthogonal(), bh=init.Constant(0.0),\n",
    "                      Wt=init.Orthogonal(), bt=init.Constant(-4.0),\n",
    "                      nonlinearity=nonlinearities.rectify, **kwargs):\n",
    "        num_inputs = int(np.prod(incoming.output_shape[1:]))\n",
    "        # regular layer\n",
    "        l_h = layers.DenseLayer(incoming, num_units=num_inputs, W=Wh, b=bh,\n",
    "                                   nonlinearity=nonlinearity)\n",
    "        # gate layer\n",
    "        l_t = layers.DenseLayer(incoming, num_units=num_inputs, W=Wt, b=bt,\n",
    "                                   nonlinearity=T.nnet.sigmoid)\n",
    "\n",
    "        return MultiplicativeGatingLayer(gate=l_t, input1=l_h, input2=incoming)\n",
    "\n",
    "    def build_model(input_var, batch_size=100,\n",
    "                    num_hidden_units=500, num_hidden_layers=50):\n",
    "\n",
    "        l_in = layers.InputLayer(shape=(batch_size, 970), input_var=input_var)\n",
    "\n",
    "        # first, project it down to the desired number of units per layer\n",
    "        l_hidden1 = layers.DenseLayer(l_in, num_units=num_hidden_units)\n",
    "\n",
    "        # then stack highway layers on top of this\n",
    "        l_current = l_hidden1\n",
    "        for k in range(num_hidden_layers - 1):\n",
    "            l_current = highway_dense(l_current)\n",
    "\n",
    "\n",
    "        l_hidden2 = layers.DenseLayer(l_current, num_units=3000)\n",
    "\n",
    "        # finally add an output layer\n",
    "        l_out = layers.DenseLayer( l_hidden2, num_units=11350, nonlinearity=nonlinearities.linear)\n",
    "\n",
    "        return l_out\n",
    "\n",
    "\n",
    "    # setup model\n",
    "    input_var = T.dmatrix('landmark')\n",
    "    #network = build_model(input_var, batch_size=100, num_hidden_units=500, num_hidden_layers=50)\n",
    "    network = mlp_model(input_var)\n",
    "\n",
    "    f = open(best_path, 'rb')\n",
    "    best_parameters = cPickle.load(f)\n",
    "    f.close()\n",
    "    layers.set_all_param_values(network['output'], best_parameters)\n",
    "\n",
    "\n",
    "    prediction = layers.get_output(network['output'], deterministic=True)\n",
    "\n",
    "    get_prediction = theano.function([input_var], prediction, allow_input_downcast=True)\n",
    "    prediction = get_prediction(landmark)\n",
    "\n",
    "    prediction = prediction.transpose([1,0]).astype(np.float64)\n",
    "    num_samples = prediction.shape[1]\n",
    "    prediction = (prediction*np.outer(std_nonlandmark,np.ones(num_samples))) + np.outer(mean_nonlandmark,np.ones(num_samples))\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "savepath='/home/peter/Data/CMAP/data_set_norm.hd5f'\n",
    "trainmat = h5py.File(savepath, 'r')\n",
    "mean_landmark = np.array(trainmat['mean_landmark']).astype(np.float32)\n",
    "std_landmark = np.array(trainmat['std_landmark']).astype(np.float32)\n",
    "mean_nonlandmark = np.array(trainmat['mean_nonlandmark']).astype(np.float32)\n",
    "std_nonlandmark = np.array(trainmat['std_nonlandmark']).astype(np.float32)\n",
    "\n",
    "\n",
    "landmarkpath='/home/peter/Data/CMAP/test/landmarks.csv'\n",
    "data = pd.read_csv(landmarkpath, header=None, dtype=np.float32)\n",
    "landmark = data.as_matrix()\n",
    "\n",
    "nonlandmarkpath='/home/peter/Data/CMAP/test/truth.csv'\n",
    "data = pd.read_csv(nonlandmarkpath, header=None, dtype=np.float32)\n",
    "nonlandmark = data.as_matrix()\n",
    "\n",
    "def normalize_data(landmark, mean_landmark, std_landmark, num_samples):\n",
    "    landmark = (landmark - np.outer(mean_landmark,np.ones(num_samples)))/np.outer(std_landmark,np.ones(num_samples))\n",
    "    landmark = landmark.transpose([1,0])\n",
    "    return landmark\n",
    "\n",
    "landmark = normalize_data(landmark, mean_landmark, std_landmark, landmark.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_path = '/home/peter/Data/CMAP/Results/1hidden_epoch_5.pickle'\n",
    "prediction = model_prediction(best_path, landmark, mean_nonlandmark, std_nonlandmark)\n",
    "\n",
    "df2 = pd.DataFrame(prediction)\n",
    "df2.to_csv('/home/peter/Data/CMAP/test/prediction_ff.csv', header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77148872560580317"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path='/home/peter/Data/CMAP/test/prediction_ff.csv'\n",
    "data = pd.read_csv(path, header=None, dtype=np.float32)\n",
    "test = data.as_matrix()\n",
    "test.shape\n",
    "\n",
    "nonlandmarkpath='/home/peter/Data/CMAP/test/truth.csv'\n",
    "data = pd.read_csv(nonlandmarkpath, header=None, dtype=np.float32)\n",
    "truth = data.as_matrix()\n",
    "truth.shape\n",
    "\n",
    "R = []\n",
    "for i in range(prediction.shape[1]):\n",
    "    R.append(stats.spearmanr(prediction[i,:], truth[i,:])[0])\n",
    "np.mean(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   2.,    0.,    2.,    1.,    9.,   15.,    5.,   17.,    9.,\n",
       "          14.,   15.,   14.,   16.,   25.,   21.,   17.,   26.,   19.,\n",
       "          22.,   27.,   41.,   56.,   58.,   77.,  109.,  130.,  120.,\n",
       "          76.,   45.,   12.]),\n",
       " array([ 0.27196105,  0.29502364,  0.31808623,  0.34114883,  0.36421142,\n",
       "         0.38727401,  0.4103366 ,  0.4333992 ,  0.45646179,  0.47952438,\n",
       "         0.50258698,  0.52564957,  0.54871216,  0.57177475,  0.59483735,\n",
       "         0.61789994,  0.64096253,  0.66402513,  0.68708772,  0.71015031,\n",
       "         0.7332129 ,  0.7562755 ,  0.77933809,  0.80240068,  0.82546327,\n",
       "         0.84852587,  0.87158846,  0.89465105,  0.91771365,  0.94077624,\n",
       "         0.96383883]),\n",
       " <a list of 30 Patch objects>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEACAYAAACwB81wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBr4kNcLAl6RG/D/vT6JUxWnmpgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f83b4981410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(R,bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Rscript cmap_scoring_function.R --inf_ds prediction_ff.csv \\\n",
    "     --truth_ds truth.csv \\\n",
    "     --reference_scores refScores.csv \\\n",
    "     --out here\n",
    "\n",
    "\n",
    "java ConnectivityMap prediction_ff.csv truth.csv refScores.csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "savepath='/home/peter/Data/CMAP/data_set_norm.hd5f'\n",
    "trainmat = h5py.File(savepath, 'r')\n",
    "mean_landmark = np.array(trainmat['mean_landmark']).astype(np.float32)\n",
    "std_landmark = np.array(trainmat['std_landmark']).astype(np.float32)\n",
    "mean_nonlandmark = np.array(trainmat['mean_nonlandmark']).astype(np.float32)\n",
    "std_nonlandmark = np.array(trainmat['std_nonlandmark']).astype(np.float32)\n",
    "\n",
    "landmarkpath='/home/peter/Data/CMAP/test/testData.csv'\n",
    "data = pd.read_csv(landmarkpath, header=None, dtype=np.float32)\n",
    "landmark = data.as_matrix()\n",
    "\n",
    "def normalize_data(landmark, mean_landmark, std_landmark, num_samples):\n",
    "    landmark = (landmark - np.outer(mean_landmark,np.ones(num_samples)))/np.outer(std_landmark,np.ones(num_samples))\n",
    "    landmark = landmark.transpose([1,0])\n",
    "    return landmark\n",
    "\n",
    "landmark = normalize_data(landmark, mean_landmark, std_landmark, landmark.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "best_path = '/home/peter/Data/CMAP/Results/1hidden_epoch_5.pickle'\n",
    "prediction = model_prediction(best_path, landmark, mean_nonlandmark, std_nonlandmark)\n",
    "\n",
    "filename = 'submission14.csv'\n",
    "df2 = pd.DataFrame(prediction)\n",
    "df2.to_csv('/home/peter/Data/CMAP/test/'+filename, header=None, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "from lasagne import layers, init, nonlinearities, utils, regularization, objectives, updates\n",
    "from six.moves import cPickle\n",
    "sys.setrecursionlimit(10000)\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from lasagne.layers.base import Layer\n",
    "from scipy import stats\n",
    "import time\n",
    "import h5py\n",
    "\n",
    "np.random.seed(727) # for reproducibility\n",
    "\n",
    "\n",
    "class BatchNormLayer(Layer):\n",
    "\tdef __init__(self, incoming, axes='auto', epsilon=1e-4, alpha=0.1,\n",
    "\t\t\t\t beta=init.Constant(0), gamma=init.Constant(1),\n",
    "\t\t\t\t mean=init.Constant(0), inv_std=init.Constant(1), **kwargs):\n",
    "\t\tsuper(BatchNormLayer, self).__init__(incoming, **kwargs)\n",
    "\n",
    "\t\tif axes == 'auto':\n",
    "\t\t\t# default: normalize over all but the second axis\n",
    "\t\t\taxes = (0,) + tuple(range(2, len(self.input_shape)))\n",
    "\t\telif isinstance(axes, int):\n",
    "\t\t\taxes = (axes,)\n",
    "\t\tself.axes = axes\n",
    "\n",
    "\t\tself.epsilon = utils.floatX(epsilon)\n",
    "\t\tself.alpha = utils.floatX(alpha)\n",
    "\n",
    "\t\t# create parameters, ignoring all dimensions in axes\n",
    "\t\tshape = [size for axis, size in enumerate(self.input_shape)\n",
    "\t\t\t\t if axis not in self.axes]\n",
    "\t\tif any(size is None for size in shape):\n",
    "\t\t\traise ValueError(\"BatchNormLayer needs specified input sizes for \"\n",
    "\t\t\t\t\t\t\t \"all axes not normalized over.\")\n",
    "\t\tif beta is None:\n",
    "\t\t\tself.beta = None\n",
    "\t\telse:\n",
    "\t\t\tself.beta = self.add_param(beta, shape, 'beta',\n",
    "\t\t\t\t\t\t\t\t\t   trainable=True, regularizable=False)\n",
    "\t\tif gamma is None:\n",
    "\t\t\tself.gamma = None\n",
    "\t\telse:\n",
    "\t\t\tself.gamma = self.add_param(gamma, shape, 'gamma',\n",
    "\t\t\t\t\t\t\t\t\t\ttrainable=True, regularizable=False)\n",
    "\t\tself.mean = self.add_param(mean, shape, 'mean',\n",
    "\t\t\t\t\t\t\t\t   trainable=False, regularizable=False)\n",
    "\t\tself.inv_std = self.add_param(inv_std, shape, 'inv_std',\n",
    "\t\t\t\t\t\t\t\t\t  trainable=False, regularizable=False)\n",
    "\n",
    "\t\tself.beta = T.cast(self.beta, dtype='floatX')\n",
    "\t\tself.gamma = T.cast(self.gamma, dtype='floatX')\n",
    "\t\tself.mean = T.cast(self.mean, dtype='floatX')\n",
    "\t\tself.inv_std = T.cast(self.inv_std, dtype='floatX')\n",
    "\t\t\n",
    "\tdef get_output_for(self, input, deterministic=False,\n",
    "\t\t\t\t\t   batch_norm_use_averages=None,\n",
    "\t\t\t\t\t   batch_norm_update_averages=None, **kwargs):\n",
    "\t\tinput_mean = input.mean(self.axes)\n",
    "\t\tinput_inv_std = T.inv(T.sqrt(input.var(self.axes) + self.epsilon))\n",
    "\n",
    "\t\t# Decide whether to use the stored averages or mini-batch statistics\n",
    "\t\tif batch_norm_use_averages is None:\n",
    "\t\t\tbatch_norm_use_averages = deterministic\n",
    "\t\tuse_averages = batch_norm_use_averages\n",
    "\n",
    "\t\tif use_averages:\n",
    "\t\t\tmean = self.mean\n",
    "\t\t\tinv_std = self.inv_std\n",
    "\t\telse:\n",
    "\t\t\tmean = input_mean\n",
    "\t\t\tinv_std = input_inv_std\n",
    "\n",
    "\t\t# Decide whether to update the stored averages\n",
    "\t\tif batch_norm_update_averages is None:\n",
    "\t\t\tbatch_norm_update_averages = not deterministic\n",
    "\t\tupdate_averages = batch_norm_update_averages\n",
    "\n",
    "\t\tif update_averages:\n",
    "\t\t\t# Trick: To update the stored statistics, we create memory-aliased\n",
    "\t\t\t# clones of the stored statistics:\n",
    "\t\t\trunning_mean = theano.clone(self.mean, share_inputs=False)\n",
    "\t\t\trunning_inv_std = theano.clone(self.inv_std, share_inputs=False)\n",
    "\t\t\t# set a default update for them:\n",
    "\t\t\trunning_mean.default_update = ((1 - self.alpha) * running_mean +\n",
    "\t\t\t\t\t\t\t\t\t\t   self.alpha * input_mean)\n",
    "\t\t\trunning_inv_std.default_update = ((1 - self.alpha) *\n",
    "\t\t\t\t\t\t\t\t\t\t\t  running_inv_std +\n",
    "\t\t\t\t\t\t\t\t\t\t\t  self.alpha * input_inv_std)\n",
    "\t\t\t# and make sure they end up in the graph without participating in\n",
    "\t\t\t# the computation (this way their default_update will be collected\n",
    "\t\t\t# and applied, but the computation will be optimized away):\n",
    "\t\t\tmean += 0 * running_mean\n",
    "\t\t\tinv_std += 0 * running_inv_std\n",
    "\n",
    "\t\t# prepare dimshuffle pattern inserting broadcastable axes as needed\n",
    "\t\tparam_axes = iter(range(input.ndim - len(self.axes)))\n",
    "\t\tpattern = ['x' if input_axis in self.axes\n",
    "\t\t\t\t   else next(param_axes)\n",
    "\t\t\t\t   for input_axis in range(input.ndim)]\n",
    "\n",
    "\t\t# apply dimshuffle pattern to all parameters\n",
    "\t\tbeta = 0 if self.beta is None else self.beta.dimshuffle(pattern)\n",
    "\t\tgamma = 1 if self.gamma is None else self.gamma.dimshuffle(pattern)\n",
    "\t\tmean = mean.dimshuffle(pattern)\n",
    "\t\tinv_std = inv_std.dimshuffle(pattern)\n",
    "\n",
    "\t\t# normalize\n",
    "\t\tnormalized = (input - mean) * (gamma * inv_std) + beta\n",
    "\t\treturn normalized\n",
    "\n",
    "\n",
    "def dae_model(input_var):\n",
    "\tnet = {}\n",
    "\tnet['input'] = layers.InputLayer(shape=(None, 970), input_var=input_var)\n",
    "\n",
    "\t# encode layer 1\n",
    "\tnet['encode1'] = layers.DenseLayer(net['input'], num_units=1000, W=init.GlorotUniform(), \n",
    "\t\t\t\t\t\t\t\t\t  b=init.Constant(0.05), nonlinearity=None)\n",
    "\tnet['encode1_norm'] = BatchNormLayer(net['encode1'])\n",
    "\tnet['encode1_active'] = layers.NonlinearityLayer(net['encode1_norm'], nonlinearity=nonlinearities.rectify)\n",
    "\tnet['encode1_dropout'] = layers.DropoutLayer(net['encode1_active'],p=0.5)\n",
    "\n",
    "\t# encode layer 2\n",
    "\tnet['encode2'] = layers.DenseLayer(net['encode1_dropout'], num_units=3000, W=init.GlorotUniform(), \n",
    "\t\t\t\t\t\t\t\t\t  b=init.Constant(0.05), nonlinearity=None)\n",
    "\tnet['encode2_norm'] = BatchNormLayer(net['encode2'])\n",
    "\tnet['encode2_active'] = layers.NonlinearityLayer(net['encode2_norm'], nonlinearity=nonlinearities.rectify)\n",
    "\tnet['encode2_dropout'] = layers.DropoutLayer(net['encode2_active'],p=0.5)\n",
    "\n",
    "\t# encode layer\n",
    "\tnet['encode'] = layers.DenseLayer(net['encode2_dropout'], num_units=11350, W=init.GlorotUniform(), \n",
    "\t\t\t\t\t\t\t\t\t  b=init.Constant(0.05), nonlinearity=nonlinearities.linear)\n",
    "\n",
    "\t# encode layer 2\n",
    "\tnet['decode2'] = layers.DenseLayer(net['encode'], num_units=3000, W=net['encode2'].W.T, \n",
    "\t\t\t\t\t\t\t\t  b=init.Constant(0.05), nonlinearity=None)\n",
    "\tnet['decode2_norm'] = BatchNormLayer(net['decode2'])\n",
    "\tnet['decode2_active'] = layers.NonlinearityLayer(net['decode2_norm'], nonlinearity=nonlinearities.rectify)\n",
    "\tnet['decode2_dropout'] = layers.DropoutLayer(net['decode2_active'],p=0.5)\n",
    "\n",
    "\n",
    "\t# encode layer 1\n",
    "\tnet['decode1'] = layers.DenseLayer(net['decode2_dropout'], num_units=1000, W=net['encode1'].W.T, \n",
    "\t\t\t\t\t\t\t\t\t  b=init.Constant(0.05), nonlinearity=None)\n",
    "\tnet['decode1_norm'] = BatchNormLayer(net['decode1'])\n",
    "\tnet['decode1_active'] = layers.NonlinearityLayer(net['decode1_norm'], nonlinearity=nonlinearities.rectify)\n",
    "\tnet['decode1_dropout'] = layers.DropoutLayer(net['decode1_active'],p=0.5)\n",
    "\n",
    "\n",
    "\t# encode layer\n",
    "\tnet['decode'] = layers.DenseLayer(net['decode1_dropout'], num_units=970, W=init.GlorotUniform(), \n",
    "\t\t\t\t\t\t\t\t\t  b=init.Constant(0.05), nonlinearity=None)\n",
    "\tnet['output'] = layers.NonlinearityLayer(net['decode'], nonlinearity=nonlinearities.linear)\n",
    "\n",
    "\n",
    "\n",
    "\treturn net\n",
    "\n",
    "\n",
    "def mlp_model(input_var):\n",
    "\tnet = {}\n",
    "\tnet['input'] = layers.InputLayer(shape=(None, 970), input_var=input_var)\n",
    "\n",
    "\t# encode layer 1\n",
    "\tnet['encode1'] = layers.DenseLayer(net['input'], num_units=2000, W=init.GlorotUniform(), \n",
    "\t\t\t\t\t\t\t\t\t  b=init.Constant(0.05), nonlinearity=None)\n",
    "\tnet['encode1_norm'] = BatchNormLayer(net['encode1'])\n",
    "\tnet['encode1_active'] = layers.NonlinearityLayer(net['encode1_norm'], nonlinearity=nonlinearities.rectify)\n",
    "\tnet['encode1_dropout'] = layers.DropoutLayer(net['encode1_active'],p=0.5)\n",
    "\n",
    "\t# encode layer 2\n",
    "\t#net['encode2'] = layers.DenseLayer(net['encode1_dropout'], num_units=2000, W=init.GlorotUniform(), \n",
    "\t#\t\t\t\t\t\t\t\t  b=init.Constant(0.05), nonlinearity=None)\n",
    "\t#net['encode2_norm'] = BatchNormLayer(net['encode2'])\n",
    "\t#net['encode2_active'] = layers.NonlinearityLayer(net['encode2_norm'], nonlinearity=nonlinearities.rectify)\n",
    "\t#net['encode2_dropout'] = layers.DropoutLayer(net['encode2_active'],p=0.5)\n",
    "\n",
    "\t# encode layer 2\n",
    "\t#net['encode3'] = layers.DenseLayer(net['encode2_dropout'], num_units=3000, W=init.GlorotUniform(), \n",
    "\t#\t\t\t\t\t\t\t\t  b=init.Constant(0.05), nonlinearity=None)\n",
    "\t#net['encode3_norm'] = BatchNormLayer(net['encode3'])\n",
    "\t#net['encode3_active'] = layers.NonlinearityLayer(net['encode3_norm'], nonlinearity=nonlinearities.rectify)\n",
    "\t#net['encode3_dropout'] = layers.DropoutLayer(net['encode3_active'],p=0.5)\n",
    "\n",
    "\t# encode layer\n",
    "\tnet['encode'] = layers.DenseLayer(net['encode1_dropout'], num_units=11350, W=init.GlorotUniform(), \n",
    "\t\t\t\t\t\t\t\t\t  b=init.Constant(0.05), nonlinearity=None)\n",
    "\tnet['output'] = layers.NonlinearityLayer(net['encode'], nonlinearity=nonlinearities.linear)\n",
    "\treturn net\n",
    "\n",
    "def batch_generator(X, y, batch_size=128, shuffle=True):\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(X))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(X)-batch_size+1, batch_size):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx+batch_size]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx+batch_size)\n",
    "        yield X[excerpt], y[excerpt]\n",
    "\n",
    "\n",
    "\n",
    "#def main(trainmat, filepath):\n",
    "\n",
    "# data file and output files\n",
    "outputname = '1hidden'\n",
    "datapath='/home/peter/Data/CMAP'\n",
    "filepath = os.path.join(datapath, 'Results', outputname)\n",
    "\n",
    "trainmat = h5py.File(os.path.join(datapath, 'data_set.hd5f'), 'r')\n",
    "filepath = os.path.join(datapath, 'Results', outputname)\n",
    "landmark= np.array(trainmat['landmark']).astype(np.float32)\n",
    "nonlandmark = np.array(trainmat['nonlandmark']).astype(np.float32)\n",
    "\n",
    "def normalize_data(landmark, mean_landmark, std_landmark, num_samples):\n",
    "    landmark = (landmark - np.outer(mean_landmark,np.ones(num_samples)))/np.outer(std_landmark,np.ones(num_samples))\n",
    "    landmark = landmark.transpose([1,0])\n",
    "    return landmark.astype(np.float32)\n",
    "\n",
    "#mean_landmark = np.mean(landmark, axis=1)\n",
    "#std_landmark = np.std(landmark, axis=1)\n",
    "#landmark = normalize_data(landmark, mean_landmark, std_landmark, landmark.shape[1])\n",
    "\n",
    "#mean_nonlandmark = np.mean(nonlandmark, axis=1)\n",
    "#std_nonlandmark = np.std(nonlandmark, axis=1)\n",
    "#nonlandmark = normalize_data(nonlandmark, mean_nonlandmark, std_nonlandmark, nonlandmark.shape[1])\n",
    "\n",
    "landmark = landmark.transpose([1,0]).astype(np.float32)\n",
    "nonlandmark = nonlandmark.transpose([1,0]).astype(np.float32)\n",
    "\n",
    "\n",
    "split=10000\n",
    "test_landmark = landmark[:split]\n",
    "test_nonlandmark = nonlandmark[:split]\n",
    "landmark = landmark[split:]\n",
    "nonlandmark = nonlandmark[split:]\n",
    "\n",
    "\n",
    "# setup model\n",
    "input_var = T.dmatrix('landmark')\n",
    "network = dae_model(input_var)\n",
    "\n",
    "# setup objective \n",
    "target_var = T.dmatrix('nonlandmark')\n",
    "#prediction = layers.get_output(network['encode'], deterministic=False)\n",
    "#loss_nonlandmark = objectives.squared_error(prediction, target_var)\n",
    "#loss = loss_nonlandmark.mean()\n",
    "\n",
    "prediction = layers.get_output(network['output'], deterministic=False)\n",
    "loss_landmark = objectives.squared_error(prediction, target_var)\n",
    "loss = loss_landmark.mean()\n",
    "\n",
    "# weight-decay regularization\n",
    "all_params = layers.get_all_params(network['output'], regularizable=True)\n",
    "#l1_penalty = regularization.apply_penalty(all_params, regularization.l1) * 1e-5\n",
    "l2_penalty = regularization.apply_penalty(all_params, regularization.l2) * 1e-6        \n",
    "loss = loss + l2_penalty \n",
    "\n",
    "\n",
    "# setup updates\n",
    "learning_rate_schedule = {\n",
    "0: 0.001\n",
    "#2: 0.01,\n",
    "#5: 0.001,\n",
    "#15: 0.0001\n",
    "}\n",
    "learning_rate = theano.shared(np.float32(learning_rate_schedule[0]))\n",
    "\n",
    "all_params = layers.get_all_params(network['output'], trainable=True, deterministic=False)\n",
    "#updates = updates.nesterov_momentum(loss, all_params, learning_rate=learning_rate, momentum=0.9)\n",
    "updates = updates.adam(loss, all_params, learning_rate=learning_rate)\n",
    "\n",
    "# setup cross-validation\n",
    "test_prediction = layers.get_output(network['output'], deterministic=True)\n",
    "test_loss = objectives.squared_error(test_prediction, target_var)\n",
    "test_loss = test_loss.mean()\n",
    "\n",
    "# compile theano functions\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "valid_fn = theano.function([input_var, target_var], [test_loss, test_prediction])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
