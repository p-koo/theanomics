{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "sys.path.append('..')\n",
    "from src import NeuralNet\n",
    "from src import train as fit\n",
    "from src import make_directory \n",
    "from models import load_model\n",
    "from data import load_data\n",
    "from six.moves import cPickle\n",
    "np.random.seed(247) # for reproducibility\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'figure.autolayout': True})\n",
    "%matplotlib inline\n",
    "from scipy.misc import imresize\n",
    "\n",
    "from lasagne import layers, nonlinearities, updates, objectives, init \n",
    "from lasagne.layers import Conv2DLayer, TransposedConv2DLayer, DenseLayer, InputLayer, ExpressionLayer, BiasLayer\n",
    "\n",
    "from lasagne.layers import get_output, get_output_shape, get_all_params\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "np.random.seed(247) # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle\n",
      "loading train data\n",
      "loading cross-validation data\n",
      "loading test data\n"
     ]
    }
   ],
   "source": [
    "filename = 'Unlocalized_N=100000_S=200_M=50_G=20_data.pickle'\n",
    "datapath = '/home/peter/Data/SequenceMotif'\n",
    "filepath = os.path.join(datapath, filename)\n",
    "\n",
    "# load training set\n",
    "print \"loading data from: \" + filepath\n",
    "f = open(filepath, 'rb')\n",
    "print \"loading train data\"\n",
    "train = cPickle.load(f)\n",
    "print \"loading cross-validation data\"\n",
    "cross_validation = cPickle.load(f)\n",
    "print \"loading test data\"\n",
    "test = cPickle.load(f)\n",
    "f.close()\n",
    "\n",
    "X_train = train[0].transpose((0,1,2)).astype(np.float32)\n",
    "y_train = train[1].astype(np.int32)\n",
    "X_val = cross_validation[0].transpose((0,1,2)).astype(np.float32)\n",
    "y_val = cross_validation[1].astype(np.int32)\n",
    "X_test = test[0].transpose((0,1,2)).astype(np.float32)\n",
    "y_test = test[1].astype(np.int32)\n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=3)\n",
    "X_val = np.expand_dims(X_val, axis=3)\n",
    "X_test = np.expand_dims(X_test, axis=3)\n",
    "\n",
    "train = (X_train, y_train, train[2])\n",
    "valid = (X_val, y_val, cross_validation[2])\n",
    "test = (X_test, y_test, test[2])\n",
    "\n",
    "shape = (None, train[0].shape[1], train[0].shape[2], train[0].shape[3])\n",
    "num_labels = train[1].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_name = \"autoencode_motif_model\"\n",
    "nnmodel = NeuralNet(model_name, shape, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.62469 -- accuracy=96.69%  \n",
      "  valid loss:\t\t0.34866\n",
      "  valid accuracy:\t0.97487+/-0.01821\n",
      "  valid auc-roc:\t0.94353+/-0.06067\n",
      "  valid auc-pr:\t\t0.66452+/-0.30329\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_0.pickle\n",
      "Epoch 2 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.27481 -- accuracy=97.87%  \n",
      "  valid loss:\t\t0.21779\n",
      "  valid accuracy:\t0.97921+/-0.01729\n",
      "  valid auc-roc:\t0.96768+/-0.03402\n",
      "  valid auc-pr:\t\t0.74875+/-0.25617\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_1.pickle\n",
      "Epoch 3 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.18341 -- accuracy=98.11%  \n",
      "  valid loss:\t\t0.16256\n",
      "  valid accuracy:\t0.98157+/-0.01576\n",
      "  valid auc-roc:\t0.97544+/-0.02507\n",
      "  valid auc-pr:\t\t0.78476+/-0.23307\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_2.pickle\n",
      "Epoch 4 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.14778 -- accuracy=98.25%  \n",
      "  valid loss:\t\t0.13497\n",
      "  valid accuracy:\t0.98301+/-0.01553\n",
      "  valid auc-roc:\t0.97843+/-0.02100\n",
      "  valid auc-pr:\t\t0.80378+/-0.21242\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_3.pickle\n",
      "Epoch 5 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.12624 -- accuracy=98.32%  \n",
      "  valid loss:\t\t0.11646\n",
      "  valid accuracy:\t0.98408+/-0.01482\n",
      "  valid auc-roc:\t0.97957+/-0.02000\n",
      "  valid auc-pr:\t\t0.81816+/-0.19828\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_4.pickle\n",
      "Epoch 6 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.11243 -- accuracy=98.35%  \n",
      "  valid loss:\t\t0.10640\n",
      "  valid accuracy:\t0.98400+/-0.01456\n",
      "  valid auc-roc:\t0.97973+/-0.02058\n",
      "  valid auc-pr:\t\t0.81625+/-0.20597\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_5.pickle\n",
      "Epoch 7 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.10443 -- accuracy=98.37%  \n",
      "  valid loss:\t\t0.09971\n",
      "  valid accuracy:\t0.98454+/-0.01456\n",
      "  valid auc-roc:\t0.98095+/-0.01867\n",
      "  valid auc-pr:\t\t0.82153+/-0.19912\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_6.pickle\n",
      "Epoch 8 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.09929 -- accuracy=98.40%  \n",
      "  valid loss:\t\t0.09597\n",
      "  valid accuracy:\t0.98438+/-0.01466\n",
      "  valid auc-roc:\t0.98161+/-0.01868\n",
      "  valid auc-pr:\t\t0.82421+/-0.19865\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_7.pickle\n",
      "Epoch 9 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.09608 -- accuracy=98.41%  \n",
      "  valid loss:\t\t0.09382\n",
      "  valid accuracy:\t0.98446+/-0.01450\n",
      "  valid auc-roc:\t0.98145+/-0.01812\n",
      "  valid auc-pr:\t\t0.82540+/-0.19226\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_8.pickle\n",
      "Epoch 10 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.09413 -- accuracy=98.41%  \n",
      "  valid loss:\t\t0.09213\n",
      "  valid accuracy:\t0.98438+/-0.01417\n",
      "  valid auc-roc:\t0.98167+/-0.01712\n",
      "  valid auc-pr:\t\t0.82291+/-0.20167\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_9.pickle\n",
      "Epoch 11 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.09250 -- accuracy=98.42%  \n",
      "  valid loss:\t\t0.09095\n",
      "  valid accuracy:\t0.98452+/-0.01430\n",
      "  valid auc-roc:\t0.98110+/-0.01812\n",
      "  valid auc-pr:\t\t0.82433+/-0.19301\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_10.pickle\n",
      "Epoch 12 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.09119 -- accuracy=98.42%  \n",
      "  valid loss:\t\t0.08990\n",
      "  valid accuracy:\t0.98453+/-0.01423\n",
      "  valid auc-roc:\t0.98013+/-0.01958\n",
      "  valid auc-pr:\t\t0.82409+/-0.19401\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_11.pickle\n",
      "Epoch 13 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.09012 -- accuracy=98.43%  \n",
      "  valid loss:\t\t0.08891\n",
      "  valid accuracy:\t0.98460+/-0.01429\n",
      "  valid auc-roc:\t0.98185+/-0.01781\n",
      "  valid auc-pr:\t\t0.82609+/-0.19101\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_12.pickle\n",
      "Epoch 14 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.08929 -- accuracy=98.43%  \n",
      "  valid loss:\t\t0.08843\n",
      "  valid accuracy:\t0.98481+/-0.01442\n",
      "  valid auc-roc:\t0.98090+/-0.01889\n",
      "  valid auc-pr:\t\t0.82545+/-0.19252\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_13.pickle\n",
      "Epoch 15 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.08828 -- accuracy=98.44%  \n",
      "  valid loss:\t\t0.08725\n",
      "  valid accuracy:\t0.98464+/-0.01443\n",
      "  valid auc-roc:\t0.98185+/-0.01795\n",
      "  valid auc-pr:\t\t0.82399+/-0.19601\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_14.pickle\n",
      "Epoch 16 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.08785 -- accuracy=98.43%  \n",
      "  valid loss:\t\t0.08611\n",
      "  valid accuracy:\t0.98490+/-0.01440\n",
      "  valid auc-roc:\t0.98219+/-0.01823\n",
      "  valid auc-pr:\t\t0.83187+/-0.18841\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_15.pickle\n",
      "Epoch 17 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.08741 -- accuracy=98.43%  \n",
      "  valid loss:\t\t0.08629\n",
      "  valid accuracy:\t0.98442+/-0.01492\n",
      "  valid auc-roc:\t0.98166+/-0.01775\n",
      "  valid auc-pr:\t\t0.83110+/-0.18426\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_16.pickle\n",
      "Epoch 18 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.08684 -- accuracy=98.44%  \n",
      "  valid loss:\t\t0.08487\n",
      "  valid accuracy:\t0.98472+/-0.01447\n",
      "  valid auc-roc:\t0.98293+/-0.01585\n",
      "  valid auc-pr:\t\t0.83166+/-0.18486\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_17.pickle\n",
      "Epoch 19 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.08648 -- accuracy=98.43%  \n",
      "  valid loss:\t\t0.08562\n",
      "  valid accuracy:\t0.98489+/-0.01424\n",
      "  valid auc-roc:\t0.98250+/-0.01619\n",
      "  valid auc-pr:\t\t0.83144+/-0.18169\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_18.pickle\n",
      "Epoch 20 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.08619 -- accuracy=98.43%  \n",
      "  valid loss:\t\t0.08471\n",
      "  valid accuracy:\t0.98486+/-0.01426\n",
      "  valid auc-roc:\t0.98213+/-0.01716\n",
      "  valid auc-pr:\t\t0.83249+/-0.18113\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_19.pickle\n",
      "Epoch 21 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.08559 -- accuracy=98.45%  \n",
      "  valid loss:\t\t0.08457\n",
      "  valid accuracy:\t0.98461+/-0.01414\n",
      "  valid auc-roc:\t0.98271+/-0.01586\n",
      "  valid auc-pr:\t\t0.83427+/-0.17412\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_20.pickle\n",
      "Epoch 22 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.08529 -- accuracy=98.44%  \n",
      "  valid loss:\t\t0.08414\n",
      "  valid accuracy:\t0.98486+/-0.01425\n",
      "  valid auc-roc:\t0.98214+/-0.01639\n",
      "  valid auc-pr:\t\t0.83624+/-0.17206\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_21.pickle\n",
      "Epoch 23 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.08517 -- accuracy=98.44%  \n",
      "  valid loss:\t\t0.08393\n",
      "  valid accuracy:\t0.98468+/-0.01440\n",
      "  valid auc-roc:\t0.98200+/-0.01636\n",
      "  valid auc-pr:\t\t0.83100+/-0.18005\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_22.pickle\n",
      "Epoch 24 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.08451 -- accuracy=98.44%  \n",
      "  valid loss:\t\t0.08358\n",
      "  valid accuracy:\t0.98459+/-0.01454\n",
      "  valid auc-roc:\t0.98191+/-0.01669\n",
      "  valid auc-pr:\t\t0.83661+/-0.16636\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_23.pickle\n",
      "Epoch 25 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.08406 -- accuracy=98.45%  \n",
      "  valid loss:\t\t0.08329\n",
      "  valid accuracy:\t0.98464+/-0.01456\n",
      "  valid auc-roc:\t0.98147+/-0.01724\n",
      "  valid auc-pr:\t\t0.83685+/-0.16615\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_24.pickle\n",
      "Epoch 26 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.08390 -- accuracy=98.44%  \n",
      "  valid loss:\t\t0.08245\n",
      "  valid accuracy:\t0.98470+/-0.01447\n",
      "  valid auc-roc:\t0.98300+/-0.01533\n",
      "  valid auc-pr:\t\t0.83942+/-0.16508\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_25.pickle\n",
      "Epoch 27 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.08374 -- accuracy=98.44%  \n",
      "  valid loss:\t\t0.08335\n",
      "  valid accuracy:\t0.98447+/-0.01419\n",
      "  valid auc-roc:\t0.98190+/-0.01606\n",
      "  valid auc-pr:\t\t0.83415+/-0.17274\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_26.pickle\n",
      "Epoch 28 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.08323 -- accuracy=98.45%  \n",
      "  valid loss:\t\t0.08163\n",
      "  valid accuracy:\t0.98487+/-0.01437\n",
      "  valid auc-roc:\t0.98206+/-0.01631\n",
      "  valid auc-pr:\t\t0.83569+/-0.16782\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_27.pickle\n",
      "Epoch 29 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.08315 -- accuracy=98.45%  \n",
      "  valid loss:\t\t0.08284\n",
      "  valid accuracy:\t0.98461+/-0.01454\n",
      "  valid auc-roc:\t0.98184+/-0.01709\n",
      "  valid auc-pr:\t\t0.83381+/-0.17546\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_28.pickle\n",
      "Epoch 30 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.08299 -- accuracy=98.44%  \n",
      "  valid loss:\t\t0.08198\n",
      "  valid accuracy:\t0.98495+/-0.01436\n",
      "  valid auc-roc:\t0.98216+/-0.01647\n",
      "  valid auc-pr:\t\t0.83917+/-0.16563\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_29.pickle\n",
      "Epoch 31 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.08310 -- accuracy=98.45%  \n",
      "  valid loss:\t\t0.08177\n",
      "  valid accuracy:\t0.98475+/-0.01460\n",
      "  valid auc-roc:\t0.98279+/-0.01560\n",
      "  valid auc-pr:\t\t0.84117+/-0.16301\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_30.pickle\n",
      "Epoch 32 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.08285 -- accuracy=98.44%  \n",
      "  valid loss:\t\t0.08158\n",
      "  valid accuracy:\t0.98476+/-0.01413\n",
      "  valid auc-roc:\t0.98244+/-0.01668\n",
      "  valid auc-pr:\t\t0.83674+/-0.17347\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_31.pickle\n",
      "Epoch 33 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.08268 -- accuracy=98.45%  \n",
      "  valid loss:\t\t0.08302\n",
      "  valid accuracy:\t0.98407+/-0.01496\n",
      "  valid auc-roc:\t0.98256+/-0.01641\n",
      "  valid auc-pr:\t\t0.83478+/-0.17409\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_32.pickle\n",
      "Epoch 34 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.08275 -- accuracy=98.45%  \n",
      "  valid loss:\t\t0.08118\n",
      "  valid accuracy:\t0.98508+/-0.01413\n",
      "  valid auc-roc:\t0.98302+/-0.01574\n",
      "  valid auc-pr:\t\t0.83845+/-0.17014\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_33.pickle\n",
      "Epoch 35 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.08243 -- accuracy=98.45%  \n",
      "  valid loss:\t\t0.08138\n",
      "  valid accuracy:\t0.98494+/-0.01435\n",
      "  valid auc-roc:\t0.98170+/-0.01797\n",
      "  valid auc-pr:\t\t0.83761+/-0.17083\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_34.pickle\n",
      "Epoch 36 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.08244 -- accuracy=98.45%  \n",
      "  valid loss:\t\t0.08124\n",
      "  valid accuracy:\t0.98483+/-0.01437\n",
      "  valid auc-roc:\t0.98207+/-0.01757\n",
      "  valid auc-pr:\t\t0.83984+/-0.16375\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_35.pickle\n",
      "Epoch 37 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.08241 -- accuracy=98.45%  \n",
      "  valid loss:\t\t0.08201\n",
      "  valid accuracy:\t0.98462+/-0.01441\n",
      "  valid auc-roc:\t0.98247+/-0.01625\n",
      "  valid auc-pr:\t\t0.83865+/-0.16781\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_36.pickle\n",
      "Epoch 38 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.08219 -- accuracy=98.44%  \n",
      "  valid loss:\t\t0.08077\n",
      "  valid accuracy:\t0.98488+/-0.01427\n",
      "  valid auc-roc:\t0.98295+/-0.01587\n",
      "  valid auc-pr:\t\t0.83917+/-0.16542\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_37.pickle\n",
      "Epoch 39 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.08232 -- accuracy=98.43%  \n",
      "  valid loss:\t\t0.08282\n",
      "  valid accuracy:\t0.98427+/-0.01388\n",
      "  valid auc-roc:\t0.98262+/-0.01656\n",
      "  valid auc-pr:\t\t0.83520+/-0.17405\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_38.pickle\n",
      "Epoch 40 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.08202 -- accuracy=98.45%  \n",
      "  valid loss:\t\t0.08090\n",
      "  valid accuracy:\t0.98472+/-0.01457\n",
      "  valid auc-roc:\t0.98180+/-0.01809\n",
      "  valid auc-pr:\t\t0.83709+/-0.17197\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_39.pickle\n",
      "Epoch 41 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.08197 -- accuracy=98.45%  \n",
      "  valid loss:\t\t0.08072\n",
      "  valid accuracy:\t0.98489+/-0.01432\n",
      "  valid auc-roc:\t0.98323+/-0.01573\n",
      "  valid auc-pr:\t\t0.83753+/-0.17349\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_40.pickle\n",
      "Epoch 42 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.08178 -- accuracy=98.45%  \n",
      "  valid loss:\t\t0.08094\n",
      "  valid accuracy:\t0.98471+/-0.01404\n",
      "  valid auc-roc:\t0.98337+/-0.01508\n",
      "  valid auc-pr:\t\t0.84319+/-0.16025\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_41.pickle\n",
      "Epoch 43 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.08179 -- accuracy=98.45%  \n",
      "  valid loss:\t\t0.08227\n",
      "  valid accuracy:\t0.98434+/-0.01472\n",
      "  valid auc-roc:\t0.98230+/-0.01687\n",
      "  valid auc-pr:\t\t0.83323+/-0.17846\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_42.pickle\n",
      "Epoch 44 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.08181 -- accuracy=98.44%  \n",
      "  valid loss:\t\t0.08155\n",
      "  valid accuracy:\t0.98482+/-0.01417\n",
      "  valid auc-roc:\t0.98282+/-0.01610\n",
      "  valid auc-pr:\t\t0.84034+/-0.16427\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_43.pickle\n",
      "Epoch 45 out of 500 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.08197 -- accuracy=98.45%  \n",
      "  valid loss:\t\t0.08088\n",
      "  valid accuracy:\t0.98463+/-0.01471\n",
      "  valid auc-roc:\t0.98366+/-0.01562\n",
      "  valid auc-pr:\t\t0.83956+/-0.16782\n",
      "saving model parameters to: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=50_G=20_data.pickle_epoch_44.pickle\n",
      "Patience ran out... Early stopping.\n"
     ]
    }
   ],
   "source": [
    "nnmodel = fit.train_minibatch(nnmodel, train, valid, batch_size=128, num_epochs=500, \n",
    "                        patience=3, verbose=1, filepath=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv1': <lasagne.layers.dnn.Conv2DDNNLayer at 0x7fe91a1b87d0>,\n",
       " 'conv1_active': <lasagne.layers.special.NonlinearityLayer at 0x7fe9204c7ed0>,\n",
       " 'conv1_batch': <lasagne.layers.normalization.BatchNormLayer at 0x7fe9204c7e90>,\n",
       " 'conv1_bias': <lasagne.layers.special.BiasLayer at 0x7fe91a1b8810>,\n",
       " 'conv1_pool': <lasagne.layers.pool.MaxPool2DLayer at 0x7fe92049bbd0>,\n",
       " 'conv2': <lasagne.layers.dnn.Conv2DDNNLayer at 0x7fe92049bc10>,\n",
       " 'conv2_active': <lasagne.layers.special.NonlinearityLayer at 0x7fe92049bdd0>,\n",
       " 'conv2_batch': <lasagne.layers.normalization.BatchNormLayer at 0x7fe92049bd90>,\n",
       " 'conv2_bias': <lasagne.layers.special.BiasLayer at 0x7fe92049bc50>,\n",
       " 'conv2_pool': <lasagne.layers.pool.MaxPool2DLayer at 0x7fe9204a7350>,\n",
       " 'conv3': <lasagne.layers.dnn.Conv2DDNNLayer at 0x7fe9204a7390>,\n",
       " 'conv3_active': <lasagne.layers.special.NonlinearityLayer at 0x7fe9204a7610>,\n",
       " 'conv3_batch': <lasagne.layers.normalization.BatchNormLayer at 0x7fe9204a75d0>,\n",
       " 'conv3_bias': <lasagne.layers.special.BiasLayer at 0x7fe9204a73d0>,\n",
       " 'conv3_pool': <lasagne.layers.pool.MaxPool2DLayer at 0x7fe9204a7b50>,\n",
       " 'conv4': <lasagne.layers.dnn.Conv2DDNNLayer at 0x7fe9204a7b90>,\n",
       " 'conv4_active': <lasagne.layers.special.NonlinearityLayer at 0x7fe9204a7d10>,\n",
       " 'conv4_batch': <lasagne.layers.normalization.BatchNormLayer at 0x7fe9204a7cd0>,\n",
       " 'conv4_bias': <lasagne.layers.special.BiasLayer at 0x7fe9204a7450>,\n",
       " 'conv4_pool': <lasagne.layers.pool.MaxPool2DLayer at 0x7fe9204b1290>,\n",
       " 'dense': <lasagne.layers.dense.DenseLayer at 0x7fe9204b12d0>,\n",
       " 'dense_active': <lasagne.layers.special.NonlinearityLayer at 0x7fe9204b17d0>,\n",
       " 'dense_bias': <lasagne.layers.special.BiasLayer at 0x7fe9204b1310>,\n",
       " 'input': <lasagne.layers.input.InputLayer at 0x7fe91a1b8790>,\n",
       " 'output': <lasagne.layers.special.NonlinearityLayer at 0x7fe9204b17d0>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = nnmodel.network\n",
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VariationalSampleLayer(layers.MergeLayer):\n",
    "    def __init__(self, incoming_mu, incoming_logsigma, **kwargs):\n",
    "        super(VariationalSampleLayer, self).__init__(incomings=[incoming_mu, incoming_logsigma], **kwargs)\n",
    "        self.srng = RandomStreams(seed=234)\n",
    "\n",
    "    def get_output_shape_for(self, input_shapes):\n",
    "        return input_shapes[0]\n",
    "\n",
    "    def get_output_for(self, inputs, deterministic=False, **kwargs):\n",
    "        mu, logsigma = inputs\n",
    "        shape=(self.input_shapes[0][0] or inputs[0].shape[0],\n",
    "                self.input_shapes[0][1] or inputs[0].shape[1])\n",
    "        if deterministic:\n",
    "            return mu\n",
    "        return mu + T.exp(logsigma) * self.srng.normal(shape, avg=0.0, std=1).astype(theano.config.floatX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_encode = 40\n",
    "\n",
    "input_var = T.tensor4('inputs')\n",
    "net = {}\n",
    "\n",
    "# inference network\n",
    "net['input'] = layers.InputLayer(shape=shape, input_var=input_var)\n",
    "net['conv1'] = layers.Conv2DLayer(net['input'],  num_filters=network['conv1'].output_shape[1],\n",
    "                                  filter_size=network['conv1'].filter_size,\n",
    "                                  W=network['conv1'].W,\n",
    "                                  b=init.Constant(0.05), \n",
    "                                  pad='same',\n",
    "                                  nonlinearity=None, flip_filters=False)\n",
    "net['conv1_norm'] = layers.BatchNormLayer(net['conv1'])\n",
    "net['conv1_active'] = layers.NonlinearityLayer(net['conv1_norm'], nonlinearity=nonlinearities.rectify) \n",
    "net['conv1_pool'] = layers.MaxPool2DLayer(net['conv1_active'], pool_size=network['conv1_pool'].pool_size)\n",
    "\n",
    "net['conv2'] = layers.Conv2DLayer(net['conv1_pool'],  num_filters=network['conv2'].output_shape[1],\n",
    "                                  filter_size=network['conv2'].filter_size,\n",
    "                                  W=network['conv2'].W,\n",
    "                                  b=init.Constant(0.05), \n",
    "                                  pad='same',\n",
    "                                  nonlinearity=None, flip_filters=False)\n",
    "net['conv2_norm'] = layers.BatchNormLayer(net['conv2'])\n",
    "net['conv2_active'] = layers.NonlinearityLayer(net['conv2_norm'], nonlinearity=nonlinearities.rectify) \n",
    "net['conv2_pool'] = layers.MaxPool2DLayer(net['conv2_active'], pool_size=network['conv2_pool'].pool_size)\n",
    "\n",
    "net['conv3'] = layers.Conv2DLayer(net['conv2_pool'],  num_filters=network['conv3'].output_shape[1],\n",
    "                                  filter_size=network['conv3'].filter_size,\n",
    "                                  W=network['conv3'].W,\n",
    "                                  b=init.Constant(0.05), \n",
    "                                  pad='same',\n",
    "                                  nonlinearity=None, flip_filters=False)\n",
    "net['conv3_norm'] = layers.BatchNormLayer(net['conv3'])\n",
    "net['conv3_active'] = layers.NonlinearityLayer(net['conv3_norm'], nonlinearity=nonlinearities.rectify) \n",
    "net['conv3_pool'] = layers.MaxPool2DLayer(net['conv3_active'], pool_size=network['conv3_pool'].pool_size)\n",
    "\n",
    "net['conv4'] = layers.Conv2DLayer(net['conv3_pool'],  num_filters=network['conv4'].output_shape[1],\n",
    "                                  filter_size=network['conv1'].filter_size,\n",
    "                                  W=network['conv4'].W,\n",
    "                                  b=init.Constant(0.05), \n",
    "                                  pad='same',\n",
    "                                  nonlinearity=None, flip_filters=False)\n",
    "net['conv4_norm'] = layers.BatchNormLayer(net['conv4'])\n",
    "net['conv4_active'] = layers.NonlinearityLayer(net['conv4_norm'], nonlinearity=nonlinearities.rectify) \n",
    "net['conv4_pool'] = layers.MaxPool2DLayer(net['conv4_active'], pool_size=network['conv4_pool'].pool_size)\n",
    "\n",
    "\n",
    "# variational layer\n",
    "net['encode_mu'] = layers.DenseLayer(net['conv4_pool'], num_units=num_encode, W=init.GlorotUniform(), \n",
    "                                  b=init.Constant(.0), nonlinearity=nonlinearities.linear)\n",
    "net['encode_logsigma'] = layers.DenseLayer(net['conv4_pool'], num_units=num_encode, W=init.GlorotUniform(), \n",
    "                                  b=init.Constant(.0), nonlinearity=nonlinearities.linear)\n",
    "net['Z'] = VariationalSampleLayer(net['encode_mu'], net['encode_logsigma'])\n",
    "\n",
    "\n",
    "# generative network\n",
    "num_units = np.prod(list(network['dense'].input_shape)[1:])\n",
    "net['dense'] = layers.DenseLayer(net['Z'], num_units=num_units, W=init.GlorotUniform(), \n",
    "                                  b=init.Constant(.0), nonlinearity=nonlinearities.rectify)\n",
    "shape2 = list(get_output_shape(net['conv4_pool']))\n",
    "shape2[0] = -1\n",
    "net['reshape'] = layers.ReshapeLayer(net['dense'], shape=tuple(shape2))\n",
    "\n",
    "net['invpool4'] = layers.Upscale2DLayer(net['reshape'], net['conv4_pool'].pool_size)\n",
    "#net['invpool4'] = layers.InverseLayer(net['reshape'], net['conv4_pool'])\n",
    "net['invconv4']  = Conv2DLayer(net['invpool4'], num_filters=net['conv4'].input_shape[1],\n",
    "                                          filter_size=net['conv4'].filter_size,\n",
    "                                          W=init.GlorotUniform(),\n",
    "                                          b=init.Constant(0.05), \n",
    "                                          pad='same',\n",
    "                                          nonlinearity=None)\n",
    "net['invconv4_norm'] = layers.BatchNormLayer(net['invconv4'])\n",
    "net['invconv4_active'] = layers.NonlinearityLayer(net['invconv4_norm'], nonlinearity=nonlinearities.rectify) \n",
    "\n",
    "net['invpool3'] = layers.Upscale2DLayer(net['invconv4_active'], net['conv3_pool'].pool_size)\n",
    "#net['invpool3'] = layers.InverseLayer(net['invconv4_active'], net['conv3_pool'])\n",
    "net['invconv3']  = Conv2DLayer(net['invpool3'], num_filters=net['conv3'].input_shape[1],\n",
    "                                          filter_size=net['conv3'].filter_size,\n",
    "                                          W=init.GlorotUniform(),\n",
    "                                          b=init.Constant(0.05), \n",
    "                                          pad='same',\n",
    "                                          nonlinearity=None)\n",
    "net['invconv3_norm'] = layers.BatchNormLayer(net['invconv3'])\n",
    "net['invconv3_active'] = layers.NonlinearityLayer(net['invconv3_norm'], nonlinearity=nonlinearities.rectify) \n",
    "\n",
    "\n",
    "net['invpool2'] = layers.Upscale2DLayer(net['invconv3_active'], net['conv2_pool'].pool_size)\n",
    "#net['invpool2'] = layers.InverseLayer(net['invconv2_active'], net['conv2_pool'])\n",
    "net['invconv2']  = Conv2DLayer(net['invpool2'], num_filters=net['conv2'].input_shape[1],\n",
    "                                          filter_size=net['conv2'].filter_size,\n",
    "                                          W=init.GlorotUniform(),\n",
    "                                          b=init.Constant(0.05), \n",
    "                                          pad='same',\n",
    "                                          nonlinearity=None)\n",
    "net['invconv2_norm'] = layers.BatchNormLayer(net['invconv2'])\n",
    "net['invconv2_active'] = layers.NonlinearityLayer(net['invconv2_norm'], nonlinearity=nonlinearities.rectify) \n",
    "\n",
    "\n",
    "net['invpool1'] = layers.Upscale2DLayer(net['invconv2_active'], net['conv1_pool'].pool_size)\n",
    "#net['invpool1'] = layers.InverseLayer(net['invconv2_active'], net['conv1_pool'])\n",
    "net['invconv1']  = Conv2DLayer(net['invpool1'], num_filters=net['conv1'].input_shape[1],\n",
    "                                          filter_size=net['conv1'].filter_size,\n",
    "                                          W=init.GlorotUniform(),\n",
    "                                          b=init.Constant(0.05), \n",
    "                                          pad='same',\n",
    "                                          nonlinearity=None)\n",
    "net['invconv1_norm'] = layers.BatchNormLayer(net['invconv1'])\n",
    "net['invconv1_active'] = layers.NonlinearityLayer(net['invconv1_norm'], nonlinearity=nonlinearities.sigmoid) \n",
    "\n",
    "net['X'] = net['invconv4_active']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_loss(net, target_var, deterministic):\n",
    "    \n",
    "    z_mu = get_output(net['encode_mu'], deterministic=deterministic)\n",
    "    z_logsigma = get_output(net['encode_logsigma'], deterministic=deterministic)\n",
    "    kl_divergence = 0.5*T.sum(1 + 2*z_logsigma - T.sqr(z_mu) - T.exp(2*z_logsigma), axis=1)\n",
    "    x_mu = get_output(net['X'], deterministic=deterministic)\n",
    "    x_mu = T.clip(x_mu, 1e-7, 1-1e-7)\n",
    "    x_logsigma = .5#T.log(T.sqrt(x_mu*(1-x_mu)))\n",
    "    log_likelihood = T.sum(-0.5*T.log(2*np.float32(np.pi))- x_logsigma - 0.5*T.sqr(target_var-x_mu)/T.exp(2*x_logsigma),axis=1)\n",
    "    #log_likelihood = T.sum(target_var*T.log(1e-10+x_mu) + (1.0-target_var)*T.log(1e-10+1.0-x_mu), axis=1)\n",
    "    variational_lower_bound = -log_likelihood - kl_divergence\n",
    "    prediction = x_mu\n",
    "    return variational_lower_bound.mean(), prediction, kl_divergence, log_likelihood \n",
    "train_loss, train_prediction, train_kl, train_like = build_loss(net, input_var, deterministic=False)\n",
    "test_loss, test_prediction, test_kl, test_like = build_loss(net, input_var, deterministic=True)\n",
    "\n",
    "\n",
    "# ADAM updates\n",
    "params = get_all_params(net['X'], trainable=True)\n",
    "update_op = updates.adam(train_loss, params, learning_rate=.0001)\n",
    "train_fun = theano.function([input_var], [train_loss] , updates=update_op)\n",
    "valid_fun = theano.function([input_var], [test_loss, test_prediction, test_kl, test_like])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_generator(X, batch_size=128, shuffle=True):\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(X))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(X)-batch_size+1, batch_size):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx+batch_size]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx+batch_size)\n",
    "        yield X[excerpt].astype(np.float32)\n",
    "        \n",
    "        \n",
    "num_epochs = 100\n",
    "batch_size = 100\n",
    "num_train_batches = X_train.shape[0] // batch_size\n",
    "num_valid_batches = X_valid.shape[0] // batch_size\n",
    "    \n",
    "for epoch in range(num_epochs):\n",
    "    train_batches = batch_generator(X_train, batch_size, shuffle=True)\n",
    "    valid_batches = batch_generator(X_valid, batch_size, shuffle=False)\n",
    "\n",
    "    train_loss = 0\n",
    "    for index in range(num_train_batches):\n",
    "        loss = train_fun(next(train_batches))\n",
    "        train_loss += loss[0]\n",
    "        \n",
    "    print(\"Epoch {} of {}\".format(epoch+1, num_epochs))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_loss/num_train_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def seq_logo(pwm, height=100, nt_width=20, norm=0, rna=1, filepath='.'):\n",
    "    \"\"\"generate a sequence logo from a pwm\"\"\"\n",
    "    \n",
    "    def load_alphabet(filepath, rna):\n",
    "        \"\"\"load images of nucleotide alphabet \"\"\"\n",
    "        df = pd.read_table(os.path.join(filepath, 'A.txt'), header=None);\n",
    "        A_img = df.as_matrix()\n",
    "        A_img = np.reshape(A_img, [72, 65, 3], order=\"F\").astype(np.uint8)\n",
    "\n",
    "        df = pd.read_table(os.path.join(filepath, 'C.txt'), header=None);\n",
    "        C_img = df.as_matrix()\n",
    "        C_img = np.reshape(C_img, [76, 64, 3], order=\"F\").astype(np.uint8)\n",
    "\n",
    "        df = pd.read_table(os.path.join(filepath, 'G.txt'), header=None);\n",
    "        G_img = df.as_matrix()\n",
    "        G_img = np.reshape(G_img, [76, 67, 3], order=\"F\").astype(np.uint8)\n",
    "\n",
    "        if rna == 1:\n",
    "            df = pd.read_table(os.path.join(filepath, 'U.txt'), header=None);\n",
    "            T_img = df.as_matrix()\n",
    "            T_img = np.reshape(T_img, [74, 57, 3], order=\"F\").astype(np.uint8)\n",
    "        else:\n",
    "            df = pd.read_table(os.path.join(filepath, 'T.txt'), header=None);\n",
    "            T_img = df.as_matrix()\n",
    "            T_img = np.reshape(T_img, [72, 59, 3], order=\"F\").astype(np.uint8)\n",
    "\n",
    "        return A_img, C_img, G_img, T_img\n",
    "\n",
    "\n",
    "    def get_nt_height(pwm, height, norm):\n",
    "        \"\"\"get the heights of each nucleotide\"\"\"\n",
    "\n",
    "        def entropy(p):\n",
    "            \"\"\"calculate entropy of each nucleotide\"\"\"\n",
    "            s = 0\n",
    "            for i in range(4):\n",
    "                if p[i] > 0:\n",
    "                    s -= p[i]*np.log2(p[i])\n",
    "            return s\n",
    "\n",
    "        num_nt, num_seq = pwm.shape\n",
    "        heights = np.zeros((num_nt,num_seq));\n",
    "        for i in range(num_seq):\n",
    "            if norm == 1:\n",
    "                total_height = height\n",
    "            else:\n",
    "                total_height = (np.log2(4) - entropy(pwm[:, i]))*height;\n",
    "            heights[:,i] = np.floor(pwm[:,i]*total_height);\n",
    "        return heights.astype(int)\n",
    "\n",
    "    \n",
    "    # get the alphabet images of each nucleotide\n",
    "    A_img, C_img, G_img, T_img = load_alphabet(filepath='.', rna=1)\n",
    "    \n",
    "    \n",
    "    # get the heights of each nucleotide\n",
    "    heights = get_nt_height(pwm, height, norm)\n",
    "    \n",
    "    # resize nucleotide images for each base of sequence and stack\n",
    "    num_nt, num_seq = pwm.shape\n",
    "    width = np.ceil(nt_width*num_seq).astype(int)\n",
    "    \n",
    "    total_height = np.sum(heights,axis=0)\n",
    "    max_height = np.max(total_height)\n",
    "    logo = np.ones((height*2, width, 3)).astype(int)*255;\n",
    "    for i in range(num_seq):\n",
    "        remaining_height = total_height[i];\n",
    "        offset = max_height-remaining_height\n",
    "        nt_height = np.sort(heights[:,i]);\n",
    "        index = np.argsort(heights[:,i])\n",
    "\n",
    "        for j in range(num_nt):\n",
    "            if nt_height[j] > 0:\n",
    "                # resized dimensions of image\n",
    "                resize = (nt_height[j], nt_width)\n",
    "                if index[j] == 0:\n",
    "                    nt_img = imresize(A_img, resize)\n",
    "                elif index[j] == 1:\n",
    "                    nt_img = imresize(C_img, resize)\n",
    "                elif index[j] == 2:\n",
    "                    nt_img = imresize(G_img, resize)\n",
    "                elif index[j] == 3:\n",
    "                    nt_img = imresize(T_img, resize)\n",
    "\n",
    "                # determine location of image\n",
    "                height_range = range(remaining_height-nt_height[j], remaining_height)\n",
    "                width_range = range(i*nt_width, i*nt_width+nt_width)\n",
    "\n",
    "                # 'annoying' way to broadcast resized nucleotide image\n",
    "                if height_range:\n",
    "                    for k in range(3):\n",
    "                        for m in range(len(width_range)):\n",
    "                            logo[height_range+offset, width_range[m],k] = nt_img[:,m,k];\n",
    "\n",
    "                remaining_height -= nt_height[j]\n",
    "\n",
    "    return logo.astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X2, labels = reconstruct_layer2(network, train)\n",
    "map_index = range(10000)\n",
    "X = train[0][map_index]\n",
    "X = np.expand_dims(X,0)\n",
    "y = np.argmax(train[1][map_index])\n",
    "\n",
    "def get_class_pwm(X, class_index, norm=0):\n",
    "    class_pwm = 0\n",
    "    for i in class_index:\n",
    "        x = np.squeeze(X[i])\n",
    "        if norm == 1:\n",
    "            for i in range(5):\n",
    "                x[:,:10]=0\n",
    "                x[:,-10:]=0\n",
    "                MEAN = np.nanmean(x,axis=1)\n",
    "                x -= np.outer(MEAN, np.ones(x.shape[1]))\n",
    "            sumX = np.sum(x,axis=0)\n",
    "            x /= np.outer(np.ones(4),sumX)\n",
    "        class_pwm += x\n",
    "    class_pwm /= len(class_index)\n",
    "    return class_pwm\n",
    "\n",
    "model = []\n",
    "for class_plot in range(20):\n",
    "    y = np.argmax(train[1], axis=1)\n",
    "    y = y[map_index]\n",
    "    class_index = np.where(y == class_plot)[0]\n",
    "    model.append(get_class_pwm(np.squeeze(X), class_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save some example pictures so we can see what it's done \n",
    "pred_fn = theano.function([input_var], get_output(net['X'], deterministic=True))\n",
    "\n",
    "for index in range(270,290):\n",
    "    \n",
    "    prediction = pred_fn(np.expand_dims(X_valid[index],0).astype(np.float32))\n",
    "    prediction = prediction.reshape([4,200])\n",
    "    class_index = y_test[index]\n",
    "    \n",
    "\n",
    "    height=300\n",
    "    bp_width=30\n",
    "    num_seq = prediction.shape[1]\n",
    "    width = bp_width*num_seq\n",
    "    size = (25.,25.0)\n",
    "\n",
    "    logo = seq_logo(np.squeeze(test[0][index]), height, width, norm=0, rna=1, filepath='.')\n",
    "    fig = plt.figure(figsize=size);\n",
    "    plt.imshow(logo, interpolation='none');\n",
    "    plt.axis('off');\n",
    "\n",
    "    logo = seq_logo(valid[2][index], height, width, norm=0, rna=1, filepath='.')\n",
    "    fig = plt.figure(figsize=size);\n",
    "    plt.imshow(logo, interpolation='none');\n",
    "    plt.axis('off');\n",
    "    plt.title(str(class_index))\n",
    "\n",
    "    \n",
    "    pwm =np.squeeze(prediction[0])\n",
    "    norm = np.outer(np.ones(4), np.sum(pwm, axis=0))\n",
    "    pwm = pwm/norm\n",
    "    logo = seq_logo(pwm, height, width, norm=0, rna=1, filepath='.')\n",
    "    fig = plt.figure(figsize=size);\n",
    "    plt.imshow(logo, interpolation='none');\n",
    "    plt.axis('off');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z = get_output(net['Z'], deterministic=True)\n",
    "test_fun = theano.function([input_var], z)\n",
    "\n",
    "batch_size = 100\n",
    "num_test_batches = X_test.shape[0] // batch_size\n",
    "test_batches = batch_generator(X_test, batch_size, shuffle=False)\n",
    "    \n",
    "prediction = []\n",
    "for index in range(num_test_batches):\n",
    "    prediction.append(test_fun(next(test_batches)))\n",
    "    \n",
    "prediction = np.array(prediction).reshape([-1,2])\n",
    "\n",
    "plt.figure(figsize=(8, 6)) \n",
    "plt.scatter(prediction[:,0], prediction[:,1], c=y_test, cmap=plt.cm.get_cmap(\"jet\", 10),  edgecolor='none')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z_var = T.vector()\n",
    "generated_x = get_output(net['X'], {net['encode_mu']:z_var}, \n",
    "            deterministic=True)\n",
    "gen_fn = theano.function([z_var], generated_x)\n",
    "\n",
    "num_grid = 20\n",
    "width = 28\n",
    "height = 28\n",
    "tile_img = np.zeros((width*num_grid,height*num_grid))\n",
    "MIN = np.min(prediction)\n",
    "MAX = np.max(prediction)\n",
    "pos = np.linspace(MIN,MAX,num_grid)\n",
    "for i in range(num_grid):\n",
    "    for j in range(num_grid):\n",
    "        z = np.asarray([pos[i], pos[j]], dtype=theano.config.floatX)\n",
    "        x_gen = gen_fn(z).reshape(-1, 1, width, height)\n",
    "        tile_img[i*height:(i+1)*height, j*width:(j+1)*width] = x_gen\n",
    "        \n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(tile_img, cmap='gray')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load mnist data\n",
    "fname = '/home/peter/Data/mnist/mnist.pkl.gz'\n",
    "f = gzip.open(fname, 'rb')\n",
    "train_set, valid_set, test_set = pickle.load(f)\n",
    "f.close()\n",
    "X_train, y_train = train_set\n",
    "X_valid, y_valid = valid_set\n",
    "X_test, y_test = test_set\n",
    "\n",
    "num_labels = 10\n",
    "num_train = len(y_train)\n",
    "num_valid = len(y_valid)\n",
    "num_test = len(y_test)\n",
    "X_train = X_train.reshape([-1,1,28,28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VariationalSampleLayer(layers.MergeLayer):\n",
    "    def __init__(self, incoming_mu, incoming_logsigma, **kwargs):\n",
    "        super(VariationalSampleLayer, self).__init__(incomings=[incoming_mu, incoming_logsigma], **kwargs)\n",
    "        self.srng = RandomStreams(seed=234)\n",
    "\n",
    "    def get_output_shape_for(self, input_shapes):\n",
    "        return input_shapes[0]\n",
    "\n",
    "    def get_output_for(self, inputs, deterministic=False, **kwargs):\n",
    "        mu, logsigma = inputs\n",
    "        shape=(self.input_shapes[0][0] or inputs[0].shape[0],\n",
    "                self.input_shapes[0][1] or inputs[0].shape[1])\n",
    "        if deterministic:\n",
    "            return mu\n",
    "        return mu + T.exp(logsigma) * self.srng.normal(shape, avg=0.0, std=1).astype(theano.config.floatX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_encode=2\n",
    "num_units=500\n",
    "x_dim = 28*28\n",
    "\n",
    "input_var = T.tensor4('inputs')\n",
    "net = {}\n",
    "net['input'] = layers.InputLayer(shape=(None,1,28,28), input_var=input_var)\n",
    "net['conv1'] = layers.Conv2DLayer(net['input'],  num_filters=25,\n",
    "                                  filter_size=(5,5),\n",
    "                                  W=init.GlorotUniform(),\n",
    "                                  b=None, \n",
    "                                  pad='same')\n",
    "\n",
    "net['conv1_bias'] = layers.BiasLayer(net['conv1'], b=init.Constant(0.05))\n",
    "net['conv1_norm'] = layers.BatchNormLayer(net['conv1_bias'])\n",
    "net['conv1_active'] = layers.NonlinearityLayer(net['conv1_norm'],nonlinearity=nonlinearities.rectify) \n",
    "net['conv1_pool'] = layers.MaxPool2DLayer(net['conv1_active'], pool_size=(2,2))\n",
    "\n",
    "net['conv2'] = layers.Conv2DLayer(net['conv1_pool'],  num_filters=50,\n",
    "                                  filter_size=(3,3),\n",
    "                                  W=init.GlorotUniform(),\n",
    "                                  b=None, \n",
    "                                  pad='same')\n",
    "net['conv2_bias'] = layers.BiasLayer(net['conv2'], b=init.Constant(0.05))\n",
    "net['conv2_norm'] = layers.BatchNormLayer(net['conv2_bias'])\n",
    "net['conv2_active'] = layers.NonlinearityLayer(net['conv2_norm'],nonlinearity=nonlinearities.rectify)                          \n",
    "net['conv2_pool'] = layers.MaxPool2DLayer(net['conv2_active'], pool_size=(2,2))\n",
    "                                            \n",
    "         \n",
    "net['dense1'] = layers.DenseLayer(net['conv2_pool'], num_units=100, W=init.GlorotUniform(), \n",
    "                                  b=init.Constant(.0), nonlinearity=nonlinearities.rectify)\n",
    "net['dense2'] = layers.DenseLayer(net['dense1'], num_units=100, W=init.GlorotUniform(), \n",
    "                                  b=init.Constant(.0), nonlinearity=nonlinearities.rectify)\n",
    "                                  \n",
    "net['encode_mu'] = layers.DenseLayer(net['dense2'], num_units=num_encode, W=init.GlorotUniform(), \n",
    "                                  b=init.Constant(.0), nonlinearity=nonlinearities.linear)\n",
    "net['encode_logsigma'] = layers.DenseLayer(net['dense2'], num_units=num_encode, W=init.GlorotUniform(), \n",
    "                                  b=init.Constant(.0), nonlinearity=nonlinearities.linear)\n",
    "net['Z'] = VariationalSampleLayer(net['encode_mu'], net['encode_logsigma'])\n",
    "                                  \n",
    "net['invline'] = layers.DenseLayer(net['Z'], num_units=net['dense2'].input_shape[1], W=init.GlorotUniform(), \n",
    "                                  b=init.Constant(.0), nonlinearity=nonlinearities.linear)\n",
    "net['invdense2'] = layers.DenseLayer(net['invline'], num_units=net['dense2'].input_shape[1], W=net['dense2'].W.dimshuffle([1,0]), \n",
    "                                  b=init.Constant(.0), nonlinearity=nonlinearities.rectify)\n",
    "num_units = np.prod(list(net['dense1'].input_shape)[1:])\n",
    "net['invdense1'] = layers.DenseLayer(net['invdense2'], num_units=num_units, W=net['dense1'].W.dimshuffle([1,0]), \n",
    "                                  b=init.Constant(.0), nonlinearity=nonlinearities.rectify)\n",
    "shape = list(get_output_shape(net['conv2_pool']))\n",
    "shape[0] = -1\n",
    "net['reshape'] = layers.ReshapeLayer(net['invdense1'], shape=tuple(shape))\n",
    "\n",
    "net['invpool2'] = layers.Upscale2DLayer(net['reshape'], (2,2))\n",
    "net['invconv2']  = Conv2DLayer(net['invpool2'], num_filters=net['conv2'].input_shape[1],\n",
    "                                          filter_size=net['conv2'].filter_size,\n",
    "                                          W=net['conv2'].W.dimshuffle([1,0,2,3]), #W2_inv, #\n",
    "                                          b=init.Constant(0.05), \n",
    "                                          pad='same',\n",
    "                                          nonlinearity=nonlinearities.rectify, flip_filters=True)\n",
    "\n",
    "\n",
    "net['invpool1'] = layers.Upscale2DLayer(net['invconv2'], (2,2))\n",
    "net['X']  = Conv2DLayer(net['invpool1'], num_filters=net['conv1'].input_shape[1],\n",
    "                                          filter_size=net['conv1'].filter_size,\n",
    "                                          W=net['conv1'].W.dimshuffle([1,0,2,3]), #W2_inv, #\n",
    "                                          b=init.Constant(0.05), \n",
    "                                          pad='same',\n",
    "                                          nonlinearity=nonlinearities.sigmoid, flip_filters=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_loss(net, target_var, deterministic):\n",
    "    \n",
    "    z_mu = get_output(net['encode_mu'], deterministic=deterministic)\n",
    "    z_logsigma = get_output(net['encode_logsigma'], deterministic=deterministic)\n",
    "    kl_divergence = 0.5*T.sum(1 + 2*z_logsigma - T.sqr(z_mu) - T.exp(2*z_logsigma), axis=1)\n",
    "    \n",
    "    x_mu = get_output(net['X'], deterministic=deterministic)\n",
    "    x_mu = T.clip(x_mu, 1e-7, 1-1e-7)\n",
    "    \n",
    "    num_data = target_var.shape[0]\n",
    "    target_var = target_var.reshape([num_data,-1])\n",
    "    x_mu = x_mu.reshape([num_data, -1])\n",
    "    log_likelihood = T.sum(target_var*T.log(1e-10+x_mu) + (1.0-target_var)*T.log(1e-10+1.0-x_mu), axis=1)\n",
    "    variational_lower_bound = -log_likelihood - kl_divergence\n",
    "    prediction = x_mu\n",
    "    return variational_lower_bound.mean(), prediction\n",
    "\n",
    "train_loss, train_prediction = build_loss(net, input_var, deterministic=False)\n",
    "\n",
    "# ADAM updates\n",
    "params = get_all_params(net['X'], trainable=True)\n",
    "update_op = updates.adam(train_loss, params, learning_rate=1e-3)\n",
    "train_fun = theano.function([input_var], train_loss, updates=update_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_generator(X, batch_size=128, shuffle=True):\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(X))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(X)-batch_size+1, batch_size):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx+batch_size]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx+batch_size)\n",
    "        yield X[excerpt].astype(np.float32)\n",
    "        \n",
    "num_epochs = 60\n",
    "batch_size = 100\n",
    "num_train_batches = X_train.shape[0] // batch_size\n",
    "    \n",
    "for epoch in range(num_epochs):\n",
    "    train_batches = batch_generator(X_train, batch_size, shuffle=True)\n",
    "\n",
    "    train_loss = 0\n",
    "    for index in range(num_train_batches):\n",
    "        loss = train_fun(next(train_batches))\n",
    "        train_loss += loss\n",
    "        \n",
    "    print(\"Epoch {} of {}\".format(epoch+1, num_epochs))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_loss/num_train_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save some example pictures so we can see what it's done \n",
    "example_batch_size = 100\n",
    "X_comp = X_test[:example_batch_size]\n",
    "X_comp = X_comp.reshape([-1,1,28,28])\n",
    "pred_fn = theano.function([input_var], get_output(net['X'], deterministic=True))\n",
    "X_pred = pred_fn(X_comp)\n",
    "\n",
    "width = 10\n",
    "height = 10\n",
    "\n",
    "plt.figure(figsize = (height,width))\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(100, 100, forward=True)\n",
    "gs = mpl.gridspec.GridSpec(height, width)\n",
    "gs.update(wspace=0.1, hspace=0.1, left=0.1, right=0.2, bottom=0.1, top=0.2) \n",
    "for i in range(width*height):\n",
    "    img = np.reshape(X_comp[i], [28,28])\n",
    "    plt.subplot(gs[i])\n",
    "    plt.imshow(np.squeeze(img), cmap='gray', interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize = (height,width))\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(100, 100, forward=True)\n",
    "gs = mpl.gridspec.GridSpec(height, width)\n",
    "gs.update(wspace=0.1, hspace=0.1, left=0.1, right=0.2, bottom=0.1, top=0.2) \n",
    "for i in range(width*height):\n",
    "    img = np.reshape(X_pred[i], [28,28])\n",
    "    plt.subplot(gs[i])\n",
    "    plt.imshow(np.squeeze(img), cmap='gray', interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z = get_output(net['Z'], deterministic=True)\n",
    "test_fun = theano.function([input_var], z)\n",
    "\n",
    "batch_size = 100\n",
    "num_test_batches = X_test.shape[0] // batch_size\n",
    "test_batches = batch_generator(X_test.reshape([-1,1,28,28]), batch_size, shuffle=False)\n",
    "    \n",
    "prediction = []\n",
    "for index in range(num_test_batches):\n",
    "    prediction.append(test_fun(next(test_batches)))\n",
    "    \n",
    "prediction = np.array(prediction).reshape([-1,2])\n",
    "\n",
    "plt.figure(figsize=(8, 6)) \n",
    "plt.scatter(prediction[:,0], prediction[:,1], c=y_test, cmap=plt.cm.get_cmap(\"jet\", 10),  edgecolor='none')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z_var = T.vector()\n",
    "generated_x = get_output(net['X'], {net['encode_mu']:z_var}, \n",
    "            deterministic=True)\n",
    "gen_fn = theano.function([z_var], generated_x)\n",
    "\n",
    "num_grid = 20\n",
    "width = 28\n",
    "height = 28\n",
    "tile_img = np.zeros((width*num_grid,height*num_grid))\n",
    "MIN = np.min(prediction)\n",
    "MAX = np.max(prediction)\n",
    "pos = np.linspace(MIN,MAX,num_grid)\n",
    "for i in range(num_grid):\n",
    "    for j in range(num_grid):\n",
    "        z = np.asarray([pos[i], pos[j]], dtype=theano.config.floatX)\n",
    "        x_gen = gen_fn(z).reshape(-1, 1, width, height)\n",
    "        tile_img[i*height:(i+1)*height, j*width:(j+1)*width] = x_gen\n",
    "        \n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(tile_img, cmap='gray')\n",
    "plt.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
