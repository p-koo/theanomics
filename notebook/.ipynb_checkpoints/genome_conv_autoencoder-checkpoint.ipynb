{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 980 (CNMeM is disabled, CuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "sys.path.append('..')\n",
    "from src import NeuralNet\n",
    "from src import train as fit\n",
    "from src import make_directory \n",
    "from models import load_model\n",
    "from data import load_data\n",
    "from six.moves import cPickle\n",
    "np.random.seed(247) # for reproducibility\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'figure.autolayout': True})\n",
    "%matplotlib inline\n",
    "from scipy.misc import imresize\n",
    "\n",
    "from lasagne import layers, nonlinearities, updates, objectives, init \n",
    "from lasagne.layers import get_output, get_output_shape, get_all_params\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "np.random.seed(247) # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from: /home/peter/Data/SequenceMotif/N=100000_S=200_M=30_G=20_data.pickle\n",
      "loading train data\n",
      "loading cross-validation data\n",
      "loading test data\n"
     ]
    }
   ],
   "source": [
    "name = 'MotifSimulation_categorical'\n",
    "datapath = '/home/peter/Data/SequenceMotif'\n",
    "filepath = os.path.join(datapath, 'N=100000_S=200_M=30_G=20_data.pickle')\n",
    "train, valid, test = load_data(name, filepath)\n",
    "shape = (None, train[0].shape[1], train[0].shape[2], train[0].shape[3])\n",
    "num_labels = np.max(train[1])+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_name = \"conv_autoencoder\"\n",
    "nnmodel = NeuralNet(model_name, shape, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making directory: /home/peter/Data/SequenceMotif/cae\n"
     ]
    }
   ],
   "source": [
    "outputname = 'simple'\n",
    "datapath = make_directory(datapath, 'cae')\n",
    "filepath = os.path.join(datapath, outputname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1 out of 500 \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "('Bad input argument to theano function with name \"../src/neuralnetwork.py:354\"  at index 1(0-based)', 'CudaNdarrayType(float32, 4D), with dtype float32, cannot store a value of dtype int32 without risking loss of precision.If you do not mind, please cast your data to float32.', array([18,  1, 12,  4, 15, 14, 17,  7,  7,  0,  7, 15,  8,  0,  7,  8, 12,\n       16, 17,  7,  4,  9, 13, 17,  8, 14, 19, 11, 13, 13, 12, 13,  4,  4,\n       18,  7, 13,  5, 18, 19,  2,  7,  5, 11,  9,  5, 17,  8,  1,  9,  1,\n       16, 17,  5, 15, 12,  4, 15, 11, 17, 17, 11, 11,  8, 13, 13, 12, 13,\n       16,  0,  4, 11,  7, 15, 17,  9, 13, 15, 16, 12, 11, 13, 12,  8, 16,\n       15,  8, 15,  9,  0,  1, 13, 17, 13, 13,  1, 11, 12, 11,  4], dtype=int32))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-c29d19fa3dc3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m nnmodel = fit.train_minibatch(nnmodel, train, valid, batch_size=100, num_epochs=500, \n\u001b[1;32m----> 2\u001b[1;33m                         patience=3, verbose=1, filepath=filepath)\n\u001b[0m",
      "\u001b[1;32m/home/peter/Code/Deepomics/src/train.pyc\u001b[0m in \u001b[0;36mtrain_minibatch\u001b[1;34m(nnmodel, train, valid, batch_size, num_epochs, patience, verbose, filepath)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                 \u001b[1;31m# training set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m                 \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnnmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m                 \u001b[0mnnmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_monitor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/peter/Code/Deepomics/src/neuralnetwork.py\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(self, train, batch_size, verbose)\u001b[0m\n\u001b[0;32m    140\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m                         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m                         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m                         \u001b[0mvalue\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                         \u001b[0mperformance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/peter/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    784\u001b[0m                         s.storage[0] = s.type.filter(\n\u001b[0;32m    785\u001b[0m                             \u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m                             allow_downcast=s.allow_downcast)\n\u001b[0m\u001b[0;32m    787\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/peter/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/type.pyc\u001b[0m in \u001b[0;36mfilter\u001b[1;34m(self, data, strict, allow_downcast)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_downcast\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         return self.filter_inplace(data, None, strict=strict,\n\u001b[1;32m--> 105\u001b[1;33m                                    allow_downcast=allow_downcast)\n\u001b[0m\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     def filter_inplace(self, data, old_data, strict=False,\n",
      "\u001b[1;32m/home/peter/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/type.pyc\u001b[0m in \u001b[0;36mfilter_inplace\u001b[1;34m(self, data, old_data, strict, allow_downcast)\u001b[0m\n\u001b[0;32m    123\u001b[0m                         \u001b[1;34m'If you do not mind, please cast your data to %s.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m                         \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m                         data)\n\u001b[0m\u001b[0;32m    126\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                 \u001b[0mconverted_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_asarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: ('Bad input argument to theano function with name \"../src/neuralnetwork.py:354\"  at index 1(0-based)', 'CudaNdarrayType(float32, 4D), with dtype float32, cannot store a value of dtype int32 without risking loss of precision.If you do not mind, please cast your data to float32.', array([18,  1, 12,  4, 15, 14, 17,  7,  7,  0,  7, 15,  8,  0,  7,  8, 12,\n       16, 17,  7,  4,  9, 13, 17,  8, 14, 19, 11, 13, 13, 12, 13,  4,  4,\n       18,  7, 13,  5, 18, 19,  2,  7,  5, 11,  9,  5, 17,  8,  1,  9,  1,\n       16, 17,  5, 15, 12,  4, 15, 11, 17, 17, 11, 11,  8, 13, 13, 12, 13,\n       16,  0,  4, 11,  7, 15, 17,  9, 13, 15, 16, 12, 11, 13, 12,  8, 16,\n       15,  8, 15,  9,  0,  1, 13, 17, 13, 13,  1, 11, 12, 11,  4], dtype=int32))"
     ]
    }
   ],
   "source": [
    "nnmodel = fit.train_minibatch_ae(nnmodel, train, batch_size=100, num_epochs=500, \n",
    "                        patience=3, verbose=1, filepath=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_var = T.tensor4('input')\n",
    "target_var = T.dmatrix('output')\n",
    "\n",
    "net = []\n",
    "net = layers.InputLayer(shape=shape, input_var=input_var, name='input')\n",
    "net = layers.Conv2DLayer(net, num_filters=200, filter_size=(8,1), W=init.GlorotUniform(), \n",
    "                             nonlinearity=None, b=None, pad='valid', name='conv1')\n",
    "net = layers.BatchNormLayer(net)\n",
    "net = layers.BiasLayer(net, b=init.Constant(0.05))\n",
    "net = layers.NonlinearityLayer(net, nonlinearity=nonlinearities.rectify)\n",
    "net = layers.MaxPool2DLayer(net, pool_size=(4,1))\n",
    "\n",
    "net = layers.Conv2DLayer(net, num_filters=200, filter_size=(8,1), W=init.GlorotUniform(), \n",
    "                             nonlinearity=None, b=None, pad='valid', name='conv1')\n",
    "net = layers.BatchNormLayer(net)\n",
    "net = layers.BiasLayer(net, b=init.Constant(0.05))\n",
    "net = layers.NonlinearityLayer(net, nonlinearity=nonlinearities.rectify)\n",
    "net = layers.MaxPool2DLayer(net, pool_size=(4,1))\n",
    "#net = layers.DropoutLayer(net, p=0.3)\n",
    "\n",
    "net = layers.DenseLayer(net, num_units=200, W=init.GlorotUniform(), name='inter')\n",
    "net = layers.BatchNormLayer(net)\n",
    "net = layers.BiasLayer(net, init.Constant(0.05))\n",
    "net = layers.NonlinearityLayer(net, nonlinearity=nonlinearities.sigmoid)\n",
    "#net = layers.DropoutLayer(net, p=0.5)\n",
    "\n",
    "net = layers.DenseLayer(net, num_units=20, W=init.GlorotUniform(), name='inter')\n",
    "net = layers.BiasLayer(net, init.Constant(0.05))\n",
    "net = layers.NonlinearityLayer(net, nonlinearity=nonlinearities.sigmoid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set_all_param_values(net, all_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = get_output(net)\n",
    "train_loss = objectives.binary_crossentropy(prediction, target_var)\n",
    "train_loss = train_loss.mean()\n",
    "\n",
    "valid_prediction = get_output(net, deterministic=True)\n",
    "valid_loss = objectives.binary_crossentropy(valid_prediction, target_var)\n",
    "valid_loss = valid_loss.mean()\n",
    "\n",
    "valid_acc = objectives.binary_accuracy(valid_prediction, target_var)\n",
    "valid_acc = valid_acc.mean()\n",
    "\n",
    "params = get_all_params(net, trainable=True)\n",
    "update_op = updates.adam(train_loss, params)\n",
    "\n",
    "train_fn = theano.function([input_var, target_var], train_loss, updates=update_op, allow_input_downcast=True)\n",
    "val_fn = theano.function([input_var, target_var], [valid_loss, valid_acc], allow_input_downcast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "num_train_batches = len(train[0]) // batch_size\n",
    "train_batches = batch_generator(train[0], train[1], batch_size)\n",
    "\n",
    "num_valid_batches = len(valid[0]) // batch_size\n",
    "valid_batches = batch_generator(valid[0], valid[1], batch_size)\n",
    "\n",
    "n_epochs = 10\n",
    "for e in range(n_epochs):\n",
    "    ave_loss = 0\n",
    "    for index in range(num_train_batches):\n",
    "        X_batch, y_batch = next(train_batches)\n",
    "        train_loss = train_fn(X_batch, y_batch)\n",
    "        ave_loss += train_loss\n",
    "    print(\"train: %f\" % float(ave_loss/num_train_batches))\n",
    "\n",
    "    ave_loss = 0\n",
    "    ave_acc = 0\n",
    "    for index in range(num_valid_batches):\n",
    "        X_batch, y_batch = next(valid_batches)\n",
    "        valid_loss, valid_acc = val_fn(X_batch, y_batch)\n",
    "        ave_loss += valid_loss\n",
    "        ave_acc += valid_acc\n",
    "    print(\"valid: %f\" % float(ave_loss/num_valid_batches))\n",
    "    print(\"accuracy: %f\" % float(ave_acc/num_valid_batches))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_var = T.tensor4('input')\n",
    "target_var = T.dmatrix('output')\n",
    "\n",
    "net = []\n",
    "net = layers.InputLayer(shape=shape, input_var=input_var, name='input')\n",
    "net = layers.Conv2DLayer(net, num_filters=200, filter_size=(8,1), W=init.GlorotUniform(), \n",
    "                             nonlinearity=None, b=None, pad='valid', name='conv1')\n",
    "net = layers.BatchNormLayer(net)\n",
    "net = layers.BiasLayer(net, b=init.Constant(0.05))\n",
    "net = layers.NonlinearityLayer(net, nonlinearity=nonlinearities.rectify)\n",
    "net = layers.MaxPool2DLayer(net, pool_size=(4,1))\n",
    "\n",
    "net = layers.Conv2DLayer(net, num_filters=200, filter_size=(8,1), W=init.GlorotUniform(), \n",
    "                             nonlinearity=None, b=None, pad='valid', name='conv1')\n",
    "net = layers.BatchNormLayer(net)\n",
    "net = layers.BiasLayer(net, b=init.Constant(0.05))\n",
    "net = layers.NonlinearityLayer(net, nonlinearity=nonlinearities.rectify)\n",
    "net = layers.MaxPool2DLayer(net, pool_size=(4,1))\n",
    "#net = layers.DropoutLayer(net, p=0.3)\n",
    "\n",
    "net = layers.DenseLayer(net, num_units=200, W=init.GlorotUniform(), name='inter')\n",
    "net = layers.BatchNormLayer(net)\n",
    "net = layers.BiasLayer(net, init.Constant(0.05))\n",
    "net = layers.NonlinearityLayer(net, nonlinearity=nonlinearities.sigmoid)\n",
    "#net = layers.DropoutLayer(net, p=0.5)\n",
    "\n",
    "net = layers.DenseLayer(net, num_units=20, W=init.GlorotUniform(), name='inter')\n",
    "net = layers.BiasLayer(net, init.Constant(0.05))\n",
    "net = layers.NonlinearityLayer(net, nonlinearity=nonlinearities.sigmoid)\n",
    "\n",
    "prediction = get_output(net)\n",
    "train_loss = objectives.binary_crossentropy(prediction, target_var)\n",
    "train_loss = train_loss.mean()\n",
    "\n",
    "valid_prediction = get_output(net, deterministic=True)\n",
    "valid_loss = objectives.binary_crossentropy(valid_prediction, target_var)\n",
    "valid_loss = valid_loss.mean()\n",
    "\n",
    "valid_acc = objectives.binary_accuracy(valid_prediction, target_var)\n",
    "valid_acc = valid_acc.mean()\n",
    "\n",
    "params = get_all_params(net, trainable=True)\n",
    "update_op = updates.adam(train_loss, params)\n",
    "\n",
    "train_fn = theano.function([input_var, target_var], train_loss, updates=update_op, allow_input_downcast=True)\n",
    "val_fn = theano.function([input_var, target_var], [valid_loss, valid_acc], allow_input_downcast=True)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "num_train_batches = len(train[0]) // batch_size\n",
    "train_batches = batch_generator(train[0], train[1], batch_size)\n",
    "\n",
    "num_valid_batches = len(valid[0]) // batch_size\n",
    "valid_batches = batch_generator(valid[0], valid[1], batch_size)\n",
    "\n",
    "n_epochs = 10\n",
    "for e in range(n_epochs):\n",
    "    ave_loss = 0\n",
    "    for index in range(num_train_batches):\n",
    "        X_batch, y_batch = next(train_batches)\n",
    "        train_loss = train_fn(X_batch, y_batch)\n",
    "        ave_loss += train_loss\n",
    "    print(\"train: %f\" % float(ave_loss/num_train_batches))\n",
    "\n",
    "    ave_loss = 0\n",
    "    ave_acc = 0\n",
    "    for index in range(num_valid_batches):\n",
    "        X_batch, y_batch = next(valid_batches)\n",
    "        valid_loss, valid_acc = val_fn(X_batch, y_batch)\n",
    "        ave_loss += valid_loss\n",
    "        ave_acc += valid_acc\n",
    "    print(\"valid: %f\" % float(ave_loss/num_valid_batches))\n",
    "    print(\"accuracy: %f\" % float(ave_acc/num_valid_batches))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
