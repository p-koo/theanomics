{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys, gzip\n",
    "import cPickle as pickle\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1004)   # for reproducibility\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "from scipy.misc import imresize\n",
    "\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "from lasagne import layers, nonlinearities, updates, objectives, init, regularization\n",
    "from lasagne.layers import get_output, get_output_shape, get_all_params\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "#from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datapath='/home/peter/Data/CMAP/training.csv'\n",
    "savepath='/home/peter/Data/CMAP/dataset.hd5f'\n",
    "\n",
    "f = h5py.File(savepath, \"w\")\n",
    "batch_size = 20000\n",
    "shuffle_index = np.random.permutation(100000)\n",
    "for i in range(1,5):\n",
    "    batch_index =range(i*batch_size, (i+1)*batch_size)\n",
    "    data = pd.read_csv(datapath, usecols=shuffle_index[batch_index], header=None, dtype=np.float32)\n",
    "    genes = data.as_matrix()\n",
    "    num_landmark = 970\n",
    "    num_nonlandmark = 11350\n",
    "    num_samples = genes.shape[1]\n",
    "    landmark = genes[:970,:]\n",
    "    nonlandmark = genes[970:,:]\n",
    "    del genes\n",
    "\n",
    "    dset = f.create_dataset(\"landmark\"+str(i), data=landmark)\n",
    "    dset = f.create_dataset(\"nonlandmark\"+str(i), data=nonlandmark)\n",
    "    del landmark\n",
    "    del nonlandmark\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def normalize_data(landmark, nonlandmark, mean_landmark, std_landmark, mean_nonlandmark, std_nonlandmark, num_samples):\n",
    "    landmark = (landmark - np.outer(mean_landmark,np.ones(num_samples)))/np.outer(std_landmark,np.ones(num_samples))\n",
    "    nonlandmark = (nonlandmark - np.outer(mean_nonlandmark,np.ones(num_samples)))/np.outer(std_nonlandmark,np.ones(num_samples))\n",
    "    landmark = landmark.transpose([1,0])\n",
    "    nonlandmark = nonlandmark.transpose([1,0])\n",
    "\n",
    "    return landmark, nonlandmark\n",
    "\n",
    "\n",
    "filepath='/home/peter/Data/CMAP/dataset.hd5f'\n",
    "trainmat = h5py.File(filepath, 'r')\n",
    "landmark= np.array(trainmat['landmark1']).astype(np.float32)\n",
    "nonlandmark = np.array(trainmat['nonlandmark1']).astype(np.float32)\n",
    "\n",
    "num_files = 5\n",
    "mean_landmark = 0\n",
    "std_landmark = 0\n",
    "mean_nonlandmark = 0\n",
    "std_nonlandmark = 0\n",
    "for i in range(num_files):\n",
    "    landmark= np.array(trainmat['landmark'+str(i)]).astype(np.float32)\n",
    "    nonlandmark = np.array(trainmat['nonlandmark'+str(i)]).astype(np.float32)\n",
    "    mean_landmark += np.mean(landmark, axis=1)\n",
    "    std_landmark += np.std(landmark,axis=1)\n",
    "    mean_nonlandmark += np.mean(nonlandmark, axis=1)\n",
    "    std_nonlandmark += np.std(nonlandmark, axis=1)\n",
    "\n",
    "mean_landmark /= num_files\n",
    "std_landmark /= num_files\n",
    "mean_nonlandmark /= num_files\n",
    "std_nonlandmark /= num_files\n",
    "\n",
    "savepath='/home/peter/Data/CMAP/dataset_norm.hd5f'\n",
    "f = h5py.File(savepath, \"w\")\n",
    "batch_size = 20000\n",
    "for i in range(0,5):\n",
    "    landmark = np.array(trainmat['landmark'+str(i)]).astype(np.float32)\n",
    "    nonlandmark = np.array(trainmat['nonlandmark'+str(i)]).astype(np.float32)\n",
    "    landmark, nonlandmark = normalize_data(landmark, nonlandmark, mean_landmark, std_landmark, mean_nonlandmark, std_nonlandmark, num_samples)\n",
    "    dset = f.create_dataset(\"landmark\"+str(i), data=landmark)\n",
    "    dset = f.create_dataset(\"nonlandmark\"+str(i), data=nonlandmark)\n",
    "dset = f.create_dataset(\"mean_landmark\", data=mean_landmark)\n",
    "dset = f.create_dataset(\"std_landmark\", data=std_landmark)\n",
    "dset = f.create_dataset(\"mean_nonlandmark\", data=mean_nonlandmark)\n",
    "dset = f.create_dataset(\"std_nonlandmark\", data=std_nonlandmark)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filepath='/home/peter/Data/CMAP/dataset_norm.hd5f'\n",
    "trainmat = h5py.File(filepath, 'r')\n",
    "landmark= np.array(trainmat['landmark4']).astype(np.float32)\n",
    "nonlandmark = np.array(trainmat['nonlandmark4']).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nO+3AlsO4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f46c218d150>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 2\n",
    "plt.hist(landmark[:,index]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_encode=100\n",
    "\n",
    "input_var = T.dmatrix('inputs')\n",
    "shape = (None, landmark.shape[1])\n",
    "net = {}\n",
    "net['input'] = layers.InputLayer(shape=shape, input_var=input_var)\n",
    "net['dense1'] = layers.DenseLayer(net['input'], num_units=nonlandmark.shape[1], W=init.GlorotUniform(), \n",
    "                                  b=init.Constant(.0), nonlinearity=None)\n",
    "net['dense1_active'] = layers.NonlinearityLayer(net['dense1'], nonlinearity=nonlinearities.linear)\n",
    "net['output'] = net['dense1_active']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_var = T.dmatrix('inputs')\n",
    "prediction = get_output(net['output'], deterministic=False )\n",
    "loss = objectives.squared_error(target_var, prediction)\n",
    "loss = objectives.aggregate(loss, mode='mean')\n",
    "\n",
    "# ADAM updates\n",
    "params = get_all_params(net['output'], trainable=True)\n",
    "update_op = updates.adam(loss, params, learning_rate=1e-3)\n",
    "\n",
    "train_fun = theano.function([input_var, target_var], loss , updates=update_op, allow_input_downcast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size=128):\n",
    "    for start_idx in range(0, len(X)-batch_size+1, batch_size):\n",
    "        excerpt = slice(start_idx, start_idx+batch_size)\n",
    "        yield X[excerpt], y[excerpt]\n",
    "\n",
    "batch_size = 100        \n",
    "num_samples = 20000\n",
    "num_files = 5        \n",
    "num_epochs = 60    \n",
    "num_batches = num_samples // batch_size\n",
    "for epoch in range(num_epochs):\n",
    "    sys.stdout.write(\"\\rEpoch %d \\n\"%(epoch+1))\n",
    "\n",
    "    train_loss = 0\n",
    "    for i in range(num_files):\n",
    "        sys.stdout.write(\"\\r  File %d \\n\"%(i+1))\n",
    "        landmark= np.array(trainmat['landmark'+str(i)]).astype(np.float32)\n",
    "        nonlandmark = np.array(trainmat['nonlandmark'+str(i)]).astype(np.float32)\n",
    "        batches = batch_generator(landmark, nonlandmark, batch_size)\n",
    "        \n",
    "        loss = 0\n",
    "        for i in range(num_batches):\n",
    "            X, y = next(batches)\n",
    "            loss += train_fun(X, y)\n",
    "        print(\"    training loss:\\t\\t{:.6f}\".format(loss/num_batches))    \n",
    "        train_loss += loss/num_batches              \n",
    "    train_loss /= num_files\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 1 \n",
    "  File 1 \n",
    "    training loss:\t\t0.588965\n",
    "  File 2 \n",
    "    training loss:\t\t0.599399\n",
    "  File 3 \n",
    "    training loss:\t\t0.509065\n",
    "  File 4 \n",
    "    training loss:\t\t0.578905\n",
    "  File 5 \n",
    "    training loss:\t\t0.566761\n",
    "  training loss:\t\t0.568619\n",
    "Epoch 2 \n",
    "  File 1 \n",
    "    training loss:\t\t0.464421\n",
    "  File 2 \n",
    "    training loss:\t\t0.486734\n",
    "  File 3 \n",
    "    training loss:\t\t0.432946\n",
    "  File 4 \n",
    "    training loss:\t\t0.476401\n",
    "  File 5 \n",
    "    training loss:\t\t0.494727\n",
    "  training loss:\t\t0.471046\n",
    "Epoch 3 \n",
    "  File 1 \n",
    "    training loss:\t\t0.448270\n",
    "  File 2 \n",
    "    training loss:\t\t0.484055\n",
    "  File 3 \n",
    "    training loss:\t\t0.431769\n",
    "  File 4 \n",
    "    training loss:\t\t0.487850\n",
    "  File 5 \n",
    "    training loss:\t\t0.504944\n",
    "  training loss:\t\t0.471378"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed forward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_encode=100\n",
    "\n",
    "input_var = T.dmatrix('inputs')\n",
    "shape = (None, landmark.shape[1])\n",
    "net = {}\n",
    "net['input'] = layers.InputLayer(shape=shape, input_var=input_var)\n",
    "net['dense1'] = layers.DenseLayer(net['input'], num_units=5000, W=init.GlorotUniform(), \n",
    "                                  b=init.Constant(.0), nonlinearity=None)\n",
    "net['dense1_active'] = layers.NonlinearityLayer(net['dense1'], nonlinearity=nonlinearities.rectify)\n",
    "#net['dense1_active'] = layers.ParametricRectifierLayer(net['dense1_norm'])\n",
    "#net['dense1_droput'] = layers.DropoutLayer(net['dense1_active'], p=0.5)\n",
    "\n",
    "\n",
    "net['dense2'] = layers.DenseLayer(net['dense1_active'], num_units=nonlandmark.shape[1], W=init.GlorotUniform(), \n",
    "                                  b=init.Constant(.0), nonlinearity=None)\n",
    "net['dense2_active'] = layers.NonlinearityLayer(net['dense2'], nonlinearity=nonlinearities.linear)\n",
    "#net['dense2_active'] = layers.ParametricRectifierLayer(net['dense2_norm'])\n",
    "#net['dense2_droput'] = layers.DropoutLayer(net['dense2_active'], p=0.5)\n",
    "\n",
    "net['output'] = net['dense2_active']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_var = T.dmatrix('inputs')\n",
    "prediction = get_output(net['output'], deterministic=False )\n",
    "loss = objectives.squared_error(target_var, prediction)\n",
    "loss = objectives.aggregate(loss, mode='mean')\n",
    "\n",
    "#l1_penalty = regularization.regularize_network_params(net, regularization.l1) * 1e-5\n",
    "#loss += l1_penalty\n",
    "#l2_penalty = regularization.regularize_network_params(net, regularization.l2) * 1e-6    \n",
    "#loss += l2_penalty \n",
    "    \n",
    "# ADAM updates\n",
    "params = get_all_params(net['output'], trainable=True)\n",
    "update_op = updates.adam(loss, params, learning_rate=1e-3)\n",
    "\n",
    "train_fun = theano.function([input_var, target_var], loss , updates=update_op, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1 \n",
      "\r",
      "  File 1 \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input dimension mis-match. (input[0].shape[1] = 5000, input[1].shape[1] = 11350)\nApply node that caused the error: Elemwise{add,no_inplace}(dot.0, HostFromGpu.0)\nToposort index: 148\nInputs types: [TensorType(float64, matrix), TensorType(float32, row)]\nInputs shapes: [(100, 5000), (1, 11350)]\nInputs strides: [(40000, 8), (45400, 4)]\nInputs values: ['not shown', 'not shown']\nOutputs clients: [[Elemwise{Composite{(i0 * (Abs(i1) + i2 + i3))}}[(0, 2)](TensorConstant{(1, 1) of 0.5}, Elemwise{add,no_inplace}.0, dot.0, HostFromGpu.0), Elemwise{Composite{((i0 * i1) + (i0 * i1 * sgn(i2)))}}[(0, 1)](TensorConstant{(1, 1) of 0.5}, dot.0, Elemwise{add,no_inplace}.0)]]\n\nBacktrace when the node is created(use Theano flag traceback.limit=N to make it longer):\n  File \"/home/peter/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/peter/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/peter/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2723, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/peter/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2825, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/peter/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2885, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-dc577ed80e5c>\", line 1, in <module>\n    prediction = get_output(net['output'], deterministic=False )\n  File \"/home/peter/Lasagne/lasagne/layers/helper.py\", line 191, in get_output\n    all_outputs[layer] = layer.get_output_for(layer_inputs, **kwargs)\n  File \"/home/peter/Lasagne/lasagne/layers/dense.py\", line 89, in get_output_for\n    activation = activation + self.b.dimshuffle('x', 0)\n\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-5e129fcf7d22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrain_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"    training loss:\\t\\t{:.6f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/peter/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    869\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[0;32m    872\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m                 \u001b[1;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/peter/anaconda2/lib/python2.7/site-packages/theano/gof/link.pyc\u001b[0m in \u001b[0;36mraise_with_op\u001b[1;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;31m# extra long error message in that case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m     \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/peter/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input dimension mis-match. (input[0].shape[1] = 5000, input[1].shape[1] = 11350)\nApply node that caused the error: Elemwise{add,no_inplace}(dot.0, HostFromGpu.0)\nToposort index: 148\nInputs types: [TensorType(float64, matrix), TensorType(float32, row)]\nInputs shapes: [(100, 5000), (1, 11350)]\nInputs strides: [(40000, 8), (45400, 4)]\nInputs values: ['not shown', 'not shown']\nOutputs clients: [[Elemwise{Composite{(i0 * (Abs(i1) + i2 + i3))}}[(0, 2)](TensorConstant{(1, 1) of 0.5}, Elemwise{add,no_inplace}.0, dot.0, HostFromGpu.0), Elemwise{Composite{((i0 * i1) + (i0 * i1 * sgn(i2)))}}[(0, 1)](TensorConstant{(1, 1) of 0.5}, dot.0, Elemwise{add,no_inplace}.0)]]\n\nBacktrace when the node is created(use Theano flag traceback.limit=N to make it longer):\n  File \"/home/peter/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/peter/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/peter/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2723, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/peter/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2825, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/peter/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2885, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-dc577ed80e5c>\", line 1, in <module>\n    prediction = get_output(net['output'], deterministic=False )\n  File \"/home/peter/Lasagne/lasagne/layers/helper.py\", line 191, in get_output\n    all_outputs[layer] = layer.get_output_for(layer_inputs, **kwargs)\n  File \"/home/peter/Lasagne/lasagne/layers/dense.py\", line 89, in get_output_for\n    activation = activation + self.b.dimshuffle('x', 0)\n\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "def batch_generator(X, y, batch_size=128):\n",
    "    for start_idx in range(0, len(X)-batch_size+1, batch_size):\n",
    "        excerpt = slice(start_idx, start_idx+batch_size)\n",
    "        yield X[excerpt], y[excerpt]\n",
    "\n",
    "batch_size = 100        \n",
    "num_samples = 20000\n",
    "num_files = 5        \n",
    "num_epochs = 60    \n",
    "num_batches = num_samples // batch_size\n",
    "for epoch in range(num_epochs):\n",
    "    sys.stdout.write(\"\\rEpoch %d \\n\"%(epoch+1))\n",
    "\n",
    "    train_loss = 0\n",
    "    for i in range(num_files):\n",
    "        sys.stdout.write(\"\\r  File %d \\n\"%(i+1))\n",
    "        landmark= np.array(trainmat['landmark'+str(i)]).astype(np.float32)\n",
    "        nonlandmark = np.array(trainmat['nonlandmark'+str(i)]).astype(np.float32)\n",
    "        batches = batch_generator(landmark, nonlandmark, batch_size)\n",
    "        \n",
    "        loss = 0\n",
    "        for i in range(num_batches):\n",
    "            X, y = next(batches)\n",
    "            loss += train_fun(X, y)\n",
    "        print(\"    training loss:\\t\\t{:.6f}\".format(loss/num_batches))    \n",
    "        train_loss += loss/num_batches              \n",
    "    train_loss /= num_files\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# denoising autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_var = T.dmatrix('inputs')\n",
    "shape = (None, landmark.shape[1])\n",
    "net = {}\n",
    "net['input'] = layers.InputLayer(shape=shape, input_var=input_var)\n",
    "\n",
    "# encode layer 1\n",
    "net['corrupt1'] = layers.GaussianNoiseLayer(net['input'], sigma=0.1)\n",
    "net['encode1'] = layers.DenseLayer(net['corrupt1'], num_units=2000, W=init.GlorotUniform(), \n",
    "                                  b=init.Constant(.0), nonlinearity=None)\n",
    "net['encode1_active'] = layers.NonlinearityLayer(net['encode1'], nonlinearity=nonlinearities.rectify)\n",
    "\n",
    "# encode layer 2\n",
    "net['corrupt2'] = layers.GaussianNoiseLayer(net['encode1_active'], sigma=0.1)\n",
    "net['encode2'] = layers.DenseLayer(net['corrupt2'], num_units=4000, W=init.GlorotUniform(), \n",
    "                                  b=init.Constant(.0), nonlinearity=None)\n",
    "net['encode2_active'] = layers.NonlinearityLayer(net['encode2'], nonlinearity=nonlinearities.rectify)\n",
    "\n",
    "# encode layer\n",
    "net['encode'] = layers.DenseLayer(net['encode2_active'], num_units=nonlandmark.shape[1], W=init.GlorotUniform(), \n",
    "                                  b=init.Constant(.0), nonlinearity=None)\n",
    "net['encode_active'] = layers.NonlinearityLayer(net['encode'], nonlinearity=nonlinearities.linear)\n",
    "\n",
    "# decode layer\n",
    "net['decode'] = layers.DenseLayer(net['encode_active'], num_units=4000, W=net['encode'].W.T, \n",
    "                                  b=init.Constant(.0), nonlinearity=None)\n",
    "net['decode_active'] = layers.NonlinearityLayer(net['decode'], nonlinearity=nonlinearities.rectify)\n",
    "\n",
    "# decode layer 1\n",
    "net['decode1'] = layers.DenseLayer(net['decode_active'], num_units=2000, W=net['encode2'].W.T, \n",
    "                                  b=init.Constant(.0), nonlinearity=None)\n",
    "net['decode1_active'] = layers.NonlinearityLayer(net['decode1'], nonlinearity=nonlinearities.rectify)\n",
    "\n",
    "# decode layer 2\n",
    "net['decode2'] = layers.DenseLayer(net['decode1_active'], num_units=landmark.shape[1], W=net['encode1'].W.T, \n",
    "                                  b=init.Constant(.0), nonlinearity=None)\n",
    "net['decode2_active'] = layers.NonlinearityLayer(net['decode2'], nonlinearity=nonlinearities.linear)\n",
    "net['output'] = net['decode2_active']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = get_output(net['output'], deterministic=False )\n",
    "loss1 = objectives.squared_error(input_var, prediction)\n",
    "loss1 = objectives.aggregate(loss1, mode='mean')\n",
    "\n",
    "target_var = T.dmatrix('inputs')\n",
    "prediction = get_output(net['encode_active'], deterministic=False )\n",
    "loss2 = objectives.squared_error(target_var, prediction)\n",
    "loss2 = objectives.aggregate(loss2, mode='mean')\n",
    "\n",
    "loss = loss1 + loss2\n",
    "\n",
    "#l1_penalty = regularization.regularize_network_params(net, regularization.l1) * 1e-5\n",
    "#loss += l1_penalty\n",
    "#l2_penalty = regularization.regularize_network_params(net, regularization.l2) * 1e-6    \n",
    "#loss += l2_penalty \n",
    "    \n",
    "# ADAM updates\n",
    "params = get_all_params(net['output'], trainable=True)\n",
    "update_op = updates.adam(loss, params, learning_rate=1e-3)\n",
    "\n",
    "train_fun = theano.function([input_var, target_var], loss , updates=update_op, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1 \n",
      "\r",
      "  File 1 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-c04f3c04904e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrain_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"    training loss:\\t\\t{:.6f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/peter/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/peter/anaconda2/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    910\u001b[0m             \u001b[1;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 912\u001b[1;33m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    913\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m                     \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/peter/anaconda2/lib/python2.7/site-packages/theano/tensor/basic.pyc\u001b[0m in \u001b[0;36mperform\u001b[1;34m(self, node, inp, out)\u001b[0m\n\u001b[0;32m   5434\u001b[0m         \u001b[1;31m# gives a numpy float object but we need to return a 0d\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5435\u001b[0m         \u001b[1;31m# ndarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5436\u001b[1;33m         \u001b[0mz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5438\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def batch_generator(X, y, batch_size=128):\n",
    "    for start_idx in range(0, len(X)-batch_size+1, batch_size):\n",
    "        excerpt = slice(start_idx, start_idx+batch_size)\n",
    "        yield X[excerpt], y[excerpt]\n",
    "\n",
    "batch_size = 100        \n",
    "num_samples = 20000\n",
    "num_files = 5        \n",
    "num_epochs = 60    \n",
    "num_batches = num_samples // batch_size\n",
    "for epoch in range(num_epochs):\n",
    "    sys.stdout.write(\"\\rEpoch %d \\n\"%(epoch+1))\n",
    "\n",
    "    train_loss = 0\n",
    "    for i in range(num_files):\n",
    "        sys.stdout.write(\"\\r  File %d \\n\"%(i+1))\n",
    "        landmark= np.array(trainmat['landmark'+str(i)]).astype(np.float32)\n",
    "        nonlandmark = np.array(trainmat['nonlandmark'+str(i)]).astype(np.float32)\n",
    "        batches = batch_generator(landmark, nonlandmark, batch_size)\n",
    "        \n",
    "        loss = 0\n",
    "        for j in range(num_batches):\n",
    "            X, y = next(batches)\n",
    "            loss += train_fun(X, y)\n",
    "        print(\"    training loss:\\t\\t{:.6f}\".format(loss/num_batches))    \n",
    "        train_loss += loss/num_batches              \n",
    "    train_loss /= num_files\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
