{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 980 (CNMeM is disabled, cuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "sys.path.append('..')\n",
    "from src import NeuralNet\n",
    "from src import train as fit\n",
    "from src import make_directory \n",
    "from models import load_model\n",
    "from data import load_data\n",
    "from six.moves import cPickle\n",
    "from subprocess import call\n",
    "\n",
    "np.random.seed(247) # for reproducibility\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'figure.autolayout': True})\n",
    "%matplotlib inline\n",
    "from scipy.misc import imresize\n",
    "\n",
    "from lasagne import layers, nonlinearities, updates, objectives, init \n",
    "from lasagne.layers import Conv2DLayer, TransposedConv2DLayer, DenseLayer, InputLayer, ExpressionLayer, BiasLayer\n",
    "\n",
    "from lasagne.layers import get_output, get_output_shape, get_all_params\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "np.random.seed(247) # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=300_G=30_data.pickle\n",
      "loading train data\n",
      "loading cross-validation data\n",
      "loading test data\n"
     ]
    }
   ],
   "source": [
    "filename = 'Unlocalized_N=100000_S=200_M=300_G=30_data.pickle'\n",
    "datapath = '/home/peter/Data/SequenceMotif'\n",
    "filepath = os.path.join(datapath, filename)\n",
    "\n",
    "# load training set\n",
    "print \"loading data from: \" + filepath\n",
    "f = open(filepath, 'rb')\n",
    "print \"loading train data\"\n",
    "train = cPickle.load(f)\n",
    "print \"loading cross-validation data\"\n",
    "cross_validation = cPickle.load(f)\n",
    "print \"loading test data\"\n",
    "test = cPickle.load(f)\n",
    "f.close()\n",
    "\n",
    "X_train = train[0].transpose((0,1,2)).astype(np.float32)\n",
    "y_train = train[1].astype(np.int32)\n",
    "X_val = cross_validation[0].transpose((0,1,2)).astype(np.float32)\n",
    "y_val = cross_validation[1].astype(np.int32)\n",
    "X_test = test[0].transpose((0,1,2)).astype(np.float32)\n",
    "y_test = test[1].astype(np.int32)\n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=3)\n",
    "X_val = np.expand_dims(X_val, axis=3)\n",
    "X_test = np.expand_dims(X_test, axis=3)\n",
    "\n",
    "train = (X_train, y_train, train[2])\n",
    "valid = (X_val, y_val, cross_validation[2])\n",
    "test = (X_test, y_test, test[2])\n",
    "\n",
    "shape = (None, train[0].shape[1], train[0].shape[2], train[0].shape[3])\n",
    "num_labels = train[1].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class DenoiseLayer(layers.MergeLayer):\n",
    "    \"\"\"\n",
    "        Special purpose layer used to construct the ladder network\n",
    "        See the ladder_network example.\n",
    "    \"\"\"\n",
    "    def __init__(self, u_net, z_net,\n",
    "                 nonlinearity=nonlinearities.sigmoid, **kwargs):\n",
    "        super(DenoiseLayer, self).__init__([u_net, z_net], **kwargs)\n",
    "\n",
    "        u_shp, z_shp = self.input_shapes\n",
    "\n",
    "\n",
    "        if not u_shp[-1] == z_shp[-1]:\n",
    "            raise ValueError(\"last dimension of u and z  must be equal\"\n",
    "                             \" u was %s, z was %s\" % (str(u_shp), str(z_shp)))\n",
    "        self.num_inputs = z_shp[-1]\n",
    "        self.nonlinearity = nonlinearity\n",
    "        constant = init.Constant\n",
    "        self.a1 = self.add_param(constant(0.), (self.num_inputs,), name=\"a1\")\n",
    "        self.a2 = self.add_param(constant(1.), (self.num_inputs,), name=\"a2\")\n",
    "        self.a3 = self.add_param(constant(0.), (self.num_inputs,), name=\"a3\")\n",
    "        self.a4 = self.add_param(constant(0.), (self.num_inputs,), name=\"a4\")\n",
    "\n",
    "        self.c1 = self.add_param(constant(0.), (self.num_inputs,), name=\"c1\")\n",
    "        self.c2 = self.add_param(constant(1.), (self.num_inputs,), name=\"c2\")\n",
    "        self.c3 = self.add_param(constant(0.), (self.num_inputs,), name=\"c3\")\n",
    "\n",
    "        self.c4 = self.add_param(constant(0.), (self.num_inputs,), name=\"c4\")\n",
    "\n",
    "        self.b1 = self.add_param(constant(0.), (self.num_inputs,),\n",
    "                                 name=\"b1\", regularizable=False)\n",
    "\n",
    "    def get_output_shape_for(self, input_shapes):\n",
    "        output_shape = list(input_shapes[0])  # make a mutable copy\n",
    "        return tuple(output_shape)\n",
    "\n",
    "    def get_output_for(self, inputs, **kwargs):\n",
    "        u, z_lat = inputs\n",
    "        sigval = self.c1 + self.c2*z_lat\n",
    "        sigval += self.c3*u + self.c4*z_lat*u\n",
    "        sigval = self.nonlinearity(sigval)\n",
    "        z_est = self.a1 + self.a2 * z_lat + self.b1*sigval\n",
    "        z_est += self.a3*u + self.a4*z_lat*u\n",
    "        return z_est\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_var = T.tensor4('inputs')\n",
    "\n",
    "net = {}\n",
    "net['input'] = layers.InputLayer(input_var=input_var, shape=shape)\n",
    "net['conv1_prenoise'] = layers.GaussianNoiseLayer(net['input'], sigma=0.2)\n",
    "net['conv1'] = layers.Conv2DLayer(net['conv1_prenoise'], num_filters=64, filter_size=(9, 1), stride=(1, 1),    # 196\n",
    "                 W=init.GlorotUniform(), b=None, nonlinearity=None, pad='valid')\n",
    "net['conv1_norm'] = layers.BatchNormLayer(net['conv1'])\n",
    "net['conv1_noise'] = layers.GaussianNoiseLayer(net['conv1_norm'], sigma=0.2)\n",
    "net['conv1_active'] = layers.NonlinearityLayer(net['conv1_noise'], nonlinearity=nonlinearities.rectify)\n",
    "net['conv1_pool'] = layers.MaxPool2DLayer(net['conv1_active'], pool_size=(12, 1), stride=(12, 1), ignore_border=False) # 16\n",
    "\n",
    "net['conv2_prenoise'] = layers.GaussianNoiseLayer(net['conv1_pool'], sigma=0.3)\n",
    "net['conv2'] = layers.Conv2DLayer(net['conv2_prenoise'], num_filters=512, filter_size=(5, 1), stride=(1, 1), \n",
    "                       W=init.GlorotUniform(), b=None, nonlinearity=None, pad='valid')\n",
    "net['conv2_norm'] = layers.BatchNormLayer(net['conv2'])\n",
    "net['conv2_noise'] = layers.GaussianNoiseLayer(net['conv2_norm'], sigma=0.3)\n",
    "net['conv2_active'] = layers.NonlinearityLayer(net['conv2_noise'], nonlinearity=nonlinearities.rectify)\n",
    "net['conv2_pool'] = layers.MaxPool2DLayer(net['conv2_active'], pool_size=(3, 1), stride=(3, 1), ignore_border=False) # 4\n",
    "\n",
    "net['conv3_prenoise'] = layers.GaussianNoiseLayer(net['conv2_pool'], sigma=0.3)\n",
    "net['conv3'] = layers.Conv2DLayer(net['conv3_prenoise'], num_filters=2056, filter_size=(4, 1), stride=(1, 1),  #1\n",
    "                       W=init.GlorotUniform(), b=None, nonlinearity=None, pad='valid')\n",
    "net['conv3_norm'] = layers.BatchNormLayer(net['conv3'])\n",
    "net['conv3_noise'] = layers.GaussianNoiseLayer(net['conv3_norm'], sigma=0.3)\n",
    "net['conv3_active'] = layers.NonlinearityLayer(net['conv3_noise'], nonlinearity=nonlinearities.rectify)\n",
    "\n",
    "net['conv4_prenoise'] = layers.GaussianNoiseLayer(net['conv3_active'], sigma=0.2)\n",
    "net['conv4'] = layers.Conv2DLayer(net['conv3_active'], num_filters=num_labels, filter_size=(1, 1), stride=(1, 1),\n",
    "                       W=init.GlorotUniform(), b=None, nonlinearity=None, pad='valid')\n",
    "net['conv4_noise'] = layers.GaussianNoiseLayer(net['conv4'], sigma=0.2)\n",
    "net['conv4_active'] = layers.NonlinearityLayer(net['conv4_noise'], nonlinearity=nonlinearities.sigmoid)\n",
    "\n",
    "net['output'] = layers.ReshapeLayer(net['conv4_active'], [-1, num_labels])\n",
    "\n",
    "net['encode'] = layers.NonlinearityLayer(net['conv4_active'], nonlinearity=None)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "net['deconv4'] = layers.TransposedConv2DLayer(net['encode'], num_filters=2056, filter_size=(1, 1), stride=(1, 1),\n",
    "                       W=net['conv4'].W, b=None, nonlinearity=None, crop='valid')\n",
    "net['deconv4_active'] = DenoiseLayer(u_net=net['deconv4'],z_net=net['conv4_prenoise'])\n",
    "\n",
    "net['deconv3'] = layers.TransposedConv2DLayer(net['deconv4_active'], num_filters=512, filter_size=(4, 1), stride=(1, 1),  #1\n",
    "                       W=net['conv3'].W, b=None, nonlinearity=None, crop='valid')\n",
    "net['deconv3_norm'] = layers.BatchNormLayer(net['deconv3'])\n",
    "net['deconv3_active'] = DenoiseLayer(u_net=net['deconv3_norm'],z_net=net['conv3_prenoise'])\n",
    "\n",
    "\n",
    "net['deconv2_pool'] = layers.InverseLayer(net['deconv3_active'], net['conv2_pool'])\n",
    "net['deconv2'] = layers.TransposedConv2DLayer(net['deconv2_pool'], num_filters=64, filter_size=(5, 1), stride=(1, 1), \n",
    "                       W=net['conv2'].W, b=None, nonlinearity=None, crop='valid')\n",
    "net['deconv2_norm'] = layers.BatchNormLayer(net['deconv2'])\n",
    "net['deconv2_active'] = DenoiseLayer(u_net=net['deconv2_norm'],z_net=net['conv2_prenoise'])\n",
    "\n",
    "net['deconv1_pool'] = layers.InverseLayer(net['deconv2_active'], net['conv1_pool']) \n",
    "net['deconv1'] = layers.TransposedConv2DLayer(net['deconv1_pool'], num_filters=4, filter_size=(9, 1), stride=(1, 1),    # 196\n",
    "                 W=net['conv1'].W, b=None, nonlinearity=None, crop='valid')\n",
    "net['deconv1_norm'] = layers.BatchNormLayer(net['deconv1'])\n",
    "net['deconv1_active'] = DenoiseLayer(u_net=net['deconv1_norm'],z_net=net['conv1_prenoise'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Refreshing lock /home/peter/.theano/compiledir_Linux-4.2--generic-x86_64-with-debian-jessie-sid-x86_64-2.7.11-64/lock_dir/lock\n",
      "INFO (theano.gof.compilelock): Refreshing lock /home/peter/.theano/compiledir_Linux-4.2--generic-x86_64-with-debian-jessie-sid-x86_64-2.7.11-64/lock_dir/lock\n"
     ]
    }
   ],
   "source": [
    "target_var = T.dmatrix('targets')\n",
    "predictions = layers.get_output(net['output'], deterministic=True)\n",
    "#predictions = T.reshape(predictions,[-1, num_labels])\n",
    "\n",
    "accuracy = objectives.binary_accuracy(T.round(predictions), target_var).mean()\n",
    "loss = objectives.binary_crossentropy(predictions, target_var)\n",
    "loss = objectives.aggregate(loss, mode='mean')\n",
    "\n",
    "deconv4 = layers.get_output(net['deconv4_active'], deterministic=False)\n",
    "deconv3 = layers.get_output(net['deconv3_active'], deterministic=False)\n",
    "deconv2 = layers.get_output(net['deconv2_active'], deterministic=False)\n",
    "deconv1 = layers.get_output(net['deconv1_active'], deterministic=False)\n",
    "\n",
    "\n",
    "conv3 = layers.get_output(net['conv3_active'], deterministic=True)\n",
    "conv2 = layers.get_output(net['conv2_pool'], deterministic=True)\n",
    "conv1 = layers.get_output(net['conv1_pool'], deterministic=True)\n",
    "\n",
    "loss4 = objectives.squared_error(deconv4,conv3)\n",
    "loss4 = objectives.aggregate(loss4, mode='mean')\n",
    "loss3 = objectives.squared_error(deconv3,conv2)\n",
    "loss3 = objectives.aggregate(loss3, mode='mean')\n",
    "loss2 = objectives.squared_error(deconv2,conv1)\n",
    "loss2 = objectives.aggregate(loss2, mode='mean')\n",
    "loss1 = objectives.squared_error(deconv1,input_var)\n",
    "loss1 = objectives.aggregate(loss1, mode='mean')\n",
    "\n",
    "\n",
    "lambda1 = 10.\n",
    "lambda2 = 1\n",
    "lambda3 = .2\n",
    "lambda4 = .2\n",
    "loss = loss + lambda1*loss1 + lambda2*loss2 + lambda3*loss3 + lambda4*loss4\n",
    "\n",
    "\n",
    "params = layers.get_all_params(net['deconv1_active'], trainable=True)    \n",
    "grad = T.grad(loss, params)\n",
    "\n",
    "\n",
    "update_op = updates.adam(grad, params, learning_rate=0.001)\n",
    "train_fun = theano.function([input_var, target_var], [loss], updates=update_op, allow_input_downcast=True)\n",
    "test_fun = theano.function([input_var, target_var], [loss, accuracy], allow_input_downcast=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = train_fun(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.17188 \n",
      "  valid loss:\t\t0.16798\n",
      "  valid accuaracy:\t0.96667\n",
      "Epoch 2 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.16757 \n",
      "  valid loss:\t\t0.16701\n",
      "  valid accuaracy:\t0.96667\n",
      "Epoch 3 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.16687 \n",
      "  valid loss:\t\t0.16654\n",
      "  valid accuaracy:\t0.96667\n",
      "Epoch 4 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.16650 \n",
      "  valid loss:\t\t0.16627\n",
      "  valid accuaracy:\t0.96667\n",
      "Epoch 5 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.16630 \n",
      "  valid loss:\t\t0.16612\n",
      "  valid accuaracy:\t0.96667\n",
      "Epoch 6 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.16618 \n",
      "  valid loss:\t\t0.16599\n",
      "  valid accuaracy:\t0.96667\n",
      "Epoch 7 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.16608 \n",
      "  valid loss:\t\t0.16592\n",
      "  valid accuaracy:\t0.96667\n",
      "Epoch 8 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.16601 \n",
      "  valid loss:\t\t0.16586\n",
      "  valid accuaracy:\t0.96667\n",
      "Epoch 9 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.16596 \n",
      "  valid loss:\t\t0.16581\n",
      "  valid accuaracy:\t0.96667\n",
      "Epoch 10 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.16592 \n",
      "  valid loss:\t\t0.16580\n",
      "  valid accuaracy:\t0.96667\n",
      "Epoch 11 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.16590 \n",
      "  valid loss:\t\t0.16579\n",
      "  valid accuaracy:\t0.96667\n",
      "Epoch 12 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.16588 \n",
      "  valid loss:\t\t0.16574\n",
      "  valid accuaracy:\t0.96667\n",
      "Epoch 13 \n",
      "[==============================] 100.0% -- time=0s -- loss=0.16586 \n",
      "  valid loss:\t\t0.16570\n",
      "  valid accuaracy:\t0.96667\n",
      "Epoch 14 \n",
      "[==========================    ] 85.4% -- time=55s -- loss=0.16585 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-148-3b964e902f09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mtrain_performance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/peter/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    860\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/peter/anaconda2/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    905\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    906\u001b[0m             \u001b[1;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 907\u001b[1;33m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    908\u001b[0m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def batch_generator(X, y, batch_size=128, shuffle=True):\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(X))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(X)-batch_size+1, batch_size):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx+batch_size]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx+batch_size)\n",
    "        yield X[excerpt].astype(np.float32), y[excerpt].astype(np.float32)\n",
    "import time\n",
    "        \n",
    "# train model\n",
    "batch_size = 100     \n",
    "bar_length = 30     \n",
    "num_epochs = 100   \n",
    "verbose = 1\n",
    "train_performance = []\n",
    "valid_performance = []\n",
    "for epoch in range(num_epochs):\n",
    "    sys.stdout.write(\"\\rEpoch %d \\n\"%(epoch+1))\n",
    "    \n",
    "    train_loss = 0\n",
    "    start_time = time.time()\n",
    "    num_batches = train[0].shape[0] // batch_size\n",
    "    batches = batch_generator(train[0], train[1], batch_size)\n",
    "    for j in range(num_batches):\n",
    "        X, y = next(batches)\n",
    "        loss = train_fun(X,y)\n",
    "        train_loss += loss[0]\n",
    "        train_performance.append(loss)\n",
    "\n",
    "        percent = float(j+1)/num_batches\n",
    "        remaining_time = (time.time()-start_time)*(num_batches-j-1)/(j+1)\n",
    "        progress = '='*int(round(percent*bar_length))\n",
    "        spaces = ' '*(bar_length-int(round(percent*bar_length)))\n",
    "        sys.stdout.write(\"\\r[%s] %.1f%% -- time=%ds -- loss=%.5f \" \\\n",
    "            %(progress+spaces, percent*100, remaining_time, train_loss/(j+1)))\n",
    "        sys.stdout.flush()\n",
    "    print \"\" \n",
    "    \n",
    "    # test current model with cross-validation data and store results\n",
    "    num_batches = test[0].shape[0] // batch_size\n",
    "    batches = batch_generator(test[0], test[1], batch_size, shuffle=False)\n",
    "    value = 0\n",
    "    accuracy = 0\n",
    "    for j in range(num_batches):\n",
    "        X, y = next(batches)\n",
    "        loss = test_fun(X, y)\n",
    "        value += loss[0]\n",
    "        accuracy += loss[1]\n",
    "        valid_performance.append(loss[0])\n",
    "\n",
    "    print(\"  valid loss:\\t\\t{:.5f}\".format(value/num_batches))\n",
    "    print(\"  valid accuaracy:\\t{:.5f}\".format(accuracy/num_batches))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
