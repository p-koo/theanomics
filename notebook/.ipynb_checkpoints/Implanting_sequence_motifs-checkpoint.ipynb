{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating synthetic sequences with embedded regulatory grammars for RNA-binding proteins\n",
    "\n",
    "### Motivation\n",
    "\n",
    "RNA-binding proteins physically interact with RNAs primarily through direct nucleotide interactions via sequence binding motifs or binding to secondary structure motifs.  Here, I will focus on the former using a deep convolutional neural network. However, assessing the performance on actual data is difficult as the \"ground truth\" is usually not known. \n",
    "\n",
    "To test the performance of CNNs, I am going to simulate idealistic data where the sequences contain combinations of \"known\" binding motifs implanted in specific \"known\" locations while the rest of the sequence is generated randomly. This idealistic dataset can therefore gauge whether or not CNNs can indeed recover the motifs and their regulatory grammars in the  simplest scenario. It will also provide a baseline understanding the limitations of this approach, such as how many sequences are needed to recover different levels of regulatory grammar complexities.   \n",
    "\n",
    "Note that the regulatory grammars generated here are unrealistic -- there is no such databases with accurate annotations as far as I am aware of.  Here, only the binding motifs are derived from actual experimental data; while the regulatory grammars are arbitrary, generated from a probabilisitic framework with a user-defined level of complexity, i.e. how many motifs interact with each other.\n",
    "\n",
    "### Simulation model\n",
    "\n",
    "To generate regulatory grammars, we need to create a framework for the interactions of specific motifs across distinct spatial distances. \n",
    "\n",
    "First, we will assume there are only P proteins. Of these P proteins, we can generate G regulatory grammars, which sample combinations of the M motifs.  We can then generate arbitrary distances, sampled from an exponential distribution, between each motif. Once the motif distances and combinations have been set, these constitute the set of regulatory grammars.  \n",
    "\n",
    "We can also simulate negative results by simulating different motifs with the same distances or the same motifs with different distance or incomplete grammars.  First, we'll just assume a perfect dataset and see how it performs.  Then, we will systematically increase the complexity to see when exactly this model fails.  \n",
    "\n",
    "\n",
    "#### Create a motif database for drosophila melanogaster\n",
    "\n",
    "The data comes from Ray et al. \"A compendium of RNA-binding motifs for decoding gene regulation\" (http://www.nature.com/nature/journal/v499/n7457/abs/nature12311.html). The link to the motifs I downloaded is here: \n",
    "\n",
    "\\$ wget http://hugheslab.ccbr.utoronto.ca/supplementary-data/RNAcompete_eukarya/top10align_motifs.tar.gz\n",
    "\n",
    "\\$ tar xzvf top10align_motifs.tar.gz\n",
    "\n",
    "Here, each file is a different RBP motif as a position frequency matrix.  So, the first step is to compile all of these files into a suitable database.  In particular, we can parse each motifs (position frequency matrix) from each file in motifpath (downloaded top10align_motifs folder), create a database (list of arrays), and save as binary format (motif.list):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-05-03 11:22:48--  http://hugheslab.ccbr.utoronto.ca/supplementary-data/RNAcompete_eukarya/top10align_motifs.tar.gz\n",
      "Resolving hugheslab.ccbr.utoronto.ca (hugheslab.ccbr.utoronto.ca)... 142.150.84.64\n",
      "Connecting to hugheslab.ccbr.utoronto.ca (hugheslab.ccbr.utoronto.ca)|142.150.84.64|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 44389 (43K) [application/x-gzip]\n",
      "Saving to: ‘top10align_motifs.tar.gz.1’\n",
      "\n",
      "100%[======================================>] 44,389      --.-K/s   in 0.02s   \n",
      "\n",
      "2016-05-03 11:22:49 (1.76 MB/s) - ‘top10align_motifs.tar.gz.1’ saved [44389/44389]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://hugheslab.ccbr.utoronto.ca/supplementary-data/RNAcompete_eukarya/top10align_motifs.tar.gz\n",
    "!tar xzf top10align_motifs.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from six.moves import cPickle\n",
    "\n",
    "motifpath = 'top10align_motifs/'   # directory where motif files are located\n",
    "motiflist = 'motif.pickle'         # output filename\n",
    "\n",
    "# get all motif files in motifpath directory\n",
    "listdir = os.listdir(motifpath)\n",
    "\n",
    "# parse motifs\n",
    "motif_set = []\n",
    "for files in listdir:\n",
    "    df = pd.read_table(os.path.join(motifpath,files))\n",
    "    motif_set.append(df.iloc[0::,1::].transpose())\n",
    "\n",
    "# save motifs    \n",
    "f = open(motiflist, 'wb')\n",
    "cPickle.dump(motif_set, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simulate some sequences with some specs... Note, there are 244 motifs here. So, for simplicity, we will downsample this to create different levels of dataset complexity.  \n",
    "\n",
    "$N$: total number of sequences \n",
    "\n",
    "$M$: total number of motifs ($M \\leq 244$) \n",
    "\n",
    "$S$: size of each sequence \n",
    "\n",
    "$w$: population fraction of each regulatory grammar \n",
    "\n",
    "To make the sequence super clean, we will use a uniform distribution for the PWFs of the 'non-motif sequences'. We will first set up a hierarchical interaction of the motifs, a so-called \"regulatory grammar\". \n",
    "\n",
    "So, first, we will sample a regulatory grammar.  This will give us which motifs are present and how far apart they are separated with respect to each other.  Then, we can create a postiion frequency matrix of this regulatory grammar and simulate a set number based on the population fraction $w$. \n",
    "\n",
    "Now that we generated some simulated data, we should shuffle the data, then split the data into training set, cross-validation set, and test set, while converting the sequence data into one-hot representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "datapath = '/home/peter/Data/Basset'\n",
    "trainmat = h5py.File(os.path.join(datapath,'er.h5'), 'r')\n",
    "y_train = np.array(trainmat['train_out'])\n",
    "y = y_train.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.729081"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.sum(y, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.698700000000001"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_seq = 100000       # number of sequences\n",
    "labels = y[0:num_seq,range(0,50)]\n",
    "C = np.cov(labels.T)\n",
    "Cinv = np.linalg.inv(C)\n",
    "L = np.linalg.cholesky(C)\n",
    "Linv = np.linalg.inv(L)\n",
    "\n",
    "\n",
    "def data_subset(y, class_range, negative=True):\n",
    "    \" gets a subset of data in the class_range\"\n",
    "    data_index = []\n",
    "    for i in class_range:\n",
    "        index = np.where(y[:, i] == 1)\n",
    "        data_index = np.concatenate((data_index, index[0]), axis=0)\n",
    "    unique_index = np.unique(data_index)\n",
    "    num_data = y.shape[0]\n",
    "    non_index = np.array(list(set(range(num_data)) - set(unique_index)))\n",
    "    if negative:\n",
    "        index = np.concatenate((unique_index, non_index), axis=0)\n",
    "    else:\n",
    "        index = unique_index\n",
    "    return index.astype(int)\n",
    "\n",
    "class_range = range(0,50)\n",
    "index = data_subset(y, class_range, negative=False)\n",
    "labels = y[index[0:num_seq],:]\n",
    "labels = labels[:,class_range]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_seq = 300000 \n",
    "labels = np.random.binomial(1, 13.729/50, (num_seq,50))\n",
    "np.sum(np.sum(labels, axis=1) == 0).astype(float)/len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 50)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'figure.autolayout': True})\n",
    "matplotlib.rcParams['figure.figsize'] = (100.0, 100.0)\n",
    "\n",
    "%matplotlib inline\n",
    "from scipy.misc import imresize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_motif = y.shape[1]        # number of motifs\n",
    "# load motif list from file\n",
    "motiflist = 'motif.pickle'\n",
    "f = open(motiflist, 'rb')\n",
    "motif_set = cPickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# select M random motifs from the complete list of motif_set\n",
    "motifIndex = np.random.permutation(len(motif_set))[0:num_motif]\n",
    "\n",
    "# build subset of motifs relevant for dataset\n",
    "motifs = []\n",
    "for index in motifIndex:\n",
    "    motifs.append(motif_set[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# dataset parameters\n",
    "seq_length = 600      # length of sequence\n",
    "    \n",
    "seq = []\n",
    "for label in labels:\n",
    "    index = np.where(label==1)[0]\n",
    "    num_index = len(index)\n",
    "    buffer_size = seq_length - num_index*8\n",
    "    ave_spacing = np.floor(buffer_size/(num_index+1))\n",
    "    \n",
    "    # generate sequence with motifs\n",
    "    sequence_pwm = np.ones((4,ave_spacing))/4\n",
    "    for i in index:\n",
    "        sequence_pwm = np.hstack((sequence_pwm, motifs[i]))\n",
    "        sequence_pwm = np.hstack((sequence_pwm, np.ones((4,ave_spacing))/4))\n",
    "    sequence_pwm = np.hstack((sequence_pwm, np.ones((4,seq_length - sequence_pwm.shape[1]))/4))\n",
    "    \n",
    "    \n",
    "    nucleotide = 'ACGU'\n",
    "    cum_prob = sequence_pwm.cumsum(axis=0)\n",
    "    Z = np.random.uniform(0,1,seq_length)\n",
    "    one_hot_seq = np.zeros((4,seq_length))\n",
    "    for i in range(seq_length):\n",
    "        index=[j for j in range(4) if Z[i] < cum_prob[j,i]][0]\n",
    "        one_hot_seq[index,i] = 1\n",
    "    seq.append(one_hot_seq)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq = np.array(seq)\n",
    "seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# percentage for each dataset\n",
    "train_size = 0.7\n",
    "cross_validation_size = 0.15\n",
    "test_size = 0.15\n",
    "\n",
    "split_size = [train_size, cross_validation_size, test_size]\n",
    "num_labels = len(np.unique(labels))\n",
    "cum_index = np.cumsum(np.multiply([0, split_size[0], split_size[1], split_size[2]],num_seq)).astype(int) \n",
    "shuffle = np.random.permutation(num_seq)\n",
    "train_index = shuffle[range(cum_index[0], cum_index[1])]\n",
    "cross_validation_index = shuffle[range(cum_index[1], cum_index[2])]\n",
    "test_index = shuffle[range(cum_index[2], cum_index[3])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# create subsets of data based on indices \n",
    "train = (seq[train_index], labels[train_index])\n",
    "cross_validation = (seq[cross_validation_index], labels[cross_validation_index])\n",
    "test = (seq[test_index], labels[test_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "f = h5py.File(\"synthetic_random_motifs_100000.hdf5\", \"w\")\n",
    "dset = f.create_dataset(\"trainx\", data=train[0])\n",
    "dset = f.create_dataset(\"trainy\", data=train[1])\n",
    "dset = f.create_dataset(\"validx\", data=cross_validation[0])\n",
    "dset = f.create_dataset(\"validy\", data=cross_validation[1])\n",
    "dset = f.create_dataset(\"testx\", data=test[0])\n",
    "dset = f.create_dataset(\"testy\", data=test[1])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
