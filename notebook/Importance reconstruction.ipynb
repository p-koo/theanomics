{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "sys.path.append('..')\n",
    "from src import NeuralNet\n",
    "from src import train as fit\n",
    "from src import make_directory \n",
    "from models import load_model\n",
    "from data import load_data\n",
    "from six.moves import cPickle\n",
    "\n",
    "from subprocess import call\n",
    "\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'figure.autolayout': True})\n",
    "%matplotlib inline\n",
    "from scipy.misc import imresize\n",
    "\n",
    "from lasagne import layers, nonlinearities, updates, objectives, init \n",
    "from lasagne.layers import Conv2DLayer, TransposedConv2DLayer, DenseLayer, InputLayer, ExpressionLayer, BiasLayer\n",
    "from lasagne import regularization\n",
    "\n",
    "from lasagne.layers import get_output, get_output_shape, get_all_params\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "np.random.seed(247) # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = 'Unlocalized_N=100000_S=200_M=50_G=20_data.pickle'\n",
    "datapath = '/home/peter/Data/SequenceMotif'\n",
    "filepath = os.path.join(datapath, filename)\n",
    "\n",
    "# load training set\n",
    "print \"loading data from: \" + filepath\n",
    "f = open(filepath, 'rb')\n",
    "print \"loading train data\"\n",
    "train = cPickle.load(f)\n",
    "print \"loading cross-validation data\"\n",
    "cross_validation = cPickle.load(f)\n",
    "print \"loading test data\"\n",
    "test = cPickle.load(f)\n",
    "f.close()\n",
    "\n",
    "X_train = train[0].transpose((0,1,2)).astype(np.float32)\n",
    "y_train = train[1].astype(np.int32)\n",
    "X_val = cross_validation[0].transpose((0,1,2)).astype(np.float32)\n",
    "y_val = cross_validation[1].astype(np.int32)\n",
    "X_test = test[0].transpose((0,1,2)).astype(np.float32)\n",
    "y_test = test[1].astype(np.int32)\n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=3)\n",
    "X_val = np.expand_dims(X_val, axis=3)\n",
    "X_test = np.expand_dims(X_test, axis=3)\n",
    "\n",
    "train = (X_train, y_train, train[2])\n",
    "valid = (X_val, y_val, cross_validation[2])\n",
    "test = (X_test, y_test, test[2])\n",
    "\n",
    "shape = (None, train[0].shape[1], train[0].shape[2], train[0].shape[3])\n",
    "num_labels = train[1].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "filename = 'shuffle_random_motifs_100000.hdf5'\n",
    "datapath = '/home/peter/Data/SequenceMotif'\n",
    "filepath = os.path.join(datapath, filename)\n",
    "\n",
    "trainmat = h5py.File(filepath, 'r')\n",
    "X_train = np.array(trainmat['trainx']).astype(np.float32)\n",
    "y_train = np.array(trainmat['trainy']).astype(np.float32)\n",
    "model_train = np.array(trainmat['trainmodel']).astype(np.float32)\n",
    "\n",
    "X_val = np.array(trainmat['validx']).astype(np.float32)\n",
    "y_val = np.array(trainmat['validy']).astype(np.int32)\n",
    "model_val = np.array(trainmat['validmodel']).astype(np.float32)\n",
    "\n",
    "X_test = np.array(trainmat['testx']).astype(np.float32)\n",
    "y_test = np.array(trainmat['testy']).astype(np.int32)\n",
    "model_test = np.array(trainmat['testmodel']).astype(np.float32)\n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=3)\n",
    "X_val = np.expand_dims(X_val, axis=3)\n",
    "X_test = np.expand_dims(X_test, axis=3)\n",
    "\n",
    "train = (X_train, y_train, model_train)\n",
    "valid = (X_val, y_val, model_val)\n",
    "test = (X_test, y_test, model_test)\n",
    "\n",
    "shape = (None, train[0].shape[1], train[0].shape[2], train[0].shape[3])\n",
    "num_labels = train[1].shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_name = \"autoencode_motif_model\"\n",
    "nnmodel = NeuralNet(model_name, shape, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nnmodel = fit.train_minibatch(nnmodel, train, valid, batch_size=128, num_epochs=500, \n",
    "                        patience=3, verbose=1, filepath=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nnmodel.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "network = nnmodel.network\n",
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def seq_logo(pwm, height=100, nt_width=20, norm=0, rna=1, filepath='.'):\n",
    "    \"\"\"generate a sequence logo from a pwm\"\"\"\n",
    "    \n",
    "    def load_alphabet(filepath, rna):\n",
    "        \"\"\"load images of nucleotide alphabet \"\"\"\n",
    "        df = pd.read_table(os.path.join(filepath, 'A.txt'), header=None);\n",
    "        A_img = df.as_matrix()\n",
    "        A_img = np.reshape(A_img, [72, 65, 3], order=\"F\").astype(np.uint8)\n",
    "\n",
    "        df = pd.read_table(os.path.join(filepath, 'C.txt'), header=None);\n",
    "        C_img = df.as_matrix()\n",
    "        C_img = np.reshape(C_img, [76, 64, 3], order=\"F\").astype(np.uint8)\n",
    "\n",
    "        df = pd.read_table(os.path.join(filepath, 'G.txt'), header=None);\n",
    "        G_img = df.as_matrix()\n",
    "        G_img = np.reshape(G_img, [76, 67, 3], order=\"F\").astype(np.uint8)\n",
    "\n",
    "        if rna == 1:\n",
    "            df = pd.read_table(os.path.join(filepath, 'U.txt'), header=None);\n",
    "            T_img = df.as_matrix()\n",
    "            T_img = np.reshape(T_img, [74, 57, 3], order=\"F\").astype(np.uint8)\n",
    "        else:\n",
    "            df = pd.read_table(os.path.join(filepath, 'T.txt'), header=None);\n",
    "            T_img = df.as_matrix()\n",
    "            T_img = np.reshape(T_img, [72, 59, 3], order=\"F\").astype(np.uint8)\n",
    "\n",
    "        return A_img, C_img, G_img, T_img\n",
    "\n",
    "\n",
    "    def get_nt_height(pwm, height, norm):\n",
    "        \"\"\"get the heights of each nucleotide\"\"\"\n",
    "\n",
    "        def entropy(p):\n",
    "            \"\"\"calculate entropy of each nucleotide\"\"\"\n",
    "            s = 0\n",
    "            for i in range(4):\n",
    "                if p[i] > 0:\n",
    "                    s -= p[i]*np.log2(p[i])\n",
    "            return s\n",
    "\n",
    "        num_nt, num_seq = pwm.shape\n",
    "        heights = np.zeros((num_nt,num_seq));\n",
    "        for i in range(num_seq):\n",
    "            if norm == 1:\n",
    "                total_height = height\n",
    "            else:\n",
    "                total_height = (np.log2(4) - entropy(pwm[:, i]))*height;\n",
    "            heights[:,i] = np.floor(pwm[:,i]*total_height);\n",
    "        return heights.astype(int)\n",
    "\n",
    "    \n",
    "    # get the alphabet images of each nucleotide\n",
    "    A_img, C_img, G_img, T_img = load_alphabet(filepath='.', rna=1)\n",
    "    \n",
    "    \n",
    "    # get the heights of each nucleotide\n",
    "    heights = get_nt_height(pwm, height, norm)\n",
    "    \n",
    "    # resize nucleotide images for each base of sequence and stack\n",
    "    num_nt, num_seq = pwm.shape\n",
    "    width = np.ceil(nt_width*num_seq).astype(int)\n",
    "    \n",
    "    total_height = np.sum(heights,axis=0)\n",
    "    max_height = np.max(total_height)\n",
    "    logo = np.ones((height*2, width, 3)).astype(int)*255;\n",
    "    for i in range(num_seq):\n",
    "        remaining_height = total_height[i];\n",
    "        offset = max_height-remaining_height\n",
    "        nt_height = np.sort(heights[:,i]);\n",
    "        index = np.argsort(heights[:,i])\n",
    "\n",
    "        for j in range(num_nt):\n",
    "            if nt_height[j] > 0:\n",
    "                # resized dimensions of image\n",
    "                resize = (nt_height[j], nt_width)\n",
    "                if index[j] == 0:\n",
    "                    nt_img = imresize(A_img, resize)\n",
    "                elif index[j] == 1:\n",
    "                    nt_img = imresize(C_img, resize)\n",
    "                elif index[j] == 2:\n",
    "                    nt_img = imresize(G_img, resize)\n",
    "                elif index[j] == 3:\n",
    "                    nt_img = imresize(T_img, resize)\n",
    "\n",
    "                # determine location of image\n",
    "                height_range = range(remaining_height-nt_height[j], remaining_height)\n",
    "                width_range = range(i*nt_width, i*nt_width+nt_width)\n",
    "\n",
    "                # 'annoying' way to broadcast resized nucleotide image\n",
    "                if height_range:\n",
    "                    for k in range(3):\n",
    "                        for m in range(len(width_range)):\n",
    "                            logo[height_range+offset, width_range[m],k] = nt_img[:,m,k];\n",
    "\n",
    "                remaining_height -= nt_height[j]\n",
    "\n",
    "    return logo.astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# top of the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def maxunpool(X, pool, active):\n",
    "    pool_size = active.shape[2]/pool.shape[2]\n",
    "    fmap1 = []\n",
    "    for k in range(active.shape[0]):\n",
    "        x = np.squeeze(active[k],axis=(2,))\n",
    "        mymap = np.squeeze(pool[k],axis=(2,))\n",
    "\n",
    "        max_index = []\n",
    "        for i in range(x.shape[1]/pool_size):\n",
    "            index = range(i*pool_size,(i+1)*pool_size)\n",
    "            max_index.append(np.argmax(x[:,index],axis=1))\n",
    "        max_index = np.array(max_index)\n",
    "        max_index\n",
    "\n",
    "        dim,seq_length = mymap.shape\n",
    "        fmap_unpool = np.zeros((dim,seq_length*pool_size))\n",
    "        for i in range(x.shape[1]/pool_size):\n",
    "            index = range(i*pool_size,(i+1)*pool_size)\n",
    "            for j in range(dim):\n",
    "                fmap_unpool[j,index[max_index[i][j]]] = mymap[j,i]\n",
    "        fmap1.append(fmap_unpool)\n",
    "\n",
    "    fmap1 = np.array(fmap1)\n",
    "    fmap1 = np.expand_dims(fmap1, 3)\n",
    "    return fmap1 \n",
    "\n",
    "def deconvolution(fmap, layer):\n",
    "    # psuedo-inverse filters\n",
    "    W4 = layer.W.get_value()\n",
    "\n",
    "    # deconvolution layer 2\n",
    "    input_var4 = T.tensor4('conv4')\n",
    "    shape4 = list(fmap.shape)\n",
    "    shape4[0] = None\n",
    "    input4 = InputLayer(shape=tuple(shape4), input_var=input_var4)\n",
    "    #unpool4 = ExpressionLayer(input4, lambda X: T.log(T.exp(X)-1 + 1e-7), output_shape='auto')\n",
    "    #unpool4 = BiasLayer(unpool4, b=-network['conv2_bias'].b)\n",
    "    if layer.pad == 'valid':\n",
    "        pad = 'full'\n",
    "    else:\n",
    "        pad = 'same'\n",
    "    deconv4 = Conv2DLayer(input4, num_filters=layer.input_shape[1],\n",
    "                                          filter_size=layer.filter_size,\n",
    "                                          W=layer.W.dimshuffle([1,0,2,3]), #W2_inv, #\n",
    "                                          b=None, \n",
    "                                          pad= 'full' if layer.pad==(0,0) else 'same',\n",
    "                                          nonlinearity=None, flip_filters=True)\n",
    "    predict = theano.function([input_var4], get_output(deconv4, deterministic=True), allow_input_downcast=True)\n",
    "\n",
    "    intermediate = predict(fmap.astype(np.float32))\n",
    "    return np.array(intermediate)\n",
    "\n",
    "def get_feature_map_all(layer, input_var, X):\n",
    "    get_map = theano.function([input_var], get_output(layer), allow_input_downcast=True)\n",
    "    return get_map(X)\n",
    "\n",
    " \n",
    "def activation_filter(layer, percentile, window=0, norm=0):\n",
    "    pool = np.squeeze(layer[0], axis=2)\n",
    "    if norm:\n",
    "        pool = np.abs(pool)\n",
    "        \n",
    "    pool_flat = pool.reshape([-1,])\n",
    "    num_data = len(pool_flat)\n",
    "    cutoff = np.round(num_data*percentile).astype(int)\n",
    "    threshold = np.sort(pool_flat)[-cutoff]\n",
    "\n",
    "    if norm:\n",
    "        sum_spikes = np.max(np.abs(pool),axis=0)\n",
    "    else:\n",
    "        sum_spikes = np.max(pool,axis=0)\n",
    "        \n",
    "    index = np.where(sum_spikes > threshold)[0]\n",
    "    fmap = np.zeros(pool.shape)\n",
    "    for i in index:\n",
    "        MIN = np.maximum(0, i-window)\n",
    "        MAX = np.minimum(pool.shape[1],i+window+1)\n",
    "        fmap[:,MIN:MAX] = pool[:,MIN:MAX]\n",
    "    fmap = np.expand_dims(fmap,0)\n",
    "    fmap = np.expand_dims(fmap,3)\n",
    "    return fmap, threshold\n",
    "\n",
    "def max_filter(layer, percentile, window=0, norm=0):\n",
    "    pool = np.squeeze(layer[0], axis=2)\n",
    "    if norm:\n",
    "        pool = np.abs(pool)\n",
    "    max_pool = np.max(pool, axis=0)\n",
    "    num_data = len(max_pool)\n",
    "    cutoff = np.round(num_data*percentile).astype(int)\n",
    "    threshold = np.sort(max_pool)[-cutoff]\n",
    "    \n",
    "    index = np.where(max_pool > threshold)[0]\n",
    "    fmap = np.zeros(pool.shape)\n",
    "    for i in index:\n",
    "        MIN = np.maximum(0, i-window)\n",
    "        MAX = np.minimum(pool.shape[1],i+window+1)\n",
    "        fmap[:,MIN:MAX] = pool[:,MIN:MAX]\n",
    "    fmap = np.expand_dims(fmap,0)\n",
    "    fmap = np.expand_dims(fmap,3)\n",
    "    return fmap, threshold\n",
    "\n",
    "def same_unpool_fmap(layer, seq_length):\n",
    "    activation = np.squeeze(layer[0], axis=2)\n",
    "    num_channels, num_map = activation.shape\n",
    "\n",
    "    num_unpool = np.floor(seq_length/num_map).astype(int)\n",
    "    amap = np.zeros((num_channels, seq_length))\n",
    "    for i in range(num_map):\n",
    "        amap[:,range(i*num_unpool,(i+1)*num_unpool)] = np.outer(activation[:,i], np.ones(num_unpool))\n",
    "    return amap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def deconvolve_activation(X, network, nnmodel):\n",
    "\n",
    "    get_conv4pool = theano.function([nnmodel.input_var], layers.get_output(network['conv4_active'], deterministic=True), allow_input_downcast=True)\n",
    "    conv4_pool = get_conv4pool(X)\n",
    "    conv4_pool, threshold = activation_filter(conv4_pool, percentile=0.15, window=0, norm=0)\n",
    "\n",
    "    conv3_pool = deconvolution(conv4_pool, network['conv4'])\n",
    "    conv3_pool, threshold = activation_filter(conv3_pool, percentile=0.1, window=0, norm=0)\n",
    "    conv3_pool[conv3_pool < threshold*1] = 0\n",
    "    conv3_active = get_feature_map_all(network['conv3_active'], nnmodel.input_var, X)\n",
    "    conv3 = maxunpool(X, conv3_pool, conv3_active)\n",
    "\n",
    "    conv2_pool = deconvolution(conv3, network['conv3'])\n",
    "    conv2_pool, threshold = activation_filter(conv2_pool, percentile=0.03, window=0, norm=0)\n",
    "    conv2_pool[conv2_pool < threshold*.8] = 0\n",
    "    conv2_active = get_feature_map_all(network['conv2_active'], nnmodel.input_var, X)\n",
    "    conv2 = maxunpool(X, conv2_pool, conv2_active)\n",
    "    \n",
    "    conv1_pool = deconvolution(conv2, network['conv2'])\n",
    "    conv1_pool, threshold = activation_filter(conv1_pool, percentile=0.005, window=0, norm=1)\n",
    "    conv1_pool[conv1_pool < threshold*.8] = 0\n",
    "    conv1_active = get_feature_map_all(network['conv1_active'], nnmodel.input_var, X)\n",
    "    conv1 = maxunpool(X, conv1_pool, conv1_active)\n",
    "\n",
    "    reconstruction = deconvolution(conv1, network['conv1'])\n",
    "\n",
    "    pwm = np.squeeze(reconstruction[0])\n",
    "    pwm = pwm/np.max(pwm)\n",
    "    pwm += .25\n",
    "    pwm[pwm<0] = 0\n",
    "    #pwm += np.mean(pwm)\n",
    "    norm = np.outer(np.ones(4), np.sum(pwm, axis=0))\n",
    "    pwm = pwm/norm\n",
    "\n",
    "    return pwm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datadir = '/home/peter/Data/SequenceMotif'\n",
    "savedir = make_directory(datadir, 'classification')\n",
    "\n",
    "X_test = np.copy(test[0])\n",
    "y_test = np.copy(test[1])\n",
    "labels = np.argmax(y_test,axis=1)\n",
    "model = np.copy(test[2])\n",
    "\n",
    "get_prediction = theano.function([nnmodel.input_var], layers.get_output(network['dense'], deterministic=True), allow_input_downcast=True)\n",
    "\n",
    "for class_index in range(20):\n",
    "\n",
    "    map_index = np.where(labels == class_index)[0]\n",
    "\n",
    "    score = []\n",
    "    for i in map_index:\n",
    "        prediction = get_prediction(np.expand_dims(X_test[i],0))\n",
    "        score.append(prediction[0][class_index])\n",
    "    score = np.array(score)\n",
    "\n",
    "    top_hits = 20\n",
    "    top_index = np.argsort(score)[::-1]\n",
    "    tophits_index = map_index[top_index[:top_hits]]\n",
    "    print score[top_index[:top_hits]]\n",
    "\n",
    "\n",
    "    for index in tophits_index:\n",
    "\n",
    "        X = np.copy(np.expand_dims(X_test[index],0))\n",
    "        class_index = np.argmax(y_test[index,:])\n",
    "        prediction = np.argmax(get_prediction(X))\n",
    "\n",
    "        height=100\n",
    "        bp_width=20\n",
    "        size = (25.,10.0)\n",
    "\n",
    "        logo = seq_logo(np.squeeze(X[0]), height, bp_width, norm=0, rna=1, filepath='.')\n",
    "        fig = plt.figure(figsize=size);\n",
    "        plt.imshow(logo, interpolation='none');\n",
    "        plt.axis('off');\n",
    "        outfile = os.path.join(savedir,'top-class_sequence_'+ str(class_index)  + '_' + str(index) +'.pdf')\n",
    "        fig.savefig(outfile, format='pdf', dpi=1000)  \n",
    "        call(['pdfcrop', outfile, outfile])\n",
    "\n",
    "\n",
    "        logo = seq_logo(model[index], height, bp_width, norm=0, rna=1, filepath='.')\n",
    "        fig = plt.figure(figsize=size);\n",
    "        plt.imshow(logo, interpolation='none');\n",
    "        plt.axis('off');\n",
    "        plt.title(str(class_index),fontsize=20)\n",
    "        outfile = os.path.join(savedir,'top-class_model_'+ str(class_index)  + '_' + str(index) +'.pdf')\n",
    "        fig.savefig(outfile, format='pdf', dpi=1000)  \n",
    "        call(['pdfcrop', outfile, outfile])\n",
    "\n",
    "        pwm = deconvolve_activation(X, network, nnmodel)\n",
    "        logo = seq_logo(pwm, height, bp_width, norm=0, rna=1, filepath='.')\n",
    "        fig = plt.figure(figsize=size);\n",
    "        plt.imshow(logo, interpolation='none');\n",
    "        plt.axis('off');\n",
    "        plt.title(str(prediction),fontsize=20)\n",
    "        outfile = os.path.join(savedir,'top-class_reconstruct_'+ str(class_index)  + '_' + str(index) +'.pdf')\n",
    "        fig.savefig(outfile, format='pdf', dpi=1000)  \n",
    "        call(['pdfcrop', outfile, outfile])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity analysis (single sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map_index =  21\n",
    "\n",
    "X = test[0][map_index]\n",
    "X = np.expand_dims(X,0)\n",
    "y = test[1][map_index]\n",
    "model = test[2][map_index]\n",
    "print X.shape\n",
    "\n",
    "seq_length = X.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get prediction\n",
    "get_prediction = theano.function([nnmodel.input_var], layers.get_output(network['output'], deterministic=True), allow_input_downcast=True)\n",
    "prediction = get_prediction(X)\n",
    "print 'ground truth = ' + str(y)\n",
    "print 'prediction = ' + str(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get dense layer\n",
    "get_conv4pool = theano.function([nnmodel.input_var], layers.get_output(network['conv4_active'], deterministic=True), allow_input_downcast=True)\n",
    "conv4_pool = get_conv4pool(X)\n",
    "pool = np.squeeze(conv4_pool[0],axis=2)\n",
    "plt.plot(pool);\n",
    "\n",
    "\n",
    "#conv4_pool -= mean4\n",
    "\n",
    "conv4_pool, threshold = activation_filter(conv4_pool, percentile=0.15, window=0, norm=0)\n",
    "print threshold\n",
    "\n",
    "#index = np.where(conv4_pool > 0)[0]\n",
    "#conv4_pool[index] += mean4[index]\n",
    "\n",
    "#threshold = 0\n",
    "#conv4_pool[conv4_pool < threshold] = 0\n",
    "pool = np.squeeze(conv4_pool[0],axis=2)\n",
    "plt.plot(pool);\n",
    "plt.plot(range(pool.shape[0]),np.ones(pool.shape[0])*threshold,color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "            \"\"\"calculate entropy of each nucleotide\"\"\"\n",
    "            s = 0\n",
    "            for i in range(4):\n",
    "                if p[i] > 0:\n",
    "                    s -= p[i]*np.log2(p[i])\n",
    "            return s\n",
    "\n",
    "        num_nt, num_seq = pwm.shape\n",
    "        heights = np.zeros((num_nt,num_seq));\n",
    "        for i in range(num_seq):\n",
    "            if norm == 1:\n",
    "                total_height = height\n",
    "            else:\n",
    "                total_height = (np.log2(4) - entropy(pwm[:, i]))*height;\n",
    "            heights[:,i] = np.floor(pwm[:,i]*total_height);\n",
    "        return heights.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv3_pool = deconvolution(conv4_pool, network['conv4'])\n",
    "pool = np.squeeze(conv3_pool[0])\n",
    "#|pool = pool.reshape([-1,])\n",
    "plt.plot(pool.T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv3_pool = deconvolution(conv4_pool, network['conv4'])\n",
    "#mean = np.mean(np.mean(conv3_all,axis=0),axis=1)\n",
    "#conv3_pool = np.squeeze(conv3_pool[0]) - np.outer(mean,4)\n",
    "#conv3_pool = np.expand_dims(conv3_pool,0)\n",
    "#conv3_pool = np.expand_dims(conv3_pool,3)\n",
    "\n",
    "pool = np.squeeze(conv3_pool[0])\n",
    "pool = pool.reshape([-1,])\n",
    "plt.plot(pool);\n",
    "\n",
    "conv3_pool, threshold = activation_filter(conv3_pool, percentile=0.1, window=0, norm=0)\n",
    "#print threshold\n",
    "#threshold = 0\n",
    "conv3_pool[conv3_pool < threshold*.8] = 0\n",
    "plt.plot(range(len(pool)),np.ones(len(pool))*threshold,color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pool = get_feature_map_all(network['conv3_pool'], nnmodel.input_var, X)\n",
    "\n",
    "original_map = same_unpool_fmap(pool, seq_length)\n",
    "plt.figure()\n",
    "plt.imshow(original_map)\n",
    "plt.colorbar()\n",
    "\n",
    "\n",
    "amap3 = same_unpool_fmap(conv3_pool, seq_length)\n",
    "plt.figure()\n",
    "plt.imshow(amap3)\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv3_active = get_feature_map_all(network['conv3_active'], nnmodel.input_var, X)\n",
    "conv3 = maxunpool(X, conv3_pool, conv3_active)\n",
    "conv2_pool = deconvolution(conv3, network['conv3'])\n",
    "\n",
    "#mean = np.mean(np.mean(conv2_all,axis=0),axis=1)\n",
    "#conv2_pool = np.squeeze(conv2_pool[0]) - np.outer(mean,4)\n",
    "#conv2_pool = np.expand_dims(conv2_pool,0)\n",
    "#conv2_pool = np.expand_dims(conv2_pool,3)\n",
    "\n",
    "\n",
    "pool = np.squeeze(conv2_pool[0])\n",
    "plt.plot(pool.T);\n",
    "\n",
    "conv2_pool, threshold = activation_filter(conv2_pool, percentile=0.02, window=0, norm=0)\n",
    "#print threshold\n",
    "#conv2_pool[conv2_pool < threshold/2] = 0\n",
    "conv2_pool[conv2_pool < threshold*1.0] = 0\n",
    "\n",
    "plt.plot(range(pool.shape[1]),np.ones(pool.shape[1])*threshold,color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pool = get_feature_map_all(network['conv2_pool'], nnmodel.input_var, X)\n",
    "\n",
    "original_map = same_unpool_fmap(pool, seq_length)\n",
    "plt.figure()\n",
    "plt.imshow(original_map)\n",
    "plt.colorbar()\n",
    "\n",
    "\n",
    "amap2 = same_unpool_fmap(conv2_pool, seq_length)\n",
    "plt.figure()\n",
    "plt.imshow(amap2)\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv2_active = get_feature_map_all(network['conv2_active'], nnmodel.input_var, X)\n",
    "conv2 = maxunpool(X, conv2_pool, conv2_active)\n",
    "conv1_pool = deconvolution(conv2, network['conv2'])\n",
    "\n",
    "\n",
    "pool = np.squeeze(conv1_pool[0])\n",
    "plt.plot(pool.T);\n",
    "\n",
    "conv1_pool, threshold = activation_filter(conv1_pool, percentile=0.005, window=0, norm=0)\n",
    "#print threshold\n",
    "#threshold = 0\n",
    "conv1_pool[conv1_pool < threshold*1] = 0\n",
    "\n",
    "plt.plot(range(pool.shape[1]),np.ones(pool.shape[1])*threshold,color='r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pool = get_feature_map_all(network['conv1_pool'], nnmodel.input_var, X)\n",
    "\n",
    "original_map = same_unpool_fmap(pool, seq_length)\n",
    "plt.figure()\n",
    "plt.imshow(original_map)\n",
    "plt.colorbar()\n",
    "\n",
    "\n",
    "amap1 = same_unpool_fmap(conv1_pool, seq_length)\n",
    "plt.figure()\n",
    "plt.imshow(amap1)\n",
    "plt.colorbar()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "conv1_pool = deconvolution(conv2, network['conv2'])\n",
    "pool = np.squeeze(conv1_pool[0])\n",
    "\n",
    "pool = np.abs(pool)\n",
    "pool /= np.max(pool)*1.2\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(pool.T);\n",
    "plt.axis('tight')\n",
    "\n",
    "entropy = np.sum(-pool*np.log2(pool),axis=0)/np.log(2)\n",
    "plt.figure()\n",
    "plt.plot(entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv1_active = get_feature_map_all(network['conv1_active'], nnmodel.input_var, X)\n",
    "conv1 = maxunpool(X, conv1_pool, conv1_active)\n",
    "reconstruction = deconvolution(conv1, network['conv1'])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.squeeze(reconstruction[0]).T)\n",
    "\n",
    "#reconstruction[reconstruction<0]=0\n",
    "\n",
    "pwm = np.squeeze(reconstruction[0])\n",
    "pwm = pwm/np.max(pwm)\n",
    "pwm += .25\n",
    "pwm[pwm<0] = 0\n",
    "#pwm = pwm + np.mean(pwm)\n",
    "norm = np.outer(np.ones(4), np.sum(pwm, axis=0))\n",
    "pwm = pwm/norm\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(pwm.T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "height=100\n",
    "bp_width=20\n",
    "size = (25.,10.0)\n",
    "\n",
    "logo = seq_logo(np.squeeze(X[0]), height, bp_width, norm=0, rna=1, filepath='.')\n",
    "fig = plt.figure(figsize=size);\n",
    "plt.imshow(logo, interpolation='none');\n",
    "plt.axis('off');\n",
    "\n",
    "logo = seq_logo(model, height, bp_width, norm=0, rna=1, filepath='.')\n",
    "fig = plt.figure(figsize=size);\n",
    "plt.imshow(logo, interpolation='none');\n",
    "plt.axis('off');\n",
    "#plt.title(str(y),fontsize=20)\n",
    "\n",
    "logo = seq_logo(pwm, height, bp_width, norm=0, rna=1, filepath='.')\n",
    "fig = plt.figure(figsize=size);\n",
    "plt.imshow(logo, interpolation='none');\n",
    "plt.axis('off');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def deconvolve_activation(X, network, nnmodel, mean4):\n",
    "\n",
    "\n",
    "    get_conv4pool = theano.function([nnmodel.input_var], layers.get_output(network['conv4_active'], deterministic=True), allow_input_downcast=True)\n",
    "    conv4_pool = get_conv4pool(X)\n",
    "    conv4_pool, threshold = activation_filter(conv4_pool, percentile=0.15, window=0, norm=0)\n",
    "\n",
    "    conv3_pool = deconvolution(conv4_pool, network['conv4'])\n",
    "    conv3_pool, threshold = activation_filter(conv3_pool, percentile=0.1, window=0, norm=0)\n",
    "    conv3_pool[conv3_pool < threshold*1] = 0\n",
    "    conv3_active = get_feature_map_all(network['conv3_active'], nnmodel.input_var, X)\n",
    "    conv3 = maxunpool(X, conv3_pool, conv3_active)\n",
    "\n",
    "    conv2_pool = deconvolution(conv3, network['conv3'])\n",
    "    conv2_pool, threshold = activation_filter(conv2_pool, percentile=0.03, window=0, norm=0)\n",
    "    conv2_pool[conv2_pool < threshold*1] = 0\n",
    "    conv2_active = get_feature_map_all(network['conv2_active'], nnmodel.input_var, X)\n",
    "    conv2 = maxunpool(X, conv2_pool, conv2_active)\n",
    "    \n",
    "    conv1_pool = deconvolution(conv2, network['conv2'])\n",
    "    conv1_pool, threshold = activation_filter(conv1_pool, percentile=0.005, window=0, norm=1)\n",
    "    conv1_pool[conv1_pool < threshold*.8] = 0\n",
    "    conv1_active = get_feature_map_all(network['conv1_active'], nnmodel.input_var, X)\n",
    "    conv1 = maxunpool(X, conv1_pool, conv1_active)\n",
    "\n",
    "    reconstruction = deconvolution(conv1, network['conv1'])\n",
    "\n",
    "    pwm = np.squeeze(reconstruction[0])\n",
    "    pwm = pwm/np.max(pwm)\n",
    "    pwm += .25\n",
    "    pwm[pwm<0] = 0\n",
    "    norm = np.outer(np.ones(4), np.sum(pwm, axis=0))\n",
    "    pwm = pwm/norm\n",
    "\n",
    "    return pwm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_index = 3\n",
    "\n",
    "labels = np.argmax(test[1],axis=1)\n",
    "map_index = np.where(labels == class_index)[0]\n",
    "\n",
    "get_prediction = theano.function([nnmodel.input_var], layers.get_output(network['output'], deterministic=True), allow_input_downcast=True)\n",
    "    \n",
    "for index in map_index[:5]:\n",
    "\n",
    "    X = np.expand_dims(test[0][index],0)\n",
    "    class_index = np.argmax(test[1][index,:])\n",
    "    prediction = np.argmax(get_prediction(X))\n",
    "\n",
    "    height=100\n",
    "    bp_width=20\n",
    "    size = (25.,10.0)\n",
    "\n",
    "    logo = seq_logo(np.squeeze(X[0]), height, bp_width, norm=0, rna=1, filepath='.')\n",
    "    fig = plt.figure(figsize=size);\n",
    "    plt.imshow(logo, interpolation='none');\n",
    "    plt.axis('off');\n",
    "\n",
    "    logo = seq_logo(test[2][index], height, bp_width, norm=0, rna=1, filepath='.')\n",
    "    fig = plt.figure(figsize=size);\n",
    "    plt.imshow(logo, interpolation='none');\n",
    "    plt.axis('off');\n",
    "    plt.title(str(class_index),fontsize=20)\n",
    "\n",
    "    pwm = deconvolve_activation(X, network, nnmodel)\n",
    "    logo = seq_logo(pwm, height, bp_width, norm=0, rna=1, filepath='.')\n",
    "    fig = plt.figure(figsize=size);\n",
    "    plt.imshow(logo, interpolation='none');\n",
    "    plt.axis('off');\n",
    "    plt.title(str(prediction),fontsize=20)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get_prediction = theano.function([nnmodel.input_var], layers.get_output(network['output'], deterministic=True), allow_input_downcast=True)\n",
    "    \n",
    "map_index = range(100,120)\n",
    "for index in map_index:\n",
    "\n",
    "    X = np.expand_dims(test[0][index],0)\n",
    "    class_index = np.argmax(test[1][index,:])\n",
    "    prediction = np.argmax(get_prediction(X))\n",
    "\n",
    "    height=100\n",
    "    bp_width=20\n",
    "    size = (25.,10.0)\n",
    "\n",
    "    logo = seq_logo(np.squeeze(X[0]), height, bp_width, norm=0, rna=1, filepath='.')\n",
    "    fig = plt.figure(figsize=size);\n",
    "    plt.imshow(logo, interpolation='none');\n",
    "    plt.axis('off');\n",
    "\n",
    "    logo = seq_logo(test[2][index], height, bp_width, norm=0, rna=1, filepath='.')\n",
    "    fig = plt.figure(figsize=size);\n",
    "    plt.imshow(logo, interpolation='none');\n",
    "    plt.axis('off');\n",
    "    plt.title(str(class_index),fontsize=20)\n",
    "\n",
    "    pwm = deconvolve_activation(X, network, nnmodel, mean4)\n",
    "    logo = seq_logo(pwm, height, bp_width, norm=0, rna=1, filepath='.')\n",
    "    fig = plt.figure(figsize=size);\n",
    "    plt.imshow(logo, interpolation='none');\n",
    "    plt.axis('off');\n",
    "    plt.title(str(prediction),fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# activation inversion to get pwm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv1_active = get_feature_map_all(network['conv1_active'], nnmodel.input_var, X)\n",
    "activation = np.squeeze(conv1_active[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_var2 = T.dmatrix('inputs')\n",
    "net_filter = {}\n",
    "net_filter['input'] = InputLayer(filter_size, input_var=input_var2)\n",
    "net_filter['output']  = Conv2DLayer(net_filter['input'], num_filters=1,\n",
    "                                          filter_size=filter_size,\n",
    "                                          W=init.GlorotUniform()\n",
    "                                          b=None,\n",
    "                                          pad='valid',\n",
    "                                          nonlinearity=None, flip_filters=False)\n",
    "\n",
    "target_var2 = T.dvector('seq')\n",
    "prediction = layers.get_output(net_filter['output'], deterministic=True)\n",
    "loss = objectives.squared_error(target_var2, prediction)\n",
    "loss = objectives.aggregate(loss, mode='mean')\n",
    "\n",
    "params = layers.get_all_params(deconv['output'], trainable=True)  \n",
    "grad = T.grad(loss, params)\n",
    "\n",
    "update_op = updates.adam(grad, params, learning_rate=0.001)\n",
    "train_fun = theano.function([input_var2, target_var2], loss, updates=update_op, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(200):\n",
    "    sys.stdout.write(\"\\rEpoch %d \\n\"%(epoch+1))\n",
    "    loss = train_fun(W,activation)\n",
    "    sys.stdout.write(\"\\r  loss = %f \\n\"%(loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# try to learn optimal thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import InverseLayer, ExpressionLayer, NonlinearityLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_feature_maps(layer, nnmodel, X):\n",
    "    feature_maps = theano.function([nnmodel.input_var], layers.get_output(layer, deterministic=True), \n",
    "                                   allow_input_downcast=True)\n",
    "    map_shape = get_output_shape(layer)\n",
    "\n",
    "    # get feature maps in batches for speed (large batches may be too much memory for GPU)\n",
    "    batch_size = 256\n",
    "    num_data = len(X)\n",
    "    num_batches = num_data // batch_size\n",
    "    shape = list(map_shape)\n",
    "    shape[0] = num_data\n",
    "    fmaps = np.empty(tuple(shape))\n",
    "    for i in range(num_batches):\n",
    "        index = range(i*batch_size, (i+1)*batch_size)    \n",
    "        fmaps[index] = feature_maps(X[index])\n",
    "\n",
    "    # get the rest of the feature maps\n",
    "    excess = num_data-num_batches*batch_size\n",
    "    index = range(num_data-excess, num_data)    \n",
    "    fmaps[index] = feature_maps(X[index])\n",
    "    return fmaps\n",
    "\n",
    "X = train[0]\n",
    "#fmaps4 = get_feature_maps(network['conv4_active'], nnmodel, X)\n",
    "#fmaps3 = get_feature_maps(network['conv3_active'], nnmodel, X)\n",
    "#fmaps2 = get_feature_maps(network['conv2_active'], nnmodel, X)\n",
    "#fmaps1 = get_feature_maps(network['conv1_active'], nnmodel, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shape2 = list(fmaps.shape)\n",
    "shape2[0]=None\n",
    "shape2 = tuple(shape2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_var2 = T.tensor4('inputs')\n",
    "deconv = {}\n",
    "deconv['input'] = InputLayer(tuple(shape2), input_var=input_var2)\n",
    "\n",
    "deconv['invconv4']  = Conv2DLayer(deconv['input'], num_filters=network['conv4'].input_shape[1],\n",
    "                                          filter_size=network['conv4'].filter_size,\n",
    "                                          W=network['conv4'].W.dimshuffle([1,0,2,3]), #W2_inv, #\n",
    "                                          b=init.Constant(-.0),\n",
    "                                          pad='full' if network['conv4'].pad==(0,0) else 'same',\n",
    "                                          nonlinearity=nonlinearities.rectify, flip_filters=True)\n",
    "deconv['invconv4'].params[deconv['invconv4'].W].remove('trainable')\n",
    "\n",
    "deconv['invconv3_pool'] = layers.Upscale2DLayer(deconv['invconv4'], network['conv3_pool'].pool_size)\n",
    "deconv['invconv3']  = Conv2DLayer(deconv['invconv3_pool'], num_filters=network['conv3'].input_shape[1],\n",
    "                                          filter_size=network['conv3'].filter_size,\n",
    "                                          W=network['conv3'].W.dimshuffle([1,0,2,3]), #W2_inv, #\n",
    "                                          b=init.Constant(-0.0), \n",
    "                                          pad='full' if network['conv3'].pad==(0,0) else 'same',\n",
    "                                          nonlinearity=nonlinearities.rectify, flip_filters=True)\n",
    "deconv['invconv3'].params[deconv['invconv3'].W].remove('trainable')\n",
    "\n",
    "deconv['invconv2_pool'] = layers.Upscale2DLayer(deconv['invconv3'], network['conv2_pool'].pool_size)\n",
    "deconv['invconv2']  = Conv2DLayer(deconv['invconv2_pool'], num_filters=network['conv2'].input_shape[1],\n",
    "                                          filter_size=network['conv2'].filter_size,\n",
    "                                          W=network['conv2'].W.dimshuffle([1,0,2,3]), #W2_inv, #\n",
    "                                          b=init.Constant(-0.0), \n",
    "                                          pad='full' if network['conv2'].pad==(0,0) else 'same',\n",
    "                                          nonlinearity=nonlinearities.rectify, flip_filters=True)\n",
    "deconv['invconv2'].params[deconv['invconv2'].W].remove('trainable')\n",
    "\n",
    "deconv['invconv1_pool'] = layers.Upscale2DLayer(deconv['invconv2'], network['conv1_pool'].pool_size)\n",
    "deconv['invconv1']  = Conv2DLayer(deconv['invconv1_pool'], num_filters=network['conv1'].input_shape[1],\n",
    "                                          filter_size=network['conv1'].filter_size,\n",
    "                                          W=network['conv1'].W.dimshuffle([1,0,2,3]), #W2_inv, #\n",
    "                                          b=init.Constant(-0.0), \n",
    "                                          pad='full' if network['conv1'].pad==(0,0) else 'same',\n",
    "                                          nonlinearity=None, flip_filters=True)\n",
    "deconv['invconv1'].params[deconv['invconv1'].W].remove('trainable')\n",
    "deconv['output'] = ExpressionLayer(deconv['invconv1'], lambda X: X/T.max(X) + .25, output_shape='auto')\n",
    "deconv['output'] = NonlinearityLayer(deconv['output'], nonlinearity=nonlinearities.rectify)\n",
    "deconv['output'] = ExpressionLayer(deconv['output'], lambda X: X/T.shape_padaxis(T.tile(T.sum(X,axis=1),4).transpose([0,2,1]),3), output_shape='auto')\n",
    "\n",
    "#offset = T.mean(X, axis=(1,2,3))\n",
    "#scale = 0#T.sum(input_var3, axis=(1))\n",
    "#scale = T.tile(norm,4).transpose([0,2,1])\n",
    "#deconv['output'] = layers.standardize(deconv['output'], offset, scale, shared_axes=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_var3 = T.tensor4('inputs3')\n",
    "target_var2 = T.tensor4('inputs2')\n",
    "target_var1 = T.tensor4('inputs1')\n",
    "target_var0 = T.tensor4('seq') \n",
    "\n",
    "prediction = layers.get_output(deconv['output'], deterministic=False)\n",
    "loss = objectives.squared_error(target_var0, prediction)\n",
    "loss = objectives.aggregate(loss)\n",
    "loss = objectives.aggregate(loss, mode='mean')\n",
    "\n",
    "recon3 = layers.get_output(deconv['invconv3_pool'], deterministic=False)\n",
    "loss3 = objectives.squared_error(target_var3, recon3)\n",
    "loss3 = objectives.aggregate(loss3, mode='mean')\n",
    "\n",
    "recon2 = layers.get_output(deconv['invconv2_pool'], deterministic=False)\n",
    "loss2 = objectives.squared_error(target_var2, recon2)\n",
    "loss2 = objectives.aggregate(loss2, mode='mean')\n",
    "\n",
    "recon1 = layers.get_output(deconv['invconv1_pool'], deterministic=False)\n",
    "loss1 = objectives.squared_error(target_var1, recon1)\n",
    "loss1 = objectives.aggregate(loss1, mode='mean')\n",
    "\n",
    "loss = loss + loss3 + loss2 + loss1\n",
    "\n",
    "def pos(x):\n",
    "    return T.sum(-T.abs_(x-.25))\n",
    "penalty = regularization.apply_penalty(params, pos)*.1\n",
    "loss = loss + penalty\n",
    "\n",
    "\n",
    "params = layers.get_all_params(deconv['output'], trainable=True)    \n",
    "grad = T.grad(loss, params)\n",
    "\n",
    "update_op = updates.adam(grad, params, learning_rate=0.001)\n",
    "train_fun = theano.function([input_var2, target_var0, target_var1, target_var2, target_var3], \n",
    "                            loss, updates=update_op, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_generator2(x1, x2, x3, x4, x5, batch_size=128):\n",
    "    for start_idx in range(0, len(x1)-batch_size+1, batch_size):\n",
    "        excerpt = slice(start_idx, start_idx+batch_size)\n",
    "        yield x1[excerpt].astype(np.float32), x2[excerpt].astype(np.float32), x3[excerpt].astype(np.float32), x4[excerpt].astype(np.float32), x5[excerpt].astype(np.float32)\n",
    "\n",
    "\n",
    "batch_size = 128            \n",
    "for epoch in range(200):\n",
    "    sys.stdout.write(\"\\rEpoch %d \\n\"%(epoch+1))\n",
    "\n",
    "    num_batches = train[0].shape[0] // batch_size\n",
    "    batches = batch_generator2(fmaps4, train[0], fmaps1, fmaps2, fmaps3, batch_size)\n",
    "    value = 0\n",
    "    for i in range(num_batches):\n",
    "        x1, x2, x3, x4, x5 = next(batches)\n",
    "        loss = train_fun(x1, x2, x3, x4, x5)\n",
    "        value += np.mean(loss)\n",
    "    sys.stdout.write(\"\\r  loss = %f \\n\"%(value/num_batches))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "target_var2 = T.tensor4('seq')\n",
    "prediction = layers.get_output(deconv['output'], deterministic=True)\n",
    "loss = objectives.squared_error(target_var2, prediction)\n",
    "loss = objectives.aggregate(loss, mode='mean')\n",
    "\n",
    "params = layers.get_all_params(deconv['output'], trainable=True)  \n",
    "\n",
    "def pos(x):\n",
    "    return T.sum(-T.abs_(x-.25))\n",
    "penalty = regularization.apply_penalty(params, pos)*1.\n",
    "loss = loss + penalty\n",
    "    \n",
    "grad = T.grad(loss, params)\n",
    "\n",
    "update_op = updates.adam(grad, params, learning_rate=0.001)\n",
    "train_fun = theano.function([input_var2, target_var2], [loss, prediction], updates=update_op, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_generator2(X, y, batch_size=128):\n",
    "    for start_idx in range(0, len(X)-batch_size+1, batch_size):\n",
    "        excerpt = slice(start_idx, start_idx+batch_size)\n",
    "        yield X[excerpt].astype(np.float32), y[excerpt].astype(np.float32)\n",
    "\n",
    "\n",
    "batch_size = 128            \n",
    "for epoch in range(200):\n",
    "    sys.stdout.write(\"\\rEpoch %d \\n\"%(epoch+1))\n",
    "\n",
    "    num_batches = train[0].shape[0] // batch_size\n",
    "    batches = batch_generator2(fmaps, train[0], batch_size)\n",
    "    value = 0\n",
    "    for i in range(num_batches):\n",
    "        X,y = next(batches)\n",
    "        loss = train_fun(X,y)\n",
    "        value += np.mean(loss[0])\n",
    "    sys.stdout.write(\"\\r  loss = %f \\n\"%(value/num_batches))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = 1\n",
    "\n",
    "X = np.expand_dims(test[0][index],0)    \n",
    "class_index = np.argmax(test[1][index,:])\n",
    "model = test[2][index]\n",
    "\n",
    "finput = feature_maps(np.expand_dims(test[0][index],0))\n",
    "test_fun = theano.function([input_var2], get_output(deconv['output']), allow_input_downcast=True)\n",
    "prediction = test_fun(finput)\n",
    "pwm = np.squeeze(prediction[0])                           \n",
    "\n",
    "height=100\n",
    "bp_width=20\n",
    "size = (25.,10.0)\n",
    "\n",
    "logo = seq_logo(np.squeeze(X[0]), height, bp_width, norm=0, rna=1, filepath='.')\n",
    "fig = plt.figure(figsize=size);\n",
    "plt.imshow(logo, interpolation='none');\n",
    "plt.axis('off');\n",
    "\n",
    "logo = seq_logo(test[2][index], height, bp_width, norm=0, rna=1, filepath='.')\n",
    "fig = plt.figure(figsize=size);\n",
    "plt.imshow(logo, interpolation='none');\n",
    "plt.axis('off');\n",
    "plt.title(str(class_index),fontsize=20)\n",
    "\n",
    "                           \n",
    "logo = seq_logo(pwm, height, bp_width, norm=0, rna=1, filepath='.')\n",
    "fig = plt.figure(figsize=size);\n",
    "plt.imshow(logo, interpolation='none');\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pwm.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# End-to-end deconvolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_var2 = T.tensor4('inputs')\n",
    "deconv = {}\n",
    "deconv['input'] = InputLayer(tuple(shape), input_var=input_var2)\n",
    "\n",
    "# 1st convolutional layer\n",
    "deconv['conv1']  = Conv2DLayer(deconv['input'], num_filters=network['conv1'].num_filters,\n",
    "                                          filter_size=network['conv1'].filter_size,\n",
    "                                          W=network['conv1'].W,\n",
    "                                          b=None, \n",
    "                                          pad=network['conv1'].pad,\n",
    "                                          nonlinearity=None, flip_filters=False)\n",
    "deconv['conv1_norm'] = BatchNormLayer(deconv['conv1_bias'], beta=network['conv1_batch'].beta, \n",
    "                                                            gamma=network['conv1_batch'].gamma, \n",
    "                                                            mean=network['conv1_batch'].mean, \n",
    "                                                            inv_std=network['conv1_batch'].inv_std)\n",
    "deconv['conv1_active'] = layers.ParametricRectifierLayer(deconv['conv1_norm'], \n",
    "                                                         alpha=network['conv1_active'].alpha)\n",
    "deconv['conv1'].params[deconv['conv1'].W].remove('trainable')\n",
    "deconv['conv1_norm'].params[deconv['conv1_norm'].beta].remove('trainable')\n",
    "deconv['conv1_norm'].params[deconv['conv1_norm'].gamma].remove('trainable')\n",
    "deconv['conv1_active'].params[deconv['conv1_active'].alpha].remove('trainable')\n",
    "deconv['conv1_pool'] = MaxPool2DLayer(deconv['conv1_active'], pool_size=network['conv1_pool'].pool_size)\n",
    "\n",
    "# 2nd convolutional layer\n",
    "deconv['conv2']  = Conv2DLayer(deconv['conv1_pool'], num_filters=network['conv2'].num_filters,\n",
    "                                          filter_size=network['conv2'].filter_size,\n",
    "                                          W=network['conv2'].W,\n",
    "                                          b=None, \n",
    "                                          pad=network['conv2'].pad,\n",
    "                                          nonlinearity=None, flip_filters=False)\n",
    "deconv['conv2_norm'] = BatchNormLayer(deconv['conv2_bias'], beta=network['conv2_batch'].beta, \n",
    "                                                            gamma=network['conv2_batch'].gamma, \n",
    "                                                            mean=network['conv2_batch'].mean, \n",
    "                                                            inv_std=network['conv2_batch'].inv_std)\n",
    "deconv['conv2_active'] = layers.ParametricRectifierLayer(deconv['conv2_norm'], \n",
    "                                                         alpha=network['conv2_active'].alpha)\n",
    "deconv['conv2'].params[deconv['conv2'].W].remove('trainable')\n",
    "deconv['conv2_norm'].params[deconv['conv2_norm'].beta].remove('trainable')\n",
    "deconv['conv2_norm'].params[deconv['conv2_norm'].gamma].remove('trainable')\n",
    "deconv['conv2_active'].params[deconv['conv2_active'].alpha].remove('trainable')\n",
    "deconv['conv2_pool'] = MaxPool2DLayer(deconv['conv2_active'], pool_size=network['conv2_pool'].pool_size)\n",
    "\n",
    "# 3rd convolutional layer\n",
    "deconv['conv3']  = Conv2DLayer(deconv['conv2_pool'], num_filters=network['conv3'].num_filters,\n",
    "                                          filter_size=network['conv3'].filter_size,\n",
    "                                          W=network['conv3'].W,\n",
    "                                          b=None, \n",
    "                                          pad=network['conv3'].pad,\n",
    "                                          nonlinearity=None, flip_filters=False)\n",
    "deconv['conv3_norm'] = BatchNormLayer(deconv['conv3_bias'], beta=network['conv3_batch'].beta, \n",
    "                                                            gamma=network['conv3_batch'].gamma, \n",
    "                                                            mean=network['conv3_batch'].mean, \n",
    "                                                            inv_std=network['conv3_batch'].inv_std)\n",
    "deconv['conv3_active'] = layers.ParametricRectifierLayer(deconv['conv3_norm'], \n",
    "                                                         alpha=network['conv3_active'].alpha)\n",
    "deconv['conv3'].params[deconv['conv3'].W].remove('trainable')\n",
    "deconv['conv3_norm'].params[deconv['conv3_norm'].beta].remove('trainable')\n",
    "deconv['conv3_norm'].params[deconv['conv3_norm'].gamma].remove('trainable')\n",
    "deconv['conv3_active'].params[deconv['conv3_active'].alpha].remove('trainable')\n",
    "deconv['conv3_pool'] = MaxPool2DLayer(deconv['conv3_active'], pool_size=network['conv3_pool'].pool_size)\n",
    "\n",
    "# 4th convolutional layer\n",
    "deconv['conv4']  = Conv2DLayer(deconv['conv3_pool'], num_filters=network['conv4'].num_filters,\n",
    "                                          filter_size=network['conv4'].filter_size,\n",
    "                                          W=network['conv4'].W,\n",
    "                                          b=None, \n",
    "                                          pad=network['conv4'].pad,\n",
    "                                          nonlinearity=None, flip_filters=False)\n",
    "deconv['conv4_norm'] = BatchNormLayer(deconv['conv4_bias'], beta=network['conv4_batch'].beta, \n",
    "                                                            gamma=network['conv4_batch'].gamma, \n",
    "                                                            mean=network['conv4_batch'].mean, \n",
    "                                                            inv_std=network['conv4_batch'].inv_std)\n",
    "deconv['conv4_active'] = layers.ParametricRectifierLayer(deconv['conv4_norm'], \n",
    "                                                         alpha=network['conv4_active'].alpha)\n",
    "deconv['conv4'].params[deconv['conv4'].W].remove('trainable')\n",
    "deconv['conv4_norm'].params[deconv['conv4_norm'].beta].remove('trainable')\n",
    "deconv['conv4_norm'].params[deconv['conv4_norm'].gamma].remove('trainable')\n",
    "deconv['conv4_active'].params[deconv['conv4_active'].alpha].remove('trainable')\n",
    "\n",
    "# dense output layer\n",
    "deconv['dense'] = DenseLayer(deconv['conv4_active'], num_units=network['dense'].num_units, \n",
    "                                         W=network['dense'].W, b=None, nonlinearity=None)\n",
    "deconv['dense_active'] = NonlinearityLayer(deconv['dense'], nonlinearity=nonlinearities.sigmoid)\n",
    "deconv['dense'].params[deconv['dense'].W].remove('trainable')\n",
    "\n",
    "deconv['encode'] = NonlinearityLayer(deconv['dense_active'], nonlinearity=None)\n",
    "\n",
    "#==============================================================================================\\\n",
    "# decode\n",
    "\n",
    "num_units = np.prod(list(get_output_shape(network['conv4_pool']))[1:])\n",
    "\n",
    "#deconv['invdense_bias'] = BiasLayer(deconv['encode'],b=-network['dense_bias'].b)\n",
    "deconv['invdense'] = DenseLayer(deconv['encode'], num_units=num_units, W=network['dense'].W.dimshuffle([1,0]), \n",
    "                                     b=init.Constant(0.05), nonlinearity=None)\n",
    "deconv['invdense'].params[deconv['invdense'].W].remove('trainable')\n",
    "deconv['invdense_active'] = NonlinearityLayer(deconv['invdense'], nonlinearity=nonlinearities.rectify)\n",
    "\n",
    "\n",
    "shape2 = list(get_output_shape(deconv['conv4_pool']))\n",
    "shape2[0] = -1\n",
    "deconv['reshape'] = layers.ReshapeLayer(deconv['invdense_active'], shape=tuple(shape2))\n",
    "\n",
    "#deconv['invconv4_pool'] = InverseLayer(deconv['reshape'], deconv['conv4_pool'])\n",
    "deconv['invconv4_pool'] = layers.Upscale2DLayer(deconv['reshape'], (5,1))\n",
    "#deconv['invconv4_bias'] = BiasLayer(deconv['invconv4_pool'],b=-network['conv4_bias'].b)\n",
    "deconv['invconv4']  = Conv2DLayer(deconv['invconv4_pool'], num_filters=network['conv4'].input_shape[1],\n",
    "                                          filter_size=network['conv4'].filter_size,\n",
    "                                          W=network['conv4'].W.dimshuffle([1,0,2,3]), #W2_inv, #\n",
    "                                          b=init.Constant(0.05), \n",
    "                                          pad='same',\n",
    "                                          nonlinearity=None, flip_filters=True)\n",
    "deconv['invconv4'].params[deconv['invconv4'].W].remove('trainable')\n",
    "#deconv['invconv4_norm'] = BatchNormLayer(deconv['invconv4'])\n",
    "deconv['invconv4_active'] = NonlinearityLayer(deconv['invconv4'], nonlinearity=nonlinearities.rectify)\n",
    "\n",
    "\n",
    "#deconv['invconv3_pool'] = InverseLayer(deconv['invconv4_active'], deconv['conv3_pool'])\n",
    "deconv['invconv3_pool'] = layers.Upscale2DLayer(deconv['invconv4_active'], (5,1))\n",
    "#deconv['invconv3_bias'] = BiasLayer(deconv['invconv3_pool'],b=-network['conv3_bias'].b)\n",
    "deconv['invconv3']  = Conv2DLayer(deconv['invconv3_pool'], num_filters=network['conv3'].input_shape[1],\n",
    "                                          filter_size=network['conv3'].filter_size,\n",
    "                                          W=network['conv3'].W.dimshuffle([1,0,2,3]), #W2_inv, #\n",
    "                                          b=init.Constant(0.05), \n",
    "                                          pad='same',\n",
    "                                          nonlinearity=None, flip_filters=True)\n",
    "deconv['invconv3'].params[deconv['invconv3'].W].remove('trainable')\n",
    "#deconv['invconv3_norm'] = BatchNormLayer(deconv['invconv3'])\n",
    "deconv['invconv3_active'] = NonlinearityLayer(deconv['invconv3'], nonlinearity=nonlinearities.rectify)\n",
    "\n",
    "\n",
    "\n",
    "#deconv['invconv2_pool'] = InverseLayer(deconv['invconv3_active'], deconv['conv2_pool'])\n",
    "deconv['invconv2_pool'] = layers.Upscale2DLayer(deconv['invconv3_active'], (2,1))\n",
    "#deconv['invconv2_bias'] = BiasLayer(deconv['invconv2_pool'],b=-network['conv2_bias'].b)\n",
    "deconv['invconv2']  = Conv2DLayer(deconv['invconv2_pool'], num_filters=network['conv2'].input_shape[1],\n",
    "                                          filter_size=network['conv2'].filter_size,\n",
    "                                          W=network['conv2'].W.dimshuffle([1,0,2,3]), #W2_inv, #\n",
    "                                          b=init.Constant(0.05), \n",
    "                                          pad='same',\n",
    "                                          nonlinearity=nonlinearities.rectify, flip_filters=True)\n",
    "deconv['invconv2'].params[deconv['invconv2'].W].remove('trainable')\n",
    "#deconv['invconv2_norm'] = BatchNormLayer(deconv['invconv2'])\n",
    "deconv['invconv2_active'] = NonlinearityLayer(deconv['invconv2'], nonlinearity=nonlinearities.rectify)\n",
    "\n",
    "#deconv['invconv1_pool'] = InverseLayer(deconv['invconv2_active'], deconv['conv1_pool'])\n",
    "deconv['invconv1_pool'] = layers.Upscale2DLayer(deconv['invconv2_active'], (4,1))\n",
    "#deconv['invconv1_bias'] = BiasLayer(deconv['invconv1_pool'],b=-network['conv1_bias'].b)\n",
    "deconv['invconv1']  = Conv2DLayer(deconv['invconv1_pool'], num_filters=network['conv1'].input_shape[1],\n",
    "                                          filter_size=network['conv1'].filter_size,\n",
    "                                          W=network['conv1'].W.dimshuffle([1,0,2,3]), #W2_inv, #\n",
    "                                          b=init.Constant(0.05), \n",
    "                                          pad='same',\n",
    "                                          nonlinearity=nonlinearities.sigmoid, flip_filters=True)\n",
    "deconv['invconv1'].params[deconv['invconv1'].W].remove('trainable')\n",
    "deconv['output'] = deconv['invconv1']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# monitor prediciton effect for various filter thresholds \n",
    "Z = T.tensor4()\n",
    "get_prediction2 = theano.function([Z], get_output(network['output'], {network['conv2_pool']:Z}, deterministic=True), allow_input_downcast=True)\n",
    "\n",
    "vals = np.linspace(0,np.max(pool)+5,100)\n",
    "z = []\n",
    "for thresh in vals:\n",
    "    test = np.copy(pool)\n",
    "    test[test < thresh] = 0\n",
    "    prediction2 = get_prediction2(test)\n",
    "    z.append(prediction2[0])\n",
    "z = np.array(z)\n",
    "\n",
    "# plot the predictions\n",
    "plt.figure()\n",
    "plt.plot(vals,z);\n",
    "z = z[:,prediction]\n",
    "plt.plot(vals,z,linewidth=3,color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# determine threshold to filter dense layer\n",
    "MAX = vals[np.argmax(z)]\n",
    "if MAX > 1:\n",
    "    index = np.where(z>.7)[0]\n",
    "    if index.any():\n",
    "        MAX = vals[index[-1]]\n",
    "threshold = MAX*.5\n",
    "#threshold = np.mean(dense1[0]) + np.std(dense1[0])/2\n",
    "num_units = len(dense[0])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(dense1[0])\n",
    "plt.plot(range(num_units),np.ones(num_units)*threshold, color='r',linewidth=3)\n",
    "print threshold\n",
    "\n",
    "threshold = np.outer(threshold, np.ones(num_units))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val = np.squeeze(pool[0])\n",
    "\n",
    "num_units = val.shape[1]\n",
    "plt.plot(val.T);\n",
    "threshold = np.mean(val) + np.std(val)*3\n",
    "plt.plot(range(num_units), np.ones(num_units)*threshold, color='r', linewidth=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spikes = np.squeeze(pool[0])\n",
    "\n",
    "plt.figure()\n",
    "#plt.errorbar(range(40), np.sum(spikes,axis=1), np.std(spikes,axis=1));\n",
    "plt.plot(range(spikes.shape[1]), np.sum(spikes,axis=0));\n",
    "sum_spikes = np.sum(spikes,axis=0)\n",
    "threshold = np.mean(sum_spikes) + np.std(sum_spikes)\n",
    "print threshold\n",
    "\n",
    "intermediate = np.squeeze(pool[0])\n",
    "\n",
    "# filter intermediate layer\n",
    "window = 1\n",
    "\n",
    "\n",
    "index = np.where(sum_spikes > threshold)[0]\n",
    "print index\n",
    "filter_map = np.zeros(intermediate.shape)\n",
    "for i in index:\n",
    "    MIN = np.maximum(0, i-window)\n",
    "    MAX = np.minimum(intermediate.shape[1],i+window)\n",
    "    filter_map[:,MIN:MAX] = intermediate[:,MIN:MAX]\n",
    "    \n",
    "filter_map[filter_map < 1] = 0\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(filter_map.T)\n",
    "\n",
    "filter_map = np.expand_dims(filter_map,0)\n",
    "filter_map = np.expand_dims(filter_map,3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# determine threshold to filter conv2 layer\n",
    "MAX = vals[np.argmax(z)]\n",
    "if MAX > 1:\n",
    "    index = np.where(z>.75)[0]\n",
    "    if index.any():\n",
    "        MAX = vals[index[-1]]\n",
    "threshold = MAX*.5\n",
    "# filter conv2 layer and plot activations\n",
    "pool[pool < threshold] = 0\n",
    "\n",
    "prediction2 = get_prediction2(pool)\n",
    "print np.argmax(prediction2[0])\n",
    "print prediction2[0]\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.squeeze(pool[0].T));\n",
    "#threshold = MAX*.5\n",
    "threshold = np.minimum(MAX, 10)\n",
    "threshold\n",
    "\n",
    "intermediate = np.squeeze(pool[0])\n",
    "\n",
    "# filter intermediate layer\n",
    "window = 2\n",
    "\n",
    "#for i in range(intermediate.shape[1]-window):\n",
    "MAX = np.max(intermediate,axis=0)\n",
    "index = np.where(MAX > threshold)[0]\n",
    "\n",
    "filter_map = np.zeros(intermediate.shape)\n",
    "for i in index:\n",
    "    MIN = np.maximum(0, i-window)\n",
    "    MAX = np.minimum(intermediate.shape[1],i+window)\n",
    "    filter_map[:,MIN:MAX] = intermediate[:,MIN:MAX]\n",
    "    \n",
    "filter_map[filter_map < 0.2] = 0\n",
    "plt.plot(filter_map.T)\n",
    "\n",
    "filter_map = np.expand_dims(filter_map,0)\n",
    "filter_map = np.expand_dims(filter_map,3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# variance filter\n",
    "var = np.var(x,axis=0)\n",
    "num_units = len(var)\n",
    "plt.figure()\n",
    "plt.plot(var)\n",
    "threshold = np.mean(var) + np.std(var)/2\n",
    "plt.plot(range(num_units), np.ones(num_units)*threshold, color='r', linewidth=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def IdentifyMotifs(X, prediction, nnmodel, network):\n",
    "    \n",
    "    # get dense layer\n",
    "    get_dense1 = theano.function([nnmodel.input_var], layers.get_output(network['dense1_active'], deterministic=True), allow_input_downcast=True)\n",
    "    dense1 = get_dense1(X)\n",
    "    num_units = len(dense1)\n",
    "\n",
    "    W = network['dense2'].W.get_value()\n",
    "    b = network['dense2_bias'].b.get_value()\n",
    "    \n",
    "    # monitor prediciton effect for various filter thresholds \n",
    "    Z = T.dmatrix()\n",
    "    get_prediction2 = theano.function([Z], get_output(network['output'], {network['dense1_active']:Z}, deterministic=True), allow_input_downcast=True)\n",
    "\n",
    "    vals = np.linspace(0,20,50)\n",
    "    threshold = []\n",
    "    z = []\n",
    "    for thresh in vals:\n",
    "        test = np.copy(dense1)\n",
    "        test[test < thresh] = 0\n",
    "        prediction2 = get_prediction2(test)\n",
    "        z.append(prediction2[0])\n",
    "    z = np.array(z)\n",
    "    z = z[:,prediction]\n",
    "    \n",
    "    # determine threshold to filter dense layer\n",
    "    MAX = vals[np.argmax(z)]\n",
    "    \n",
    "    if MAX > 1:\n",
    "        index = np.where(z>.7)[0]\n",
    "        if index.any():\n",
    "            MAX = vals[index[-1]]\n",
    "    threshold.append(MAX*.8)\n",
    "    threshold = np.array(threshold)\n",
    "    threshold = np.outer(threshold, np.ones(num_units))\n",
    "    \n",
    "    # filter dense layer and plot activations\n",
    "    dense1[dense1 < threshold] = 0\n",
    "    prediction2 = get_prediction2(dense1)\n",
    "\n",
    "    W = network['dense1'].W.get_value()\n",
    "    b = network['dense1_bias'].b.get_value()\n",
    "\n",
    "    # psuedo-inverse filters\n",
    "    U, s, V = np.linalg.svd(W, full_matrices=True)\n",
    "    S = np.zeros(W.T.shape)\n",
    "    S[:W.shape[1],:W.shape[1]] = np.diag(1/s)\n",
    "    Winv = np.dot(V.T,np.dot(S, U.T))\n",
    "\n",
    "    # inverse the activation\n",
    "    inverse = dense1 # np.log(np.exp(dense1) + 1e-7)\n",
    "    inverse -= np.outer(np.ones(num_units),b)\n",
    "    inverse = np.dot(inverse, W.T)\n",
    "\n",
    "    shape = get_output_shape(network['conv2_pool'])\n",
    "    shape = (-1, shape[1], shape[2], shape[3])\n",
    "    pool = inverse.reshape(shape)\n",
    "\n",
    "    Z = T.dmatrix()\n",
    "    get_prediction2 = theano.function([Z], get_output(network['output'], {network['conv2_pool']:Z}, deterministic=True), allow_input_downcast=True)\n",
    "\n",
    "    test = np.reshape(pool,[1,-1])\n",
    "    prediction2 = get_prediction2(test)\n",
    "    \n",
    "    # monitor prediciton effect for various filter thresholds \n",
    "    Z = T.tensor4()\n",
    "    get_prediction2 = theano.function([Z], get_output(network['output'], {network['conv2_pool']:Z}, deterministic=True), allow_input_downcast=True)\n",
    "    vals = np.linspace(0,50,50)\n",
    "    threshold = []\n",
    "    z = []\n",
    "    for thresh in vals:\n",
    "        test = np.copy(pool)\n",
    "        test[test < thresh] = 0\n",
    "        prediction2 = get_prediction2(test)\n",
    "        z.append(prediction2[0])\n",
    "    z = np.array(z)\n",
    "    z = z[:,prediction]\n",
    "    \n",
    "    # determine threshold to filter conv2 layer\n",
    "    MAX = vals[np.argmax(z)]\n",
    "    if MAX > 1:\n",
    "        index = np.where(z>.7)[0]\n",
    "        if index.any():\n",
    "            MAX = vals[index[-1]]\n",
    "    threshold.append(MAX*.8)\n",
    "    threshold = np.array(threshold)\n",
    "    threshold = np.outer(threshold, np.ones(num_units))\n",
    "\n",
    "    # filter conv2 layer and plot activations\n",
    "    pool[pool < threshold] = 0\n",
    "    prediction2 = get_prediction2(pool)\n",
    "\n",
    "    # max-unpool layer 2\n",
    "    active = get_feature_map_all(network['conv2_active'], nnmodel.input_var, X)\n",
    "    pool_size = active.shape[2]/pool.shape[2]\n",
    "    fmap2 = []\n",
    "    for k in range(active.shape[0]):\n",
    "        x = np.squeeze(active[k])\n",
    "        mymap = np.squeeze(pool[k])\n",
    "\n",
    "        max_index= []\n",
    "        for i in range(x.shape[1]/pool_size):\n",
    "            index = range(i*pool_size,(i+1)*pool_size)\n",
    "            max_index.append(np.argmax(x[:,index],axis=1))\n",
    "        max_index = np.array(max_index)\n",
    "        max_index\n",
    "\n",
    "        dim,seq_length = mymap.shape\n",
    "        fmap_unpool = np.zeros((dim,seq_length*pool_size))\n",
    "        for i in range(x.shape[1]/pool_size):\n",
    "\n",
    "            index = range(i*pool_size,(i+1)*pool_size)\n",
    "            for j in range(dim):\n",
    "                fmap_unpool[j,index[max_index[i][j]]] = mymap[j,i]\n",
    "        fmap2.append(fmap_unpool)\n",
    "\n",
    "    fmap2 = np.array(fmap2)\n",
    "    fmap2 = np.expand_dims(fmap2, 3)\n",
    "    # psuedo-inverse filters\n",
    "    W2 = network['conv2'].W.get_value()\n",
    "    W2_inv = pseudoinverse_filter2(W2)\n",
    "\n",
    "    # deconvolution layer 2\n",
    "    input_var2 = T.tensor4('fmap')\n",
    "    shape2 = list(fmap2.shape)\n",
    "    shape2[0] = None\n",
    "    input2 = InputLayer(shape=tuple(shape2), input_var=input_var2)\n",
    "    unpool2 = BiasLayer(input2, b=-network['conv2_bias'].b)\n",
    "    deconv2 = Conv2DLayer(unpool2, num_filters=network['conv2'].input_shape[1],\n",
    "                                          filter_size=network['conv2'].filter_size,\n",
    "                                          W=network['conv2'].W.dimshuffle([1,0,2,3]), #W2_inv, #\n",
    "                                          b=None, \n",
    "                                          pad='same',\n",
    "                                          nonlinearity=None, flip_filters=True)\n",
    "    predict = theano.function([input_var2], get_output(deconv2, deterministic=True), allow_input_downcast=True)\n",
    "\n",
    "    intermediate = predict(fmap2.astype(np.float32))\n",
    "    intermediate = np.array(intermediate)\n",
    "\n",
    "    #plt.plot(np.squeeze(intermediate[0]).T);\n",
    "    \n",
    "    # filter intermediate layer\n",
    "    intermediate[intermediate < 0.5] = 0\n",
    "\n",
    "    pool = intermediate\n",
    "\n",
    "    # max-unpool layer 1\n",
    "    active = get_feature_map_all(network['conv1_active'], nnmodel.input_var,X)\n",
    "\n",
    "    pool_size = active.shape[2]/pool.shape[2]\n",
    "    fmap1 = []\n",
    "    for k in range(active.shape[0]):\n",
    "        x = np.squeeze(active[k])\n",
    "        mymap = np.squeeze(pool[k])\n",
    "\n",
    "        max_index = []\n",
    "        for i in range(x.shape[1]/pool_size):\n",
    "            index = range(i*pool_size,(i+1)*pool_size)\n",
    "            max_index.append(np.argmax(x[:,index],axis=1))\n",
    "        max_index = np.array(max_index)\n",
    "        max_index\n",
    "\n",
    "        dim,seq_length = mymap.shape\n",
    "        fmap_unpool = np.zeros((dim,seq_length*pool_size))\n",
    "        for i in range(x.shape[1]/pool_size):\n",
    "            index = range(i*pool_size,(i+1)*pool_size)\n",
    "            for j in range(dim):\n",
    "                fmap_unpool[j,index[max_index[i][j]]] = mymap[j,i]\n",
    "        fmap1.append(fmap_unpool)\n",
    "\n",
    "    fmap1 = np.array(fmap1)\n",
    "    fmap1 = np.expand_dims(fmap1, 3)\n",
    "\n",
    "\n",
    "    W1 = network['conv1'].W.get_value()\n",
    "    W1_inv = pseudoinverse_filter2(W1)\n",
    "\n",
    "\n",
    "    # deconvolution layer 1\n",
    "    input_var1 = T.tensor4('fmap')\n",
    "    shape1 = list(fmap1.shape)\n",
    "    shape1[0] = None\n",
    "    input1 = InputLayer(shape=tuple(shape1), input_var=input_var1)\n",
    "    unpool1 = ExpressionLayer(input1, lambda X: T.log(T.exp(X)-1 + 1e-7), output_shape='auto')\n",
    "    unpool1 = BiasLayer(unpool1, b=-network['conv1_bias'].b)\n",
    "    deconv1 = Conv2DLayer(unpool1, num_filters=network['conv1'].input_shape[1],\n",
    "                                          filter_size=network['conv1'].filter_size,\n",
    "                                          W=network['conv1'].W.dimshuffle([1,0,2,3]), #W1_inv, # \n",
    "                                          b=None, \n",
    "                                          pad='same',\n",
    "                                          nonlinearity=None, flip_filters=True)\n",
    "\n",
    "\n",
    "    reconstruction = theano.function([input_var1], get_output(deconv1, deterministic=True), allow_input_downcast=True)\n",
    "    X2 = np.squeeze(reconstruction(fmap1.astype(np.float32)))\n",
    "    return X2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X2, labels = reconstruct_layer2(network, train)\n",
    "map_index = 7\n",
    "\n",
    "X = train[0][map_index]\n",
    "X = np.expand_dims(X,0)\n",
    "y = np.argmax(train[1][map_index])\n",
    "\n",
    "# get prediction\n",
    "get_prediction = theano.function([nnmodel.input_var], layers.get_output(network['output'], deterministic=True), allow_input_downcast=True)\n",
    "prediction = get_prediction(X)\n",
    "prediction = np.argmax(prediction)\n",
    "print 'ground truth = ' + str(y)\n",
    "print 'prediction = ' + str(prediction)\n",
    "\n",
    "X2 = IdentifyMotifs(X, prediction, nnmodel, network)\n",
    "\n",
    "height=300\n",
    "bp_width=30\n",
    "num_seq = X2.shape[1]\n",
    "width = bp_width*num_seq\n",
    "size = (25.,25.0)\n",
    "\n",
    "\n",
    "logo = seq_logo(np.squeeze(X[0]), height, width, norm=0, rna=1, filepath='.')\n",
    "fig = plt.figure(figsize=size);\n",
    "plt.imshow(logo, interpolation='none');\n",
    "plt.axis('off');\n",
    "plt.title(str(y));\n",
    "\n",
    "\n",
    "logo = seq_logo(np.squeeze(model[y]), height, width, norm=0, rna=1, filepath='.')\n",
    "fig = plt.figure(figsize=size);\n",
    "plt.imshow(logo, interpolation='none');\n",
    "plt.axis('off');\n",
    "plt.title(str(y));\n",
    "\n",
    "x = X2\n",
    "for i in range(5):\n",
    "    x[:,:10]=0\n",
    "    x[:,-10:]=0\n",
    "    MEAN = np.nanmean(x,axis=1)\n",
    "    x -= np.outer(MEAN, np.ones(x.shape[1]))\n",
    "\n",
    "#x -= np.max(x, axis=0)\n",
    "#x -= np.min(x, axis=0)\n",
    "x /= np.log(2)\n",
    "x = np.exp(x)\n",
    "sumX = np.sum(x,axis=0) \n",
    "x /= np.outer(np.ones(4),sumX)\n",
    "logo = seq_logo(x, height, width, norm=0, rna=1, filepath='.')\n",
    "fig = plt.figure(figsize=size)\n",
    "plt.imshow(logo, interpolation='none') \n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = network['dense1'].W.get_value()\n",
    "b = network['dense1_bias'].b.get_value()\n",
    "\n",
    "# psuedo-inverse filters\n",
    "U, s, V = np.linalg.svd(W, full_matrices=True)\n",
    "S = np.zeros(W.T.shape)\n",
    "S[:W.shape[1],:W.shape[1]] = np.diag(1/s)\n",
    "Winv = np.dot(V.T,np.dot(S, U.T))\n",
    "\n",
    "# inverse the activation\n",
    "num_units = dense1.shape[0]\n",
    "inverse =  dense1 #np.log(np.exp(dense1) - 1 + 1e-7) #dense1 #\n",
    "#inverse -= np.outer(np.ones(num_units),b)\n",
    "inverse = np.dot(inverse, W.T)\n",
    "\n",
    "shape = get_output_shape(network['conv2_pool'])\n",
    "shape = (-1, shape[1], shape[2], shape[3])\n",
    "pool = inverse.reshape(shape)\n",
    "\n",
    "# original pool\n",
    "get_pool = theano.function([nnmodel.input_var], layers.get_output(network['conv2_pool'], deterministic=True), allow_input_downcast=True)\n",
    "pool2 = get_pool(X)\n",
    "plt.figure()\n",
    "plt.plot(np.squeeze(pool2[0].T));\n",
    "\n",
    "# reconstructed pool\n",
    "plt.figure()\n",
    "plt.plot(np.squeeze(pool[0].T));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pool = get_pool(X)\n",
    "Z = T.dmatrix()\n",
    "get_prediction2 = theano.function([Z], get_output(network['output'], {network['conv2_pool']:Z}, deterministic=True), allow_input_downcast=True)\n",
    "\n",
    "test = np.reshape(pool,[1,-1])\n",
    "prediction2 = get_prediction2(test)\n",
    "\n",
    "print np.argmax(prediction2)\n",
    "print prediction2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val = np.squeeze(pool[0])\n",
    "val[val < 0] = 0\n",
    "\n",
    "spikes = np.sum(val,axis=0)\n",
    "num_units = len(spikes)\n",
    "plt.plot(spikes.T)\n",
    "threshold = np.mean(spikes) +  np.std(spikes)*2\n",
    "plt.plot(range(num_units), np.ones(num_units)*threshold, color='r', linewidth=3)\n",
    "index = np.where(spikes > threshold)[0]\n",
    "index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val = np.squeeze(pool[0])\n",
    "val[val < 0] = 0\n",
    "\n",
    "# variance filter\n",
    "var = np.var(val,axis=0)\n",
    "num_units = len(var)\n",
    "plt.figure()\n",
    "plt.plot(var)\n",
    "threshold = np.mean(var) + np.std(var)/10\n",
    "plt.plot(range(num_units), np.ones(num_units)*threshold, color='r', linewidth=3)\n",
    "index = np.where(var > threshold)[0]\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "window = 1\n",
    "filter_map = np.zeros(val.shape)\n",
    "for i in index:\n",
    "    MIN = np.maximum(0, i-window)\n",
    "    MAX = np.minimum(val.shape[1],i+window)\n",
    "    filter_map[:,MIN:MAX+1] = val[:,MIN:MAX+1]\n",
    "    \n",
    "filter_map[filter_map < 0] = 0\n",
    "plt.figure()\n",
    "plt.plot(filter_map.T)\n",
    "\n",
    "filter_map = np.expand_dims(filter_map,0)\n",
    "filter_map = np.expand_dims(filter_map,3)\n",
    "\n",
    "\n",
    "pool = filter_map\n",
    "\n",
    "#prediction2 = get_prediction2(pool)\n",
    "#print np.argmax(prediction2[0])\n",
    "#print prediction2[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X2, labels = reconstruct_layer2(network, train)\n",
    "map_index = range(5000)\n",
    "X = train[0][map_index]\n",
    "y = np.argmax(train[1][map_index], axis=1)\n",
    "\n",
    "# psuedo-inverse filters\n",
    "W3 = network['dense1'].W.get_value()\n",
    "b3 = network['dense1_bias'].b.get_value()\n",
    "dense1 = get_feature_map(network['dense1_active'], nnmodel.input_var, X, map_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(dense1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z = []\n",
    "for thresh in vals:\n",
    "    test = np.copy(dense1[index])\n",
    "    test[test < thresh] = 0\n",
    "    z.append(np.dot(W.T,test) + b)\n",
    "z = np.array(z)\n",
    "print z.shape\n",
    "print np.argmax(z,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = network['dense2'].W.get_value()\n",
    "b = network['dense2_bias'].b.get_value()\n",
    "\n",
    "\n",
    "plot_index = range(10)\n",
    "vals = np.linspace(0,20,50)\n",
    "threshold = []\n",
    "for index in map_index:\n",
    "    z = []\n",
    "    for thresh in vals:\n",
    "        test = np.copy(dense1[index])\n",
    "        test[test < thresh] = 0\n",
    "        z.append(np.dot(W.T,test) + b)\n",
    "    z = np.array(z)\n",
    "    z[np.argmax(z,axis=0)]\n",
    "    \n",
    "    z = z[:,y[index]]\n",
    "    if index in plot_index:\n",
    "        plt.plot(vals,z);\n",
    "    MAX = vals[np.argmax(z)]\n",
    "    #MAX = vals[np.argmax(np.where(z > 1)[0])]\n",
    "    threshold.append(MAX)\n",
    "\n",
    "threshold = np.array(threshold)\n",
    "threshold = np.outer(threshold, np.ones(dense1.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# filter dense layer\n",
    "#dense1[dense1 < threshold] = 0\n",
    "bias3 = dense1-np.outer(np.ones(dense1.shape[0]),b3)\n",
    "inv3 = bias3# np.log(np.exp(bias3) + 1e-7)\n",
    "active3 = np.dot(inv3, W3.T)\n",
    "\n",
    "pool=get_feature_map_all(network['conv2_pool'], nnmodel.input_var, X)\n",
    "pool = active3.reshape(pool.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.squeeze(pool[0].T));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pool[pool<10] = 0\n",
    "pool[:,:,:2]=0\n",
    "pool[:,:,-2:]=0\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.squeeze(pool[0].T));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# psuedo-inverse filters\n",
    "W1 = network['conv1'].W.get_value()\n",
    "W2 = network['conv2'].W.get_value()\n",
    "W1_inv = pseudoinverse_filter2(W1)\n",
    "W2_inv = pseudoinverse_filter2(W2)\n",
    "\n",
    "# max-unpool layer 2\n",
    "active = get_feature_map_all(network['conv2_active'], nnmodel.input_var, X)\n",
    "#\n",
    "\n",
    "\n",
    "pool_size = active.shape[2]/pool.shape[2]\n",
    "fmap2 = []\n",
    "for k in range(active.shape[0]):\n",
    "    x = np.squeeze(active[k])\n",
    "    mymap = np.squeeze(pool[k])\n",
    "\n",
    "    max_index= []\n",
    "    for i in range(x.shape[1]/pool_size):\n",
    "        index = range(i*pool_size,(i+1)*pool_size)\n",
    "        max_index.append(np.argmax(x[:,index],axis=1))\n",
    "    max_index = np.array(max_index)\n",
    "    max_index\n",
    "\n",
    "    dim,seq_length = mymap.shape\n",
    "    fmap_unpool = np.zeros((dim,seq_length*pool_size))\n",
    "    for i in range(x.shape[1]/pool_size):\n",
    "        \n",
    "        index = range(i*pool_size,(i+1)*pool_size)\n",
    "        for j in range(dim):\n",
    "            fmap_unpool[j,index[max_index[i][j]]] = mymap[j,i]\n",
    "    fmap2.append(fmap_unpool)\n",
    "\n",
    "fmap2 = np.array(fmap2)\n",
    "fmap2 = np.expand_dims(fmap2, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# deconvolution layer 2\n",
    "input_var2 = T.tensor4('fmap')\n",
    "shape2 = list(fmap2.shape)\n",
    "shape2[0] = None\n",
    "input2 = InputLayer(shape=tuple(shape2), input_var=input_var2)\n",
    "#unpool2 = ExpressionLayer(input2, lambda X: T.log(T.exp(X)-1 + 1e-7), output_shape='auto')\n",
    "unpool2 = BiasLayer(input2, b=-network['conv2_bias'].b)\n",
    "deconv2 = Conv2DLayer(unpool2, num_filters=network['conv2'].input_shape[1],\n",
    "                                      filter_size=network['conv2'].filter_size,\n",
    "                                      W=network['conv2'].W.dimshuffle([1,0,2,3]), #W2_inv, #\n",
    "                                      b=None, \n",
    "                                      pad='same',\n",
    "                                      nonlinearity=None, flip_filters=True)\n",
    "prediction = get_output(deconv2)\n",
    "prediction = theano.function([input_var2], prediction, allow_input_downcast=True)\n",
    "\n",
    "intermediate = prediction(fmap2.astype(np.float32))\n",
    "intermediate = np.array(intermediate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.squeeze(intermediate[0]).T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(intermediate)):\n",
    "    for j in range(5):\n",
    "        x = np.squeeze(intermediate[i])\n",
    "        x[:,:5]=0\n",
    "        x[:,-5:]=0\n",
    "        MEAN = np.mean(intermediate[i], axis=1)\n",
    "        x -= np.outer(MEAN, np.ones(x.shape[1]))\n",
    "        intermediate[i] = np.expand_dims(x,2)    \n",
    "plt.plot(np.squeeze(intermediate[0]).T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val=.8\n",
    "intermediate[intermediate < val] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.squeeze(intermediate[0]).T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pool = intermediate\n",
    "\n",
    "# max-unpool layer 1\n",
    "active = get_feature_map_all(network['conv1_active'], nnmodel.input_var,X)\n",
    "\n",
    "pool_size = active.shape[2]/pool.shape[2]\n",
    "fmap1 = []\n",
    "for k in range(active.shape[0]):\n",
    "    x = np.squeeze(active[k])\n",
    "    mymap = np.squeeze(pool[k])\n",
    "\n",
    "    max_index = []\n",
    "    for i in range(x.shape[1]/pool_size):\n",
    "        index = range(i*pool_size,(i+1)*pool_size)\n",
    "        max_index.append(np.argmax(x[:,index],axis=1))\n",
    "    max_index = np.array(max_index)\n",
    "    max_index\n",
    "\n",
    "    dim,seq_length = mymap.shape\n",
    "    fmap_unpool = np.zeros((dim,seq_length*pool_size))\n",
    "    for i in range(x.shape[1]/pool_size):\n",
    "        index = range(i*pool_size,(i+1)*pool_size)\n",
    "        for j in range(dim):\n",
    "            fmap_unpool[j,index[max_index[i][j]]] = mymap[j,i]\n",
    "    fmap1.append(fmap_unpool)\n",
    "\n",
    "fmap1 = np.array(fmap1)\n",
    "fmap1 = np.expand_dims(fmap1, 3)\n",
    "\n",
    "# deconvolution layer 1\n",
    "input_var1 = T.tensor4('fmap')\n",
    "shape1 = list(fmap1.shape)\n",
    "shape1[0] = None\n",
    "input1 = InputLayer(shape=tuple(shape1), input_var=input_var1)\n",
    "unpool1 = ExpressionLayer(input1, lambda X: T.log(T.exp(X)-1 + 1e-7), output_shape='auto')\n",
    "unpool1 = BiasLayer(unpool1, b=-network['conv1_bias'].b)\n",
    "deconv1 = Conv2DLayer(unpool1, num_filters=network['conv1'].input_shape[1],\n",
    "                                      filter_size=network['conv1'].filter_size,\n",
    "                                      W=network['conv1'].W.dimshuffle([1,0,2,3]), #W1_inv, # \n",
    "                                      b=None, \n",
    "                                      pad='same',\n",
    "                                      nonlinearity=None, flip_filters=True)\n",
    "\n",
    "prediction = get_output(deconv1)\n",
    "reconstruction = theano.function([input_var1], prediction, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X2 = np.squeeze(reconstruction(fmap1.astype(np.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = 0\n",
    "x = np.squeeze(X2[index])\n",
    "\n",
    "for i in range(5):\n",
    "    x[:,:10]=0\n",
    "    x[:,-10:]=0\n",
    "    MEAN = np.nanmean(x,axis=1)\n",
    "    x -= np.outer(MEAN, np.ones(x.shape[1]))\n",
    "plt.plot(x.T);\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_class_pwm(X, class_index, norm=0):\n",
    "    class_pwm = 0\n",
    "    for i in class_index:\n",
    "        x = X[i]\n",
    "        if norm == 1:\n",
    "            for i in range(5):\n",
    "                x[:,:10]=0\n",
    "                x[:,-10:]=0\n",
    "                MEAN = np.nanmean(x,axis=1)\n",
    "                x -= np.outer(MEAN, np.ones(x.shape[1]))\n",
    "            sumX = np.sum(x,axis=0)\n",
    "            x /= np.outer(np.ones(4),sumX\n",
    "        class_pwm += x\n",
    "    class_pwm /= len(class_index)\n",
    "    return class_pwm\n",
    "\n",
    "model = []\n",
    "for class_plot in range(20):\n",
    "    y = np.argmax(train[1], axis=1)\n",
    "    y = y[map_index]\n",
    "    class_index = np.where(y == class_plot)[0]\n",
    "    model.append(get_class_pwm(np.squeeze(X), class_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for index in range(20):\n",
    "    \n",
    "    height=300\n",
    "    bp_width=30\n",
    "    num_seq = X2.shape[2]\n",
    "    width = bp_width*num_seq\n",
    "    size = (25.,25.0)\n",
    "\n",
    "    logo = seq_logo(np.squeeze(train[0][map_index[index]]), height, width, norm=0, rna=1, filepath='.')\n",
    "    fig = plt.figure(figsize=size)\n",
    "    plt.imshow(logo, interpolation='none') \n",
    "    plt.axis('off');\n",
    "    plt.title(str(y[index]))\n",
    "    \n",
    "    logo = seq_logo(model[y[index]], height, width, norm=0, rna=1, filepath='.')\n",
    "    fig = plt.figure(figsize=size)\n",
    "    plt.imshow(logo, interpolation='none') \n",
    "    plt.axis('off');\n",
    "    plt.title(str(y[index]))\n",
    "\n",
    "    \n",
    "    x = np.squeeze(X2[index])\n",
    "    for i in range(5):\n",
    "        x[:,:10]=0\n",
    "        x[:,-10:]=0\n",
    "        MEAN = np.nanmean(x,axis=1)\n",
    "        x -= np.outer(MEAN, np.ones(x.shape[1]))\n",
    "\n",
    "    #x -= np.max(x, axis=0)\n",
    "    x = np.exp(x)\n",
    "    #x = np.exp(x)\n",
    "    #x -= np.min(x, axis=0)\n",
    "    sumX = np.sum(x,axis=0)\n",
    "    x /= np.outer(np.ones(4),sumX)\n",
    "    logo = seq_logo(x, height, width, norm=0, rna=1, filepath='.')\n",
    "    fig = plt.figure(figsize=size)\n",
    "    plt.imshow(logo, interpolation='none') \n",
    "    plt.axis('off');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
