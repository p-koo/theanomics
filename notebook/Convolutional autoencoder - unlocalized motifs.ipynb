{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 980 (CNMeM is disabled, CuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "sys.path.append('..')\n",
    "from src import NeuralNet\n",
    "from src import train as fit\n",
    "from src import make_directory \n",
    "from models import load_model\n",
    "from data import load_data\n",
    "from six.moves import cPickle\n",
    "from subprocess import call\n",
    "\n",
    "np.random.seed(247) # for reproducibility\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'figure.autolayout': True})\n",
    "%matplotlib inline\n",
    "from scipy.misc import imresize\n",
    "\n",
    "from lasagne import layers, nonlinearities, updates, objectives, init \n",
    "from lasagne.layers import Conv2DLayer, TransposedConv2DLayer, DenseLayer, InputLayer, ExpressionLayer, BiasLayer\n",
    "\n",
    "from lasagne.layers import get_output, get_output_shape, get_all_params\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "np.random.seed(247) # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=40_G=20_data.pickle\n",
      "loading train data\n",
      "loading cross-validation data\n",
      "loading test data\n"
     ]
    }
   ],
   "source": [
    "filename = 'Unlocalized_N=100000_S=200_M=40_G=20_data.pickle'\n",
    "datapath = '/home/peter/Data/SequenceMotif'\n",
    "filepath = os.path.join(datapath, filename)\n",
    "\n",
    "# load training set\n",
    "print \"loading data from: \" + filepath\n",
    "f = open(filepath, 'rb')\n",
    "print \"loading train data\"\n",
    "train = cPickle.load(f)\n",
    "print \"loading cross-validation data\"\n",
    "cross_validation = cPickle.load(f)\n",
    "print \"loading test data\"\n",
    "test = cPickle.load(f)\n",
    "f.close()\n",
    "\n",
    "X_train = train[0].transpose((0,1,2)).astype(np.float32)\n",
    "y_train = train[1].astype(np.int32)\n",
    "X_val = cross_validation[0].transpose((0,1,2)).astype(np.float32)\n",
    "y_val = cross_validation[1].astype(np.int32)\n",
    "X_test = test[0].transpose((0,1,2)).astype(np.float32)\n",
    "y_test = test[1].astype(np.int32)\n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=3)\n",
    "X_val = np.expand_dims(X_val, axis=3)\n",
    "X_test = np.expand_dims(X_test, axis=3)\n",
    "\n",
    "train = (X_train, y_train, train[2])\n",
    "valid = (X_val, y_val, cross_validation[2])\n",
    "test = (X_test, y_test, test[2])\n",
    "\n",
    "shape = (None, train[0].shape[1], train[0].shape[2], train[0].shape[3])\n",
    "num_labels = train[1].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model from: /home/peter/Data/SequenceMotif/Unlocalized_N=100000_S=200_M=40_G=20_model.pickle\n"
     ]
    }
   ],
   "source": [
    "filename = 'Unlocalized_N=100000_S=200_M=40_G=20_model.pickle'\n",
    "datapath = '/home/peter/Data/SequenceMotif'\n",
    "filepath = os.path.join(datapath, filename)\n",
    "\n",
    "# load training set\n",
    "print \"loading model from: \" + filepath\n",
    "f = open(filepath, 'rb')\n",
    "options = cPickle.load(f)\n",
    "model = cPickle.load(f)\n",
    "seq_model = cPickle.load(f)\n",
    "ground_truth = cPickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_name = \"genome_motif_model\"\n",
    "nnmodel = NeuralNet(model_name, shape, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv1': <lasagne.layers.dnn.Conv2DDNNLayer at 0x7f30b8a46910>,\n",
       " 'conv1_active': <lasagne.layers.special.ParametricRectifierLayer at 0x7f30527290d0>,\n",
       " 'conv1_batch': <lasagne.layers.normalization.BatchNormLayer at 0x7f305277ff90>,\n",
       " 'conv1_bias': <lasagne.layers.special.BiasLayer at 0x7f30b8a46810>,\n",
       " 'conv2': <lasagne.layers.dnn.Conv2DDNNLayer at 0x7f3056332990>,\n",
       " 'conv2_active': <lasagne.layers.special.ParametricRectifierLayer at 0x7f3052729a90>,\n",
       " 'conv2_batch': <lasagne.layers.normalization.BatchNormLayer at 0x7f30527293d0>,\n",
       " 'conv2_bias': <lasagne.layers.special.BiasLayer at 0x7f3052729110>,\n",
       " 'conv2_pool': <lasagne.layers.pool.MaxPool2DLayer at 0x7f3052729550>,\n",
       " 'conv3': <lasagne.layers.dnn.Conv2DDNNLayer at 0x7f3052729ad0>,\n",
       " 'conv3_active': <lasagne.layers.special.ParametricRectifierLayer at 0x7f3052730590>,\n",
       " 'conv3_batch': <lasagne.layers.normalization.BatchNormLayer at 0x7f3052729dd0>,\n",
       " 'conv3_bias': <lasagne.layers.special.BiasLayer at 0x7f3052729c90>,\n",
       " 'conv4': <lasagne.layers.dnn.Conv2DDNNLayer at 0x7f3052729f50>,\n",
       " 'conv4_active': <lasagne.layers.special.ParametricRectifierLayer at 0x7f3052730f50>,\n",
       " 'conv4_batch': <lasagne.layers.normalization.BatchNormLayer at 0x7f3052730890>,\n",
       " 'conv4_bias': <lasagne.layers.special.BiasLayer at 0x7f30527305d0>,\n",
       " 'conv4_pool': <lasagne.layers.pool.MaxPool2DLayer at 0x7f3052730a10>,\n",
       " 'conv5': <lasagne.layers.dnn.Conv2DDNNLayer at 0x7f3052730f90>,\n",
       " 'conv5_active': <lasagne.layers.special.ParametricRectifierLayer at 0x7f3052735990>,\n",
       " 'conv5_batch': <lasagne.layers.normalization.BatchNormLayer at 0x7f30527352d0>,\n",
       " 'conv5_bias': <lasagne.layers.special.BiasLayer at 0x7f3052735190>,\n",
       " 'conv6': <lasagne.layers.dnn.Conv2DDNNLayer at 0x7f3052735450>,\n",
       " 'conv6_active': <lasagne.layers.special.ParametricRectifierLayer at 0x7f305273e390>,\n",
       " 'conv6_batch': <lasagne.layers.normalization.BatchNormLayer at 0x7f3052735c90>,\n",
       " 'conv6_bias': <lasagne.layers.special.BiasLayer at 0x7f30527359d0>,\n",
       " 'conv6_pool': <lasagne.layers.pool.MaxPool2DLayer at 0x7f3052735e10>,\n",
       " 'conv7': <lasagne.layers.dnn.Conv2DDNNLayer at 0x7f305273e3d0>,\n",
       " 'conv7_active': <lasagne.layers.special.ParametricRectifierLayer at 0x7f305273ed90>,\n",
       " 'conv7_batch': <lasagne.layers.normalization.BatchNormLayer at 0x7f305273e6d0>,\n",
       " 'conv7_bias': <lasagne.layers.special.BiasLayer at 0x7f305273e590>,\n",
       " 'conv7_pool': <lasagne.layers.pool.MaxPool2DLayer at 0x7f305273e850>,\n",
       " 'dense1': <lasagne.layers.dense.DenseLayer at 0x7f305273edd0>,\n",
       " 'dense1_active': <lasagne.layers.special.ParametricRectifierLayer at 0x7f3052744b90>,\n",
       " 'dense1_batch': <lasagne.layers.normalization.BatchNormLayer at 0x7f3052744490>,\n",
       " 'dense1_bias': <lasagne.layers.special.BiasLayer at 0x7f305273ef90>,\n",
       " 'dense1_dropout': <lasagne.layers.noise.DropoutLayer at 0x7f3052744610>,\n",
       " 'dense2': <lasagne.layers.dense.DenseLayer at 0x7f3052730090>,\n",
       " 'dense2_active': <lasagne.layers.special.ParametricRectifierLayer at 0x7f305274d690>,\n",
       " 'dense2_batch': <lasagne.layers.normalization.BatchNormLayer at 0x7f3052744f10>,\n",
       " 'dense2_bias': <lasagne.layers.special.BiasLayer at 0x7f3052744dd0>,\n",
       " 'dense2_dropout': <lasagne.layers.noise.DropoutLayer at 0x7f305274d110>,\n",
       " 'dense3': <lasagne.layers.dense.DenseLayer at 0x7f305274d8d0>,\n",
       " 'dense3_active': <lasagne.layers.special.NonlinearityLayer at 0x7f305274da50>,\n",
       " 'dense3_bias': <lasagne.layers.special.BiasLayer at 0x7f305274d910>,\n",
       " 'input': <lasagne.layers.input.InputLayer at 0x7f30b8a46750>,\n",
       " 'output': <lasagne.layers.special.NonlinearityLayer at 0x7f305274da50>}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = nnmodel.network\n",
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 out of 500 \n",
      "[==                            ] 7.5% -- time=86s -- loss=2.34045 -- accuracy=85.71%  "
     ]
    }
   ],
   "source": [
    "nnmodel = fit.train_minibatch(nnmodel, train, valid, batch_size=128, num_epochs=500, \n",
    "                        patience=2, verbose=1, filepath=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_output(X,layer):\n",
    "    num_data = len(X)\n",
    "    feature_maps = theano.function([nnmodel.input_var], layers.get_output(layer,deterministic=True), allow_input_downcast=True)\n",
    "    map_shape = get_output_shape(layer)\n",
    "\n",
    "    # get feature maps in batches for speed (large batches may be too much memory for GPU)\n",
    "    batch_size=500\n",
    "    num_batches = num_data // batch_size\n",
    "    shape = list(map_shape)\n",
    "    shape[0] = num_data\n",
    "    output = np.empty(tuple(shape))\n",
    "    for i in range(num_batches):\n",
    "        index = range(i*batch_size, (i+1)*batch_size)    \n",
    "        output[index] = feature_maps(X[index])\n",
    "\n",
    "    # get the rest of the feature maps\n",
    "    excess = num_data-num_batches*batch_size\n",
    "    if excess:\n",
    "        index = range(num_data-excess, num_data)  \n",
    "        output[index] = feature_maps(X[index])\n",
    "    return output\n",
    "\n",
    "\n",
    "X = train[0]\n",
    "layer = network['output']\n",
    "output = get_output(X,layer)\n",
    "output2 = get_output(test[0],layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shape2 = list(output.shape)\n",
    "shape2[0] = None\n",
    "input_var2 = T.dmatrix('input')\n",
    "input_var2 = T.cast(input_var2, 'float32')\n",
    "\n",
    "\n",
    "deconv = {}\n",
    "deconv['input'] = InputLayer(tuple(shape2), input_var=input_var2)\n",
    "num_units = list(get_output_shape(network['dense1']))[1]\n",
    "deconv['dense3'] = layers.DenseLayer(deconv['input'], num_units=num_units, W=network['dense3'].W.dimshuffle([1,0]), \n",
    "                                     b=init.Constant(0.05), nonlinearity=nonlinearities.rectify)\n",
    "num_units = np.prod(list(get_output_shape(network['conv2_pool']))[1:])\n",
    "deconv['dense2'] = layers.DenseLayer(deconv['dense3'], num_units=num_units, W=network['dense2'].W.dimshuffle([1,0]), \n",
    "                                     b=init.Constant(0.05), nonlinearity=nonlinearities.rectify)\n",
    "num_units = np.prod(list(get_output_shape(network['conv2_pool']))[1:])\n",
    "deconv['dense1'] = layers.DenseLayer(deconv['dense2'], num_units=num_units, W=network['dense1'].W.dimshuffle([1,0]), \n",
    "                                     b=init.Constant(0.05), nonlinearity=nonlinearities.rectify)\n",
    "\n",
    "shape = list(get_output_shape(network['conv7_pool']))\n",
    "shape[0] = -1\n",
    "deconv['reshape'] = layers.ReshapeLayer(deconv['dense1'], shape=tuple(shape))\n",
    "deconv['pool7'] = layers.Upscale2DLayer(deconv['reshape'], (2,1))\n",
    "deconv['conv7']  = Conv2DLayer(deconv['pool7'], num_filters=network['conv7'].input_shape[1],\n",
    "                                          filter_size=network['conv7'].filter_size,\n",
    "                                          W=network['conv7'].W.dimshuffle([1,0,2,3]), #W2_inv, #\n",
    "                                          b=init.Constant(0.05), \n",
    "                                          pad='same',\n",
    "                                          nonlinearity=nonlinearities.rectify, flip_filters=True)\n",
    "\n",
    "deconv['pool6'] = layers.Upscale2DLayer(deconv['conv7'], (2,1))\n",
    "deconv['conv6']  = Conv2DLayer(deconv['pool6'], num_filters=network['conv6'].input_shape[1],\n",
    "                                          filter_size=network['conv6'].filter_size,\n",
    "                                          W=network['conv6'].W.dimshuffle([1,0,2,3]), #W2_inv, #\n",
    "                                          b=init.Constant(0.05), \n",
    "                                          pad='same',\n",
    "                                          nonlinearity=nonlinearities.rectify, flip_filters=True)\n",
    "\n",
    "deconv['conv5']  = Conv2DLayer(deconv['conv6'], num_filters=network['conv5'].input_shape[1],\n",
    "                                         filter_size=network['conv5'].filter_size,\n",
    "                                          W=network['conv5'].W.dimshuffle([1,0,2,3]), #W2_inv, #\n",
    "                                          b=init.Constant(0.05), \n",
    "                                          pad='same',\n",
    "                                          nonlinearity=nonlinearities.rectify, flip_filters=True)\n",
    "\n",
    "deconv['pool4'] = layers.Upscale2DLayer(deconv['conv5'], (2,1))\n",
    "deconv['conv4']  = Conv2DLayer(deconv['pool7'], num_filters=network['conv4'].input_shape[1],\n",
    "                                          filter_size=network['conv4'].filter_size,\n",
    "                                          W=network['conv4'].W.dimshuffle([1,0,2,3]), #W2_inv, #\n",
    "                                          b=init.Constant(0.05), \n",
    "                                          pad='same',\n",
    "                                          nonlinearity=nonlinearities.rectify, flip_filters=True)\n",
    "\n",
    "deconv['conv3']  = Conv2DLayer(deconv['conv4'], num_filters=network['conv3'].input_shape[1],\n",
    "                                          filter_size=network['conv3'].filter_size,\n",
    "                                          W=network['conv3'].W.dimshuffle([1,0,2,3]), #W2_inv, #\n",
    "                                          b=init.Constant(0.05), \n",
    "                                          pad='same',\n",
    "                                          nonlinearity=nonlinearities.rectify, flip_filters=True)\n",
    "\n",
    "deconv['pool2'] = layers.Upscale2DLayer(deconv['conv3'], (2,1))\n",
    "deconv['conv2']  = Conv2DLayer(deconv['pool2'], num_filters=network['conv2'].input_shape[1],\n",
    "                                          filter_size=network['conv2'].filter_size,\n",
    "                                          W=network['conv2'].W.dimshuffle([1,0,2,3]), #W2_inv, #\n",
    "                                          b=init.Constant(0.05), \n",
    "                                          pad='same',\n",
    "                                          nonlinearity=nonlinearities.rectify, flip_filters=True)\n",
    "\n",
    "deconv['conv1']  = Conv2DLayer(deconv['conv2'], num_filters=network['conv1'].input_shape[1],\n",
    "                                          filter_size=network['conv1'].filter_size,\n",
    "                                          W=network['conv1'].W.dimshuffle([1,0,2,3]), #W2_inv, #\n",
    "                                          b=init.Constant(0.05), \n",
    "                                          pad='same',\n",
    "                                          nonlinearity=nonlinearities.sigmoid, flip_filters=True)\n",
    "deconv['output'] = deconv['conv1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_var = T.tensor4('targets')\n",
    "prediction = layers.get_output(deconv['output'], deterministic=False)\n",
    "\n",
    "loss = (target_var - prediction) ** 2\n",
    "loss = loss.mean()\n",
    "\n",
    "params = layers.get_all_params(deconv['output'], trainable=True)    \n",
    "grad = T.grad(loss, params)\n",
    "\n",
    "update_op = updates.adam(grad, params, learning_rate=0.001)\n",
    "train_fun = theano.function([input_var2, target_var], [loss, prediction], updates=update_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size=128):\n",
    "    for start_idx in range(0, len(X)-batch_size+1, batch_size):\n",
    "        excerpt = slice(start_idx, start_idx+batch_size)\n",
    "        yield X[excerpt].astype(np.float32), y[excerpt].astype(np.float32)\n",
    "\n",
    "batch_size = 128        \n",
    "for epoch in range(50):\n",
    "    sys.stdout.write(\"\\rEpoch %d \\n\"%(epoch+1))\n",
    "\n",
    "    num_batches = output.shape[0] // batch_size\n",
    "    batches = batch_generator(output, train[0], batch_size)\n",
    "    value = 0\n",
    "    for i in range(num_batches):\n",
    "        X,y = next(batches)\n",
    "        loss, prediction = train_fun(X, y)\n",
    "        value += np.mean(loss)\n",
    "    sys.stdout.write(\"\\r  loss = %f \\n\"%(value/num_batches))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq_logo(pwm, height=100, nt_width=20, norm=0, rna=1, filepath='.'):\n",
    "    \"\"\"generate a sequence logo from a pwm\"\"\"\n",
    "    \n",
    "    def load_alphabet(filepath, rna):\n",
    "        \"\"\"load images of nucleotide alphabet \"\"\"\n",
    "        df = pd.read_table(os.path.join(filepath, 'A.txt'), header=None);\n",
    "        A_img = df.as_matrix()\n",
    "        A_img = np.reshape(A_img, [72, 65, 3], order=\"F\").astype(np.uint8)\n",
    "\n",
    "        df = pd.read_table(os.path.join(filepath, 'C.txt'), header=None);\n",
    "        C_img = df.as_matrix()\n",
    "        C_img = np.reshape(C_img, [76, 64, 3], order=\"F\").astype(np.uint8)\n",
    "\n",
    "        df = pd.read_table(os.path.join(filepath, 'G.txt'), header=None);\n",
    "        G_img = df.as_matrix()\n",
    "        G_img = np.reshape(G_img, [76, 67, 3], order=\"F\").astype(np.uint8)\n",
    "\n",
    "        if rna == 1:\n",
    "            df = pd.read_table(os.path.join(filepath, 'U.txt'), header=None);\n",
    "            T_img = df.as_matrix()\n",
    "            T_img = np.reshape(T_img, [74, 57, 3], order=\"F\").astype(np.uint8)\n",
    "        else:\n",
    "            df = pd.read_table(os.path.join(filepath, 'T.txt'), header=None);\n",
    "            T_img = df.as_matrix()\n",
    "            T_img = np.reshape(T_img, [72, 59, 3], order=\"F\").astype(np.uint8)\n",
    "\n",
    "        return A_img, C_img, G_img, T_img\n",
    "\n",
    "\n",
    "    def get_nt_height(pwm, height, norm):\n",
    "        \"\"\"get the heights of each nucleotide\"\"\"\n",
    "\n",
    "        def entropy(p):\n",
    "            \"\"\"calculate entropy of each nucleotide\"\"\"\n",
    "            s = 0\n",
    "            for i in range(4):\n",
    "                if p[i] > 0:\n",
    "                    s -= p[i]*np.log2(p[i])\n",
    "            return s\n",
    "\n",
    "        num_nt, num_seq = pwm.shape\n",
    "        heights = np.zeros((num_nt,num_seq));\n",
    "        for i in range(num_seq):\n",
    "            if norm == 1:\n",
    "                total_height = height\n",
    "            else:\n",
    "                total_height = (np.log2(4) - entropy(pwm[:, i]))*height;\n",
    "            heights[:,i] = np.floor(pwm[:,i]*total_height);\n",
    "        return heights.astype(int)\n",
    "\n",
    "    \n",
    "    # get the alphabet images of each nucleotide\n",
    "    A_img, C_img, G_img, T_img = load_alphabet(filepath='.', rna=1)\n",
    "    \n",
    "    \n",
    "    # get the heights of each nucleotide\n",
    "    heights = get_nt_height(pwm, height, norm)\n",
    "    \n",
    "    # resize nucleotide images for each base of sequence and stack\n",
    "    num_nt, num_seq = pwm.shape\n",
    "    width = np.ceil(nt_width*num_seq).astype(int)\n",
    "    \n",
    "    total_height = np.sum(heights,axis=0)\n",
    "    max_height = np.max(total_height)\n",
    "    logo = np.ones((height*2, width, 3)).astype(int)*255;\n",
    "    for i in range(num_seq):\n",
    "        remaining_height = total_height[i];\n",
    "        offset = max_height-remaining_height\n",
    "        nt_height = np.sort(heights[:,i]);\n",
    "        index = np.argsort(heights[:,i])\n",
    "\n",
    "        for j in range(num_nt):\n",
    "            if nt_height[j] > 0:\n",
    "                # resized dimensions of image\n",
    "                resize = (nt_height[j], nt_width)\n",
    "                if index[j] == 0:\n",
    "                    nt_img = imresize(A_img, resize)\n",
    "                elif index[j] == 1:\n",
    "                    nt_img = imresize(C_img, resize)\n",
    "                elif index[j] == 2:\n",
    "                    nt_img = imresize(G_img, resize)\n",
    "                elif index[j] == 3:\n",
    "                    nt_img = imresize(T_img, resize)\n",
    "\n",
    "                # determine location of image\n",
    "                height_range = range(remaining_height-nt_height[j], remaining_height)\n",
    "                width_range = range(i*nt_width, i*nt_width+nt_width)\n",
    "\n",
    "                # 'annoying' way to broadcast resized nucleotide image\n",
    "                if height_range:\n",
    "                    for k in range(3):\n",
    "                        for m in range(len(width_range)):\n",
    "                            logo[height_range+offset, width_range[m],k] = nt_img[:,m,k];\n",
    "\n",
    "                remaining_height -= nt_height[j]\n",
    "\n",
    "    return logo.astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = layers.get_output(deconv['output'], deterministic=False)\n",
    "test_fun = theano.function([input_var2], prediction)\n",
    "\n",
    "class_index = 17\n",
    "\n",
    "labels = np.argmax(test[1],axis=1)\n",
    "map_index = np.where(labels == class_index)[0]\n",
    "for index in map_index[:20]:\n",
    "    \n",
    "    y = np.expand_dims(output2[index,:],0)\n",
    "    prediction = test_fun(y.astype(np.float32))\n",
    "    class_index = np.argmax(test[1][index,:])\n",
    "\n",
    "    height=100\n",
    "    bp_width=20\n",
    "    size = (25.,10.0)\n",
    "\n",
    "    logo = seq_logo(np.squeeze(test[0][index]), height, bp_width, norm=0, rna=1, filepath='.')\n",
    "    fig = plt.figure(figsize=size);\n",
    "    plt.imshow(logo, interpolation='none');\n",
    "    plt.axis('off');\n",
    "\n",
    "    logo = seq_logo(test[2][index], height, bp_width, norm=0, rna=1, filepath='.')\n",
    "    fig = plt.figure(figsize=size);\n",
    "    plt.imshow(logo, interpolation='none');\n",
    "    plt.axis('off');\n",
    "    plt.title(str(class_index),fontsize=20)\n",
    "\n",
    "    logo = seq_logo(np.squeeze(prediction[0]), height, bp_width, norm=0, rna=1, filepath='.')\n",
    "    fig = plt.figure(figsize=size);\n",
    "    plt.imshow(logo, interpolation='none');\n",
    "    plt.axis('off');\n",
    "    plt.title(str(np.argmax(output2[index,:]))+'; p='+str(np.max(output2[index,:])),fontsize=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = layers.get_output(deconv['output'], deterministic=False)\n",
    "test_fun = theano.function([input_var2], prediction)\n",
    "\n",
    "\n",
    "for index in range(40,60):\n",
    "\n",
    "    y = np.expand_dims(output2[index,:],0)\n",
    "    prediction = test_fun(y.astype(np.float32))\n",
    "    class_index = np.argmax(test[1][index,:])\n",
    "\n",
    "    height=100\n",
    "    bp_width=20\n",
    "    size = (25.,10.0)\n",
    "\n",
    "    logo = seq_logo(np.squeeze(test[0][index]), height, bp_width, norm=0, rna=1, filepath='.')\n",
    "    fig = plt.figure(figsize=size);\n",
    "    plt.imshow(logo, interpolation='none');\n",
    "    plt.axis('off');\n",
    "\n",
    "    logo = seq_logo(test[2][index], height, bp_width, norm=0, rna=1, filepath='.')\n",
    "    fig = plt.figure(figsize=size);\n",
    "    plt.imshow(logo, interpolation='none');\n",
    "    plt.axis('off');\n",
    "    plt.title(str(class_index),fontsize=20)\n",
    "\n",
    "    logo = seq_logo(np.squeeze(prediction[0]), height, bp_width, norm=0, rna=1, filepath='.')\n",
    "    fig = plt.figure(figsize=size);\n",
    "    plt.imshow(logo, interpolation='none');\n",
    "    plt.axis('off');\n",
    "    plt.title(str(np.argmax(output2[index,:]))+'; p='+str(np.max(output2[index,:])),fontsize=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "savename = 'unlocalized'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_conv_filter(layer,size):\n",
    "    W =  np.squeeze(layer.W.get_value())\n",
    "    num_filters = W.shape[0]\n",
    "\n",
    "    num_rows = int(np.ceil(np.sqrt(num_filters)))    \n",
    "    grid = mpl.gridspec.GridSpec(num_rows, num_rows)\n",
    "    grid.update(wspace=0.2, hspace=0.2, left=0.1, right=0.2, bottom=0.1, top=0.2) \n",
    "    \n",
    "    fig = plt.figure(figsize=size);\n",
    "    for i in range(num_filters):\n",
    "        MIN = np.min(W[i])\n",
    "        MAX = np.max(W[i])\n",
    "        pwm = (W[i] - MIN)#/(MAX-MIN)\n",
    "        norm = np.outer(np.ones(4), np.sum(pwm, axis=0))\n",
    "        pwm = pwm/norm\n",
    "\n",
    "        logo = seq_logo(pwm, height=100, nt_width=25, norm=0, rna=1, filepath='.')\n",
    "        plt.subplot(grid[i]);\n",
    "        plt.imshow(logo);\n",
    "        plt.axis('off');\n",
    "    return fig, plt\n",
    "%config InlineBackend.close_figures = False\n",
    "fig, plt = plot_conv_filter(network['conv1'],size=(100.,100.))\n",
    "fig.set_size_inches(100,100)\n",
    "\n",
    "outfile = savename +'_filter1.pdf'\n",
    "fig.savefig(outfile, format='pdf', dpi=1000)  \n",
    "call(['pdfcrop', outfile, outfile])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = test[0]\n",
    "input_var = nnmodel.input_var\n",
    "layer = network['conv1_active']\n",
    "\n",
    "# setup theano function to get feature map of a given layer\n",
    "num_data = len(X)\n",
    "feature_maps = theano.function([input_var], layers.get_output(layer), allow_input_downcast=True)\n",
    "map_shape = get_output_shape(layer)\n",
    "\n",
    "# get feature maps in batches for speed (large batches may be too much memory for GPU)\n",
    "num_batches = num_data // batch_size\n",
    "shape = list(map_shape)\n",
    "shape[0] = num_data\n",
    "fmaps = np.empty(tuple(shape))\n",
    "for i in range(num_batches):\n",
    "    index = range(i*batch_size, (i+1)*batch_size)    \n",
    "    fmaps[index] = feature_maps(X[index])\n",
    "\n",
    "# get the rest of the feature maps\n",
    "excess = num_data-num_batches*batch_size\n",
    "index = range(num_data-excess, num_data)    \n",
    "fmaps[index] = feature_maps(X[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_class_activation(fmaps, y, batch_size=512):\n",
    "    fmaps = np.squeeze(fmaps)\n",
    "    mean_activation = []\n",
    "    std_activation = []\n",
    "    for i in range(max(y)+1):\n",
    "        index = np.where(y == i)[0]\n",
    "        mean_activation.append(np.mean(fmaps[index], axis=0))\n",
    "        std_activation.append(np.std(fmaps[index], axis=0))\n",
    "    return np.array(mean_activation), np.array(std_activation)\n",
    "\n",
    "\n",
    "def plot_mean_activations(mean_activation, options):\n",
    "    num_labels = len(mean_activation)\n",
    "    nrows = np.ceil(np.sqrt(num_labels)).astype(int)\n",
    "    ncols = nrows\n",
    "\n",
    "    plt.figure()\n",
    "    grid = mpl.gridspec.GridSpec(nrows, ncols)\n",
    "    grid.update(wspace=0.2, hspace=0.2, left=0.1, right=0.2, bottom=0.1, top=0.2) \n",
    "    \n",
    "    for i in range(num_labels):\n",
    "        plt.subplot(grid[i])\n",
    "        plt.plot(mean_activation[i].T)\n",
    "        fig_options(plt, options)\n",
    "    return plt\n",
    "\n",
    "def fig_options(plt, options):\n",
    "    if 'figsize' in options:\n",
    "        fig = plt.gcf()\n",
    "        fig.set_size_inches(options['figsize'][0], options['figsize'][1], forward=True)\n",
    "    if 'ylim' in options:\n",
    "        plt.ylim(options['ylim'][0],options['ylim'][1])\n",
    "    if 'yticks' in options:\n",
    "        plt.yticks(options['yticks'])\n",
    "    if 'xticks' in options:\n",
    "        plt.xticks(options['xticks'])\n",
    "    if 'labelsize' in options:        \n",
    "        ax = plt.gca()\n",
    "        ax.tick_params(axis='x', labelsize=options['labelsize'])\n",
    "        ax.tick_params(axis='y', labelsize=options['labelsize'])\n",
    "    if 'axis' in options:\n",
    "        plt.axis(options['axis'])\n",
    "    if 'xlabel' in options:\n",
    "        plt.xlabel(options['xlabel'], fontsize=options['fontsize'])\n",
    "    if 'ylabel' in options:\n",
    "        plt.ylabel(options['ylabel'], fontsize=options['fontsize'])\n",
    "    if 'linewidth' in options:\n",
    "        plt.rc('axes', linewidth=options['linewidth'])\n",
    "        \n",
    "\n",
    "        \n",
    "mean_activation, std_activation = get_class_activation(fmaps, np.argmax(test[1],axis=1))\n",
    "options = { 'ylim': [0, 4],\n",
    "            'xticks': [0, 50, 100, 150],\n",
    "            'yticks': [0.5, 1.5, 2.5, 3.5], \n",
    "            'labelsize': 18,\n",
    "            'figsize': (150,100)}\n",
    "plt = plot_mean_activations(mean_activation, options)\n",
    "#plt.savefig('categorical_l1_activation.eps', format='eps', dpi=1000)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
