{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 980 (CNMeM is disabled, CuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "sys.path.append('..')\n",
    "from src import NeuralNet\n",
    "from src import train as fit\n",
    "from src import make_directory \n",
    "from models import load_model\n",
    "from data import load_data\n",
    "from six.moves import cPickle\n",
    "from subprocess import call\n",
    "\n",
    "np.random.seed(247) # for reproducibility\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'figure.autolayout': True})\n",
    "%matplotlib inline\n",
    "from scipy.misc import imresize\n",
    "\n",
    "from lasagne import layers, nonlinearities, updates, objectives, init \n",
    "from lasagne.layers import InverseLayer\n",
    "from lasagne.layers import Conv2DLayer, DenseLayer, InputLayer, ExpressionLayer, BiasLayer\n",
    "from lasagne.layers import NonlinearityLayer, MaxPool2DLayer, DropoutLayer, BatchNormLayer\n",
    "\n",
    "from lasagne.layers import get_output, get_output_shape, get_all_params\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "np.random.seed(247) # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from: /home/peter/Data/SequenceMotif/Localized_N=100000_S=200_M=40_G=20_data.pickle\n",
      "loading train data\n",
      "loading cross-validation data\n",
      "loading test data\n"
     ]
    }
   ],
   "source": [
    "filename = 'Localized_N=100000_S=200_M=40_G=20_data.pickle'\n",
    "datapath = '/home/peter/Data/SequenceMotif'\n",
    "filepath = os.path.join(datapath, filename)\n",
    "\n",
    "# load training set\n",
    "print \"loading data from: \" + filepath\n",
    "f = open(filepath, 'rb')\n",
    "print \"loading train data\"\n",
    "train = cPickle.load(f)\n",
    "print \"loading cross-validation data\"\n",
    "cross_validation = cPickle.load(f)\n",
    "print \"loading test data\"\n",
    "test = cPickle.load(f)\n",
    "f.close()\n",
    "\n",
    "X_train = train[0].transpose((0,1,2)).astype(np.float32)\n",
    "y_train = train[1].astype(np.int32)\n",
    "X_val = cross_validation[0].transpose((0,1,2)).astype(np.float32)\n",
    "y_val = cross_validation[1].astype(np.int32)\n",
    "X_test = test[0].transpose((0,1,2)).astype(np.float32)\n",
    "y_test = test[1].astype(np.int32)\n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=3)\n",
    "X_val = np.expand_dims(X_val, axis=3)\n",
    "X_test = np.expand_dims(X_test, axis=3)\n",
    "\n",
    "train = (X_train, y_train, train[2])\n",
    "valid = (X_val, y_val, cross_validation[2])\n",
    "test = (X_test, y_test, test[2])\n",
    "\n",
    "shape = (None, train[0].shape[1], train[0].shape[2], train[0].shape[3])\n",
    "num_labels = train[1].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_var = T.tensor4('inputs')\n",
    "network = {}\n",
    "\n",
    "network = {}\n",
    "network['input'] = InputLayer(tuple(shape), input_var=input_var)\n",
    "network['conv1']  = Conv2DLayer(network['input'], num_filters=64,\n",
    "                                          filter_size=(5,1),\n",
    "                                          W=init.GlorotUniform(),\n",
    "                                          b=None, \n",
    "                                          pad='same',\n",
    "                                          nonlinearity=None, flip_filters=False)\n",
    "network['conv1_bias'] = BiasLayer(network['conv1'], b=init.Constant(0.05))\n",
    "network['conv1_norm'] = BatchNormLayer(network['conv1_bias'])\n",
    "network['conv1_active'] = NonlinearityLayer(network['conv1_norm'], nonlinearity=nonlinearities.rectify)\n",
    "network['conv1_pool'] = MaxPool2DLayer(network['conv1_active'], pool_size=(2,1))\n",
    "\n",
    "network['conv2']  = Conv2DLayer(network['conv1_pool'], num_filters=128,\n",
    "                                          filter_size=(5,1),\n",
    "                                          W=init.GlorotUniform(),\n",
    "                                          b=None, \n",
    "                                          pad='same',\n",
    "                                          nonlinearity=nonlinearities.rectify, flip_filters=False)\n",
    "network['conv2_bias'] = BiasLayer(network['conv2'], b=init.Constant(0.05))\n",
    "network['conv2_norm'] = BatchNormLayer(network['conv2_bias'])\n",
    "network['conv2_active'] = NonlinearityLayer(network['conv2_norm'], nonlinearity=nonlinearities.rectify)\n",
    "network['conv2_pool'] = MaxPool2DLayer(network['conv2_active'], pool_size=(2,1))\n",
    "\n",
    "network['conv3']  = Conv2DLayer(network['conv2_pool'], num_filters=256,\n",
    "                                          filter_size=(5,1),\n",
    "                                          W=init.GlorotUniform(),\n",
    "                                          b=None, \n",
    "                                          pad='same',\n",
    "                                          nonlinearity=None, flip_filters=False)\n",
    "network['conv3_bias'] = BiasLayer(network['conv3'], b=init.Constant(0.05))\n",
    "network['conv3_norm'] = BatchNormLayer(network['conv3_bias'])\n",
    "network['conv3_active'] = NonlinearityLayer(network['conv3_norm'], nonlinearity=nonlinearities.rectify)\n",
    "network['conv3_pool'] = MaxPool2DLayer(network['conv3_active'], pool_size=(2,1))\n",
    "\n",
    "network['conv4']  = Conv2DLayer(network['conv3_pool'], num_filters=512,\n",
    "                                          filter_size=(5,1),\n",
    "                                          W=init.GlorotUniform(),\n",
    "                                          b=None, \n",
    "                                          pad='same',\n",
    "                                          nonlinearity=None, flip_filters=False)\n",
    "network['conv4_bias'] = BiasLayer(network['conv4'], b=init.Constant(0.05))\n",
    "network['conv4_norm'] = BatchNormLayer(network['conv4_bias'])\n",
    "network['conv4_active'] = NonlinearityLayer(network['conv4_norm'], nonlinearity=nonlinearities.rectify)\n",
    "network['conv4_pool'] = MaxPool2DLayer(network['conv4_active'], pool_size=(5,1))\n",
    "\n",
    "\n",
    "network['dense1'] = DenseLayer(network['conv4_pool'], num_units=512, W=init.GlorotUniform(), \n",
    "                                     b=None, nonlinearity=None)\n",
    "network['dense1_bias'] = BiasLayer(network['dense1'], b=init.Constant(0.05))\n",
    "network['dense1_norm'] = BatchNormLayer(network['dense1_bias'])\n",
    "network['dense1_active'] = NonlinearityLayer(network['dense1_norm'], nonlinearity=nonlinearities.rectify)\n",
    "network['dense1_dropout'] = DropoutLayer(network['dense1_active'], p=0.5)\n",
    "\n",
    "network['dense2'] = DenseLayer(network['dense1_dropout'], num_units=256, W=init.GlorotUniform(), \n",
    "                                     b=None, nonlinearity=None)\n",
    "network['dense2_bias'] = BiasLayer(network['dense2'], b=init.Constant(0.05))\n",
    "network['dense2_norm'] = BatchNormLayer(network['dense2_bias'])\n",
    "network['dense2_active'] = NonlinearityLayer(network['dense2_norm'], nonlinearity=nonlinearities.rectify)\n",
    "network['dense2_dropout'] = DropoutLayer(network['dense2_active'], p=0.5)\n",
    "\n",
    "\n",
    "network['dense3'] = DenseLayer(network['dense2_dropout'], num_units=20, W=init.GlorotUniform(), \n",
    "                                     b=None, nonlinearity=None)\n",
    "network['dense3_bias'] = BiasLayer(network['dense3'], b=init.Constant(0.05))\n",
    "network['dense3_active'] = NonlinearityLayer(network['dense3_bias'], nonlinearity=nonlinearities.sigmoid)\n",
    "\n",
    "network['output'] = network['dense3_active']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_var = T.dmatrix('targets')\n",
    "prediction = layers.get_output(network['output'], deterministic=False)\n",
    "\n",
    "loss = objectives.binary_crossentropy(prediction, target_var)\n",
    "loss = objectives.aggregate(loss)\n",
    "#loss += l1_penalty\n",
    "#l2_penalty = regularization.regularize_network_params(network, regularization.l2)*1e-6        \n",
    "#loss += l2_penalty \n",
    "    \n",
    "params = layers.get_all_params(network['output'], trainable=True)    \n",
    "grad = T.grad(loss, params)\n",
    "\n",
    "update_op = updates.adam(grad, params, learning_rate=0.001)\n",
    "train_fun = theano.function([input_var, target_var], [loss, prediction], updates=update_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \n",
      "  train loss = 0.073552 \n",
      "  valid loss = 0.012565 \n",
      "Epoch 2 \n",
      "  train loss = 0.008839 \n",
      "  valid loss = 0.005311 \n",
      "Epoch 3 \n",
      "  train loss = 0.004359 \n",
      "  valid loss = 0.003206 \n",
      "Epoch 4 \n",
      "  train loss = 0.003089 \n",
      "  valid loss = 0.002851 \n",
      "Epoch 5 \n",
      "  train loss = 0.002586 \n",
      "  valid loss = 0.002116 \n",
      "Epoch 6 \n",
      "  train loss = 0.002069 \n",
      "  valid loss = 0.001717 \n",
      "Epoch 7 \n",
      "  train loss = 0.001794 \n",
      "  valid loss = 0.001803 \n",
      "Epoch 8 \n",
      "  train loss = 0.001522 \n",
      "  valid loss = 0.001509 \n",
      "Epoch 9 \n",
      "  train loss = 0.001477 \n",
      "  valid loss = 0.001105 \n",
      "Epoch 10 \n",
      "  train loss = 0.001190 \n",
      "  valid loss = 0.000879 \n",
      "Epoch 11 \n",
      "  train loss = 0.000818 \n",
      "  valid loss = 0.000969 \n",
      "Epoch 12 \n",
      "  train loss = 0.001215 \n",
      "  valid loss = 0.000895 \n",
      "Epoch 13 \n",
      "  train loss = 0.000864 \n",
      "  valid loss = 0.000607 \n",
      "Epoch 14 \n",
      "  train loss = 0.000936 \n",
      "  valid loss = 0.000911 \n",
      "Epoch 15 \n",
      "  train loss = 0.000856 \n",
      "  valid loss = 0.000552 \n",
      "Epoch 16 \n",
      "  train loss = 0.000686 \n",
      "  valid loss = 0.000500 \n",
      "Epoch 17 \n",
      "  train loss = 0.000683 \n",
      "  valid loss = 0.000763 \n",
      "Epoch 18 \n",
      "  train loss = 0.000751 \n",
      "  valid loss = 0.000553 \n",
      "Epoch 19 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-6d7f64378639>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\r  train loss = %f \\n\"\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/peter/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/peter/anaconda2/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    909\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m             \u001b[1;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 911\u001b[1;33m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    912\u001b[0m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    913\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def batch_generator(X, y, batch_size=128):\n",
    "    for start_idx in range(0, len(X)-batch_size+1, batch_size):\n",
    "        excerpt = slice(start_idx, start_idx+batch_size)\n",
    "        yield X[excerpt].astype(np.float32), y[excerpt].astype(np.float32)\n",
    "\n",
    "status = True\n",
    "patience = 5\n",
    "min_loss = 1e10\n",
    "epoch = 0\n",
    "min_epoch = 0\n",
    "while status:\n",
    "    sys.stdout.write(\"\\rEpoch %d \\n\"%(epoch+1))\n",
    "\n",
    "    batch_size = 128        \n",
    "    num_batches = train[0].shape[0] // batch_size\n",
    "    train_batches = batch_generator(train[0], train[1], batch_size)\n",
    "    value = 0\n",
    "    for i in range(num_batches):\n",
    "        X,y = next(train_batches)\n",
    "        loss, prediction = train_fun(X, y)\n",
    "        value += np.mean(loss)\n",
    "    sys.stdout.write(\"\\r  train loss = %f \\n\"%(value/num_batches))\n",
    "    \n",
    "    num_batches = valid[0].shape[0] // batch_size\n",
    "    valid_batches = batch_generator(valid[0], valid[1], batch_size)\n",
    "    value = 0\n",
    "    for i in range(num_batches):\n",
    "        X,y = next(valid_batches)\n",
    "        loss, prediction = train_fun(X, y)\n",
    "        value += np.mean(loss)\n",
    "    sys.stdout.write(\"\\r  valid loss = %f \\n\"%(value/num_batches))\n",
    "    \n",
    "    if value < min_loss:\n",
    "        min_loss = value\n",
    "        min_epoch = epoch\n",
    "    else:\n",
    "        if patience - (epoch - min_epoch) == 0:\n",
    "            status = False\n",
    "            print \"Patience ran out... Early stopping.\"\n",
    "    epoch += 1\n",
    "    if epoch == 50:\n",
    "        status = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  test loss = 0.007969 \n",
      "\r",
      "  test accuracy = 0.981504 \n"
     ]
    }
   ],
   "source": [
    "prediction2 = layers.get_output(network['output'], deterministic=True)\n",
    "loss = -(target_var*T.log(prediction2) + (1.0-target_var)*T.log(1.0-prediction2))\n",
    "loss = loss.mean()\n",
    "test_fun = theano.function([input_var, target_var], [loss, prediction2])\n",
    "\n",
    "num_batches = test[0].shape[0] // batch_size\n",
    "valid_batches = batch_generator(test[0], test[1], batch_size)\n",
    "value = 0\n",
    "accuracy = 0\n",
    "for i in range(num_batches):\n",
    "    X,y = next(valid_batches)\n",
    "    loss, prediction = test_fun(X, y)\n",
    "    value += np.mean(loss)\n",
    "    accuracy += float(np.sum(np.argmax(prediction,axis=1) == np.argmax(y,axis=1)))/batch_size\n",
    "sys.stdout.write(\"\\r  test loss = %f \\n\"%(value/num_batches))\n",
    "sys.stdout.write(\"\\r  test accuracy = %f \\n\"%(accuracy/num_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_output(X,layer, input_var):\n",
    "    num_data = len(X)\n",
    "    feature_maps = theano.function([input_var], layers.get_output(layer,deterministic=True), allow_input_downcast=True)\n",
    "    map_shape = get_output_shape(layer)\n",
    "\n",
    "    # get feature maps in batches for speed (large batches may be too much memory for GPU)\n",
    "    batch_size=500\n",
    "    num_batches = num_data // batch_size\n",
    "    shape = list(map_shape)\n",
    "    shape[0] = num_data\n",
    "    output = np.empty(tuple(shape))\n",
    "    for i in range(num_batches):\n",
    "        index = range(i*batch_size, (i+1)*batch_size)    \n",
    "        output[index] = feature_maps(X[index])\n",
    "\n",
    "    # get the rest of the feature maps\n",
    "    excess = num_data-num_batches*batch_size\n",
    "    if excess:\n",
    "        index = range(num_data-excess, num_data)  \n",
    "        output[index] = feature_maps(X[index])\n",
    "    return output\n",
    "\n",
    "\n",
    "X = train[0]\n",
    "layer = network['output']\n",
    "output = get_output(X,layer, input_var)\n",
    "output2 = get_output(test[0],layer, input_var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9815333333333334"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(np.sum(np.argmax(output2,axis=1) == np.argmax(test[1],axis=1)))/test[1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shape2 = list(output.shape)\n",
    "shape2[0] = None\n",
    "input_var2 = T.dmatrix('input')\n",
    "input_var2 = T.cast(input_var2, 'float32')\n",
    "\n",
    "deconv = {}\n",
    "deconv['input'] = InputLayer(tuple(shape2), input_var=input_var2)\n",
    "num_units = list(get_output_shape(network['dense2']))[1]\n",
    "deconv['dense3'] = layers.DenseLayer(deconv['input'], num_units=num_units, W=network['dense3'].W.dimshuffle([1,0]), \n",
    "                                     b=init.Constant(0.05), nonlinearity=nonlinearities.rectify)\n",
    "deconv['dense3'].params[deconv['dense3'].W].remove('trainable')\n",
    "num_units = np.prod(list(get_output_shape(network['dense1']))[1:])\n",
    "deconv['dense2'] = layers.DenseLayer(deconv['dense3'], num_units=num_units, W=network['dense2'].W.dimshuffle([1,0]), \n",
    "                                     b=init.Constant(0.05), nonlinearity=nonlinearities.rectify)\n",
    "deconv['dense2'].params[deconv['dense2'].W].remove('trainable')\n",
    "num_units = np.prod(list(get_output_shape(network['conv4_pool']))[1:])\n",
    "deconv['dense1'] = layers.DenseLayer(deconv['dense2'], num_units=num_units, W=network['dense1'].W.dimshuffle([1,0]), \n",
    "                                     b=init.Constant(0.05), nonlinearity=nonlinearities.rectify)\n",
    "deconv['dense1'].params[deconv['dense1'].W].remove('trainable')\n",
    "shape = list(get_output_shape(network['conv4_pool']))\n",
    "shape[0] = -1\n",
    "deconv['reshape'] = layers.ReshapeLayer(deconv['dense1'], shape=tuple(shape))\n",
    "\n",
    "deconv['pool4'] = layers.Upscale2DLayer(deconv['reshape'], (5,1))\n",
    "deconv['conv4']  = Conv2DLayer(deconv['pool4'], num_filters=network['conv4'].input_shape[1],\n",
    "                                          filter_size=network['conv4'].filter_size,\n",
    "                                          W=network['conv4'].W.dimshuffle([1,0,2,3]), #W2_inv, #\n",
    "                                          b=init.Constant(0.05), \n",
    "                                          pad='same',\n",
    "                                          nonlinearity=nonlinearities.rectify, flip_filters=True)\n",
    "deconv['conv4'].params[deconv['conv4'].W].remove('trainable')\n",
    "\n",
    "\n",
    "deconv['pool3'] = layers.Upscale2DLayer(deconv['conv4'], (2,1))\n",
    "deconv['conv3']  = Conv2DLayer(deconv['pool3'], num_filters=network['conv3'].input_shape[1],\n",
    "                                          filter_size=network['conv3'].filter_size,\n",
    "                                          W=network['conv3'].W.dimshuffle([1,0,2,3]), #W2_inv, #\n",
    "                                          b=init.Constant(0.05), \n",
    "                                          pad='same',\n",
    "                                          nonlinearity=nonlinearities.rectify, flip_filters=True)\n",
    "deconv['conv3'].params[deconv['conv3'].W].remove('trainable')\n",
    "\n",
    "deconv['pool2'] = layers.Upscale2DLayer(deconv['conv3'], (2,1))\n",
    "deconv['conv2']  = Conv2DLayer(deconv['pool2'], num_filters=network['conv2'].input_shape[1],\n",
    "                                          filter_size=network['conv2'].filter_size,\n",
    "                                          W=network['conv2'].W.dimshuffle([1,0,2,3]), #W2_inv, #\n",
    "                                          b=init.Constant(0.05), \n",
    "                                          pad='same',\n",
    "                                          nonlinearity=nonlinearities.rectify, flip_filters=True)\n",
    "deconv['conv2'].params[deconv['conv2'].W].remove('trainable')\n",
    "\n",
    "deconv['pool1'] = layers.Upscale2DLayer(deconv['conv2'], (2,1))\n",
    "deconv['conv1']  = Conv2DLayer(deconv['pool1'], num_filters=network['conv1'].input_shape[1],\n",
    "                                          filter_size=network['conv1'].filter_size,\n",
    "                                          W=network['conv1'].W.dimshuffle([1,0,2,3]), #W2_inv, #\n",
    "                                          b=init.Constant(0.05), \n",
    "                                          pad='same',\n",
    "                                          nonlinearity=nonlinearities.sigmoid, flip_filters=True)\n",
    "deconv['conv1'].params[deconv['conv1'].W].remove('trainable')\n",
    "\n",
    "deconv['output'] = deconv['conv1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_var2 = T.tensor4('targets')\n",
    "prediction = layers.get_output(deconv['output'], deterministic=True)\n",
    "\n",
    "loss = (target_var2 - prediction)**2\n",
    "loss = loss.mean()\n",
    "#loss = objectives.squared_error(target_var2, prediction)\n",
    "#loss = objectives.aggregate(loss)\n",
    "\n",
    "params = layers.get_all_params(deconv['output'], trainable=True)    \n",
    "grad = T.grad(loss, params)\n",
    "\n",
    "update_op = updates.adam(grad, params, learning_rate=0.0001)\n",
    "train_fun = theano.function([input_var2, target_var2], [loss, prediction], updates=update_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \n",
      "  loss = 0.249168 \n",
      "Epoch 2 \n",
      "  loss = 0.249168 \n",
      "Epoch 3 \n",
      "  loss = 0.249168 \n",
      "Epoch 4 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-303d9378f9ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\r  loss = %f \\n\"\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/peter/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/peter/anaconda2/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    909\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m             \u001b[1;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 911\u001b[1;33m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    912\u001b[0m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    913\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def batch_generator(X, y, batch_size=128):\n",
    "    for start_idx in range(0, len(X)-batch_size+1, batch_size):\n",
    "        excerpt = slice(start_idx, start_idx+batch_size)\n",
    "        yield X[excerpt].astype(np.float32), y[excerpt].astype(np.float32)\n",
    "\n",
    "batch_size = 128        \n",
    "for epoch in range(50):\n",
    "    sys.stdout.write(\"\\rEpoch %d \\n\"%(epoch+1))\n",
    "\n",
    "    num_batches = output.shape[0] // batch_size\n",
    "    batches = batch_generator(output, train[0], batch_size)\n",
    "    value = 0\n",
    "    for i in range(num_batches):\n",
    "        X,y = next(batches)\n",
    "        loss, prediction = train_fun(X, y)\n",
    "        value += np.mean(loss)\n",
    "    sys.stdout.write(\"\\r  loss = %f \\n\"%(value/num_batches))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq_logo(pwm, height=100, nt_width=20, norm=0, rna=1, filepath='.'):\n",
    "    \"\"\"generate a sequence logo from a pwm\"\"\"\n",
    "    \n",
    "    def load_alphabet(filepath, rna):\n",
    "        \"\"\"load images of nucleotide alphabet \"\"\"\n",
    "        df = pd.read_table(os.path.join(filepath, 'A.txt'), header=None);\n",
    "        A_img = df.as_matrix()\n",
    "        A_img = np.reshape(A_img, [72, 65, 3], order=\"F\").astype(np.uint8)\n",
    "\n",
    "        df = pd.read_table(os.path.join(filepath, 'C.txt'), header=None);\n",
    "        C_img = df.as_matrix()\n",
    "        C_img = np.reshape(C_img, [76, 64, 3], order=\"F\").astype(np.uint8)\n",
    "\n",
    "        df = pd.read_table(os.path.join(filepath, 'G.txt'), header=None);\n",
    "        G_img = df.as_matrix()\n",
    "        G_img = np.reshape(G_img, [76, 67, 3], order=\"F\").astype(np.uint8)\n",
    "\n",
    "        if rna == 1:\n",
    "            df = pd.read_table(os.path.join(filepath, 'U.txt'), header=None);\n",
    "            T_img = df.as_matrix()\n",
    "            T_img = np.reshape(T_img, [74, 57, 3], order=\"F\").astype(np.uint8)\n",
    "        else:\n",
    "            df = pd.read_table(os.path.join(filepath, 'T.txt'), header=None);\n",
    "            T_img = df.as_matrix()\n",
    "            T_img = np.reshape(T_img, [72, 59, 3], order=\"F\").astype(np.uint8)\n",
    "\n",
    "        return A_img, C_img, G_img, T_img\n",
    "\n",
    "\n",
    "    def get_nt_height(pwm, height, norm):\n",
    "        \"\"\"get the heights of each nucleotide\"\"\"\n",
    "\n",
    "        def entropy(p):\n",
    "            \"\"\"calculate entropy of each nucleotide\"\"\"\n",
    "            s = 0\n",
    "            for i in range(4):\n",
    "                if p[i] > 0:\n",
    "                    s -= p[i]*np.log2(p[i])\n",
    "            return s\n",
    "\n",
    "        num_nt, num_seq = pwm.shape\n",
    "        heights = np.zeros((num_nt,num_seq));\n",
    "        for i in range(num_seq):\n",
    "            if norm == 1:\n",
    "                total_height = height\n",
    "            else:\n",
    "                total_height = (np.log2(4) - entropy(pwm[:, i]))*height;\n",
    "            heights[:,i] = np.floor(pwm[:,i]*total_height);\n",
    "        return heights.astype(int)\n",
    "\n",
    "    \n",
    "    # get the alphabet images of each nucleotide\n",
    "    A_img, C_img, G_img, T_img = load_alphabet(filepath='.', rna=1)\n",
    "    \n",
    "    \n",
    "    # get the heights of each nucleotide\n",
    "    heights = get_nt_height(pwm, height, norm)\n",
    "    \n",
    "    # resize nucleotide images for each base of sequence and stack\n",
    "    num_nt, num_seq = pwm.shape\n",
    "    width = np.ceil(nt_width*num_seq).astype(int)\n",
    "    \n",
    "    total_height = np.sum(heights,axis=0)\n",
    "    max_height = np.max(total_height)\n",
    "    logo = np.ones((height*2, width, 3)).astype(int)*255;\n",
    "    for i in range(num_seq):\n",
    "        remaining_height = total_height[i];\n",
    "        offset = max_height-remaining_height\n",
    "        nt_height = np.sort(heights[:,i]);\n",
    "        index = np.argsort(heights[:,i])\n",
    "\n",
    "        for j in range(num_nt):\n",
    "            if nt_height[j] > 0:\n",
    "                # resized dimensions of image\n",
    "                resize = (nt_height[j], nt_width)\n",
    "                if index[j] == 0:\n",
    "                    nt_img = imresize(A_img, resize)\n",
    "                elif index[j] == 1:\n",
    "                    nt_img = imresize(C_img, resize)\n",
    "                elif index[j] == 2:\n",
    "                    nt_img = imresize(G_img, resize)\n",
    "                elif index[j] == 3:\n",
    "                    nt_img = imresize(T_img, resize)\n",
    "\n",
    "                # determine location of image\n",
    "                height_range = range(remaining_height-nt_height[j], remaining_height)\n",
    "                width_range = range(i*nt_width, i*nt_width+nt_width)\n",
    "\n",
    "                # 'annoying' way to broadcast resized nucleotide image\n",
    "                if height_range:\n",
    "                    for k in range(3):\n",
    "                        for m in range(len(width_range)):\n",
    "                            logo[height_range+offset, width_range[m],k] = nt_img[:,m,k];\n",
    "\n",
    "                remaining_height -= nt_height[j]\n",
    "\n",
    "    return logo.astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = layers.get_output(deconv['output'], deterministic=False)\n",
    "test_fun = theano.function([input_var2], prediction)\n",
    "\n",
    "class_index = 17\n",
    "\n",
    "labels = np.argmax(test[1],axis=1)\n",
    "map_index = np.where(labels == class_index)[0]\n",
    "for index in map_index[:20]:\n",
    "    \n",
    "    y = np.expand_dims(output2[index,:],0)\n",
    "    prediction = test_fun(y.astype(np.float32))\n",
    "    class_index = np.argmax(test[1][index,:])\n",
    "\n",
    "    height=100\n",
    "    bp_width=20\n",
    "    size = (25.,10.0)\n",
    "\n",
    "    logo = seq_logo(np.squeeze(test[0][index]), height, bp_width, norm=0, rna=1, filepath='.')\n",
    "    fig = plt.figure(figsize=size);\n",
    "    plt.imshow(logo, interpolation='none');\n",
    "    plt.axis('off');\n",
    "\n",
    "    logo = seq_logo(test[2][index], height, bp_width, norm=0, rna=1, filepath='.')\n",
    "    fig = plt.figure(figsize=size);\n",
    "    plt.imshow(logo, interpolation='none');\n",
    "    plt.axis('off');\n",
    "    plt.title(str(class_index),fontsize=20)\n",
    "\n",
    "    logo = seq_logo(np.squeeze(prediction[0]), height, bp_width, norm=0, rna=1, filepath='.')\n",
    "    fig = plt.figure(figsize=size);\n",
    "    plt.imshow(logo, interpolation='none');\n",
    "    plt.axis('off');\n",
    "    plt.title(str(np.argmax(output2[index,:]))+'; p='+str(np.max(output2[index,:])),fontsize=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = layers.get_output(deconv['output'], deterministic=False)\n",
    "test_fun = theano.function([input_var2], prediction)\n",
    "\n",
    "\n",
    "for index in range(40,60):\n",
    "\n",
    "    y = np.expand_dims(output2[index,:],0)\n",
    "    prediction = test_fun(y.astype(np.float32))\n",
    "    class_index = np.argmax(test[1][index,:])\n",
    "\n",
    "    height=100\n",
    "    bp_width=20\n",
    "    size = (25.,10.0)\n",
    "\n",
    "    logo = seq_logo(np.squeeze(test[0][index]), height, bp_width, norm=0, rna=1, filepath='.')\n",
    "    fig = plt.figure(figsize=size);\n",
    "    plt.imshow(logo, interpolation='none');\n",
    "    plt.axis('off');\n",
    "\n",
    "    logo = seq_logo(test[2][index], height, bp_width, norm=0, rna=1, filepath='.')\n",
    "    fig = plt.figure(figsize=size);\n",
    "    plt.imshow(logo, interpolation='none');\n",
    "    plt.axis('off');\n",
    "    plt.title(str(class_index),fontsize=20)\n",
    "\n",
    "    logo = seq_logo(np.squeeze(prediction[0]), height, bp_width, norm=0, rna=1, filepath='.')\n",
    "    fig = plt.figure(figsize=size);\n",
    "    plt.imshow(logo, interpolation='none');\n",
    "    plt.axis('off');\n",
    "    plt.title(str(np.argmax(output2[index,:]))+'; p='+str(np.max(output2[index,:])),fontsize=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "savename = 'unlocalized'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_conv_filter(layer,size):\n",
    "    W =  np.squeeze(layer.W.get_value())\n",
    "    num_filters = W.shape[0]\n",
    "\n",
    "    num_rows = int(np.ceil(np.sqrt(num_filters)))    \n",
    "    grid = mpl.gridspec.GridSpec(num_rows, num_rows)\n",
    "    grid.update(wspace=0.2, hspace=0.2, left=0.1, right=0.2, bottom=0.1, top=0.2) \n",
    "    \n",
    "    fig = plt.figure(figsize=size);\n",
    "    for i in range(num_filters):\n",
    "        MIN = np.min(W[i])\n",
    "        MAX = np.max(W[i])\n",
    "        pwm = (W[i] - MIN)#/(MAX-MIN)\n",
    "        norm = np.outer(np.ones(4), np.sum(pwm, axis=0))\n",
    "        pwm = pwm/norm\n",
    "\n",
    "        logo = seq_logo(pwm, height=100, nt_width=25, norm=0, rna=1, filepath='.')\n",
    "        plt.subplot(grid[i]);\n",
    "        plt.imshow(logo);\n",
    "        plt.axis('off');\n",
    "    return fig, plt\n",
    "%config InlineBackend.close_figures = False\n",
    "fig, plt = plot_conv_filter(network['conv1'],size=(100.,100.))\n",
    "fig.set_size_inches(100,100)\n",
    "\n",
    "outfile = savename +'_filter1.pdf'\n",
    "fig.savefig(outfile, format='pdf', dpi=1000)  \n",
    "call(['pdfcrop', outfile, outfile])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = test[0]\n",
    "input_var = nnmodel.input_var\n",
    "layer = network['conv1_active']\n",
    "\n",
    "# setup theano function to get feature map of a given layer\n",
    "num_data = len(X)\n",
    "feature_maps = theano.function([input_var], layers.get_output(layer), allow_input_downcast=True)\n",
    "map_shape = get_output_shape(layer)\n",
    "\n",
    "# get feature maps in batches for speed (large batches may be too much memory for GPU)\n",
    "num_batches = num_data // batch_size\n",
    "shape = list(map_shape)\n",
    "shape[0] = num_data\n",
    "fmaps = np.empty(tuple(shape))\n",
    "for i in range(num_batches):\n",
    "    index = range(i*batch_size, (i+1)*batch_size)    \n",
    "    fmaps[index] = feature_maps(X[index])\n",
    "\n",
    "# get the rest of the feature maps\n",
    "excess = num_data-num_batches*batch_size\n",
    "index = range(num_data-excess, num_data)    \n",
    "fmaps[index] = feature_maps(X[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_class_activation(fmaps, y, batch_size=512):\n",
    "    fmaps = np.squeeze(fmaps)\n",
    "    mean_activation = []\n",
    "    std_activation = []\n",
    "    for i in range(max(y)+1):\n",
    "        index = np.where(y == i)[0]\n",
    "        mean_activation.append(np.mean(fmaps[index], axis=0))\n",
    "        std_activation.append(np.std(fmaps[index], axis=0))\n",
    "    return np.array(mean_activation), np.array(std_activation)\n",
    "\n",
    "\n",
    "def plot_mean_activations(mean_activation, options):\n",
    "    num_labels = len(mean_activation)\n",
    "    nrows = np.ceil(np.sqrt(num_labels)).astype(int)\n",
    "    ncols = nrows\n",
    "\n",
    "    plt.figure()\n",
    "    grid = mpl.gridspec.GridSpec(nrows, ncols)\n",
    "    grid.update(wspace=0.2, hspace=0.2, left=0.1, right=0.2, bottom=0.1, top=0.2) \n",
    "    \n",
    "    for i in range(num_labels):\n",
    "        plt.subplot(grid[i])\n",
    "        plt.plot(mean_activation[i].T)\n",
    "        fig_options(plt, options)\n",
    "    return plt\n",
    "\n",
    "def fig_options(plt, options):\n",
    "    if 'figsize' in options:\n",
    "        fig = plt.gcf()\n",
    "        fig.set_size_inches(options['figsize'][0], options['figsize'][1], forward=True)\n",
    "    if 'ylim' in options:\n",
    "        plt.ylim(options['ylim'][0],options['ylim'][1])\n",
    "    if 'yticks' in options:\n",
    "        plt.yticks(options['yticks'])\n",
    "    if 'xticks' in options:\n",
    "        plt.xticks(options['xticks'])\n",
    "    if 'labelsize' in options:        \n",
    "        ax = plt.gca()\n",
    "        ax.tick_params(axis='x', labelsize=options['labelsize'])\n",
    "        ax.tick_params(axis='y', labelsize=options['labelsize'])\n",
    "    if 'axis' in options:\n",
    "        plt.axis(options['axis'])\n",
    "    if 'xlabel' in options:\n",
    "        plt.xlabel(options['xlabel'], fontsize=options['fontsize'])\n",
    "    if 'ylabel' in options:\n",
    "        plt.ylabel(options['ylabel'], fontsize=options['fontsize'])\n",
    "    if 'linewidth' in options:\n",
    "        plt.rc('axes', linewidth=options['linewidth'])\n",
    "        \n",
    "\n",
    "        \n",
    "mean_activation, std_activation = get_class_activation(fmaps, np.argmax(test[1],axis=1))\n",
    "options = { 'ylim': [0, 4],\n",
    "            'xticks': [0, 50, 100, 150],\n",
    "            'yticks': [0.5, 1.5, 2.5, 3.5], \n",
    "            'labelsize': 18,\n",
    "            'figsize': (150,100)}\n",
    "plt = plot_mean_activations(mean_activation, options)\n",
    "#plt.savefig('categorical_l1_activation.eps', format='eps', dpi=1000)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
