{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating synthetic sequences with embedded regulatory grammars for RNA-binding proteins\n",
    "\n",
    "### Motivation\n",
    "\n",
    "This notebook discusses how to generate sequences with a simple set of motifs implanted in position specific locations.  The complexity of the number of motifs along a given sequence is pre-determined by the desired regulatory grammar complexity.  There are only positive grammars in each sequence, which basically means that there is no biological noise. This is an overly simplisitic data set with no biological noise and no positional noise, i.e. only sampling noise from the position weight matrices.  \n",
    "\n",
    "### Simulation overview\n",
    "\n",
    "To generate regulatory grammars, we first create a framework for the interactions of specific motifs across distinct spatial distances. We first create $M$ motifs, of which, we can generate $G$ regulatory grammars, which sample different combinations of the $M$ motifs with specific positions along the genome.  The specific positions between each motif is set by sampling an exponential distribution. Once the motif distances and combinations have been set, these constitute the set of regulatory grammars. Each regulatory grammar is associated to a given class.    \n",
    "\n",
    "We can also simulate negative results by simulating different motifs with the same distances or the same motifs with different distance or incomplete grammars.  But here, we'll just assume a perfect dataset and see how it performs.  Later, we can systematically increase the complexity to monitor the performance of this model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'figure.autolayout': True})\n",
    "import scipy.misc \n",
    "%matplotlib inline\n",
    "from scipy.misc import imresize\n",
    "from six.moves import cPickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a motif database for drosophila melanogaster\n",
    "\n",
    "The motifs comes from Ray et al. \"A compendium of RNA-binding motifs for decoding gene regulation\" (http://www.nature.com/nature/journal/v499/n7457/abs/nature12311.html). The link to the motifs is here: \n",
    "\n",
    "!wget http://hugheslab.ccbr.utoronto.ca/supplementary-data/RNAcompete_eukarya/top10align_motifs.tar.gz\n",
    "\n",
    "!tar xzvf top10align_motifs.tar.gz\n",
    "\n",
    "Here, each file is a different RBP motif as a position frequency matrix.  So, the first step is to compile all of these files into a suitable database.  We can parse each motifs (position frequency matrix) from each file in motifpath (downloaded top10align_motifs folder), create a database (list of arrays), and save as binary format (motif.pickle):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load motifs\n",
    "motiflist = 'motif.pickle'  \n",
    "if os.path.isfile(motiflist):\n",
    "\n",
    "    # load motif list from file\n",
    "    f = open(motiflist, 'rb')\n",
    "    motif_set = cPickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "else:\n",
    "    # download motifs\n",
    "    motifpath = 'top10align_motifs/'   # directory where motif files are located\n",
    "\n",
    "    # get all motif files in motifpath directory\n",
    "    listdir = os.listdir(motifpath)\n",
    "\n",
    "    # parse motifs\n",
    "    motif_set = []\n",
    "    for files in listdir:\n",
    "        df = pd.read_table(os.path.join(motifpath,files))\n",
    "        motif_set.append(df.iloc[0::,1::].transpose())\n",
    "\n",
    "    # save motifs    \n",
    "    f = open(motiflist, 'wb')\n",
    "    cPickle.dump(motif_set, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "There are a total of 244 motifs in the compendium. We can sample different sized subsets of these to create different levels of dataset complexity.  \n",
    "\n",
    "$N$: total number of sequences \n",
    "\n",
    "$M$: total number of motifs ($M \\leq 244$) \n",
    "\n",
    "$S$: size of each sequence (# nt per sequence)\n",
    "\n",
    "$w$: population fraction of each regulatory grammar \n",
    "\n",
    "For simplicity, we will use a uniform distribution for the PWFs for all 'non-motif' nucleotides, i.e. $p = 1/4$.\n",
    "\n",
    "\n",
    "Let's set up a hierarchical interaction model for the motifs, a so-called \"regulatory grammar\". For each grammar, we need to determine the number of motifs, which motifs, and their specific positions along the sequence.  Using this, we can then create a position frequency matrix of this regulatory grammar and simulate sequences the extent of which is determined by its population fraction $w$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from six.moves import cPickle\n",
    "import numpy as np\n",
    "\n",
    "def generate_grammar_model(options):\n",
    "    \"\"\"generate a regulatory grammar model: various numbers of motifs with \n",
    "    distinct separations.\"\"\"\n",
    "\n",
    "    # input options\n",
    "    motif_set = options[0]   # set of all motifs\n",
    "    num_motif = options[1]           # number of motifs for data set (to sample from motif_set)\n",
    "    num_grammar = options[2]           # number of regulatory grammars (combinations of motifs)\n",
    "    interaction_rate = options[3]  # exponential rate of number of motifs for each grammar\n",
    "    distance_scale = options[4]    # exponential rate of distance between motifs\n",
    "    distance_offset = options[5]   # offset addition between motif distances\n",
    "    max_motif = options[6]         # maximum number of motifs in a grammar\n",
    "\n",
    "    # select M random motifs from the complete list of motif_set\n",
    "    motifIndex = np.random.permutation(len(motif_set))[0:num_motif]\n",
    "\n",
    "    # build subset of motifs relevant for dataset\n",
    "    motifs = []\n",
    "    for index in motifIndex:\n",
    "        motifs.append(motif_set[index])\n",
    "    \n",
    "    # generate G regulatory grammars (combinations of motifs + distance between motifs)\n",
    "    Z = np.ceil(np.random.exponential(scale=interaction_rate, size=num_grammar)).astype(int)\n",
    "    num_interactions = np.minimum(Z, max_motif)\n",
    "    grammar = []\n",
    "    distance = []\n",
    "    for num in num_interactions:\n",
    "        index = np.random.randint(num_motif, size=num)\n",
    "        grammar.append(index)\n",
    "        separation = np.ceil(np.random.exponential(scale=distance_scale, size=num)).astype(int) + distance_offset\n",
    "        distance.append(separation)\n",
    "\n",
    "    return [motifs, grammar, distance]\n",
    "\n",
    "\n",
    "def generate_sequence_pwm(seq_length, gram, dist, motifs):\n",
    "    \"\"\"generates the position weight matrix (pwm) for a given regulatory grammar \n",
    "    with a string length S\"\"\"\n",
    "    \n",
    "    # figure out offset after centering grammar\n",
    "    offset = np.round(np.random.uniform(1,(seq_length - np.sum(dist) - len(dist)*8 - 20)))\n",
    "\n",
    "    # build position weight matrix\n",
    "    sequence_pwm = np.ones((4,offset))/4\n",
    "    for i in xrange(len(gram)):\n",
    "        sequence_pwm = np.hstack((sequence_pwm, motifs[gram[i]]))\n",
    "        if i < len(dist):\n",
    "            sequence_pwm = np.hstack((sequence_pwm, np.ones((4,dist[i]))/4))\n",
    "\n",
    "    # fill in the rest of the sequence with uniform distribution to have length seq_length\n",
    "    sequence_pwm = np.hstack((sequence_pwm, np.ones((4,seq_length - sequence_pwm.shape[1]))/4))\n",
    "    \n",
    "    return sequence_pwm\n",
    "\n",
    "\n",
    "def generate_sequence_model(seq_length, model):\n",
    "    \"\"\"generate the sequence models (PWMs) for each regulatory grammars. \"\"\"\n",
    "    \n",
    "    motifs = model[0]      # set of motifs\n",
    "    grammar = model[1]     # set of combinations of motifs\n",
    "    distance = model[2]    # set of distances between motifs\n",
    "\n",
    "    # build a PWM for each regulatory grammar\n",
    "    seq_model = []\n",
    "    for j in xrange(len(grammar)):\n",
    "        seq_model.append(generate_sequence_pwm(seq_length, grammar[j], distance[j], motifs)) \n",
    "        \n",
    "    return seq_model\n",
    "\n",
    "\n",
    "def simulate_sequence(sequence_pwm):\n",
    "    \"\"\"simulate a sequence given a sequence model\"\"\"\n",
    "    \n",
    "    nucleotide = 'ACGU'\n",
    "\n",
    "    # sequence length\n",
    "    seq_length = sequence_pwm.shape[1]\n",
    "\n",
    "    # generate uniform random number for each nucleotide in sequence\n",
    "    Z = np.random.uniform(0,1,seq_length)\n",
    "    \n",
    "    # calculate cumulative sum of the probabilities\n",
    "    cum_prob = sequence_pwm.cumsum(axis=0)\n",
    "\n",
    "    # go through sequence and find bin where random number falls in cumulative \n",
    "    # probabilities for each nucleotide\n",
    "    sequence = ''\n",
    "    for i in xrange(seq_length):\n",
    "        index=[j for j in xrange(4) if Z[i] < cum_prob[j,i]][0]\n",
    "        sequence += nucleotide[index]\n",
    "\n",
    "    return sequence\n",
    "\n",
    "\n",
    "def simulate_data(seq_model, num_seq):\n",
    "    \"\"\"simulates N sequences with random population fractions for each sequence \n",
    "    model (PWM) of each regulatory grammar \"\"\"\n",
    "\n",
    "    # simulate random population fractions and scale to N sequences\n",
    "    w = np.random.uniform(0, 1, size=len(seq_model))\n",
    "    w = np.round(w/sum(w)*num_seq)\n",
    "    popFrac = w.astype(int)\n",
    "    \n",
    "    # create a popFrac weighted number of simulation for each regulatory grammar\n",
    "    label = []\n",
    "    data = []\n",
    "    for i in xrange(len(popFrac)):\n",
    "        for j in xrange(popFrac[i]):\n",
    "            sequence = simulate_sequence(seq_model[i])\n",
    "            data.append(sequence)\n",
    "            label.append(i)\n",
    "            \n",
    "    return data, label\n",
    "\n",
    "\n",
    "def convert_one_hot(seq):\n",
    "    \"\"\"convert a sequence into a 1-hot representation\"\"\"\n",
    "    \n",
    "    nucleotide = 'ACGU'\n",
    "    N = len(seq)\n",
    "    one_hot_seq = np.zeros((4,N))\n",
    "    for i in xrange(200):         \n",
    "        #for j in range(4):\n",
    "        #    if seq[i] == nucleotide[j]:\n",
    "        #        one_hot_seq[j,i] = 1\n",
    "        index = [j for j in xrange(4) if seq[i] == nucleotide[j]]\n",
    "        one_hot_seq[index,i] = 1\n",
    "        \n",
    "    return one_hot_seq\n",
    "\n",
    "\n",
    "def subset_data(data, label, sub_index):\n",
    "    \"\"\"returns a subset of the data and labels based on sub_index\"\"\"\n",
    "    \n",
    "    num_labels = len(np.unique(label))\n",
    "    num_sub = len(sub_index)\n",
    "    \n",
    "    sub_set = np.zeros((num_sub, 4, len(data[0])))\n",
    "    sub_set_label = np.zeros((num_sub, num_labels))\n",
    "    \n",
    "    k = 0;\n",
    "    for index in sub_index:\n",
    "        sub_set[k] = convert_one_hot(data[index])\n",
    "        sub_set_label[k,label[index]] = 1\n",
    "        k += 1\n",
    "\n",
    "    sub_set_label = sub_set_label.astype(np.uint8)\n",
    "    \n",
    "    return (sub_set, sub_set_label)\n",
    "\n",
    "\n",
    "def split_data(data, label, split_size):\n",
    "    \"\"\"split data into train set, cross-validation set, and test set\"\"\"\n",
    "    \n",
    "    # number of labels\n",
    "    num_labels = len(np.unique(label))\n",
    "\n",
    "    # determine indices of each dataset\n",
    "    N = len(data)\n",
    "    cum_index = np.cumsum(np.multiply([0, split_size[0], split_size[1], split_size[2]],N)).astype(int) \n",
    "\n",
    "    # shuffle data\n",
    "    shuffle = np.random.permutation(N)\n",
    "\n",
    "    # training dataset\n",
    "    train_index = shuffle[range(cum_index[0], cum_index[1])]\n",
    "    cross_validation_index = shuffle[range(cum_index[1], cum_index[2])]\n",
    "    test_index = shuffle[range(cum_index[2], cum_index[3])]\n",
    "\n",
    "    # create subsets of data based on indices \n",
    "    train = subset_data(data, label, train_index)\n",
    "    cross_validation = subset_data(data, label, cross_validation_index)\n",
    "    test = subset_data(data, label, test_index)\n",
    "    \n",
    "    return train, cross_validation, test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating motif grammars\n",
      "Generating synthetic data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# dataset parameters\n",
    "num_seq = 10000       # number of sequences\n",
    "seq_length = 200       # length of sequence\n",
    "num_motif = 50         # number of motifs\n",
    "num_grammar =100       # number of regulatory grammars\n",
    "filename =  'N=' + str(num_seq) + \\\n",
    "            '_S=' + str(seq_length) + \\\n",
    "            '_M=' + str(num_motif) + \\\n",
    "            '_G=' + str(num_grammar) # output filename\n",
    "\n",
    "# motif interaction parameters (grammars)\n",
    "interaction_rate = 1.5       # exponential rate of number of motifs for each grammar\n",
    "distance_scale = seq_length/15        # exponential rate of distance between motifs\n",
    "offset = 5                   # offset addition between motif distances\n",
    "maxMotif = 5                 # maximum number of motifs in a grammar\n",
    "\n",
    "# percentage for each dataset\n",
    "train_size = 0.7\n",
    "cross_validation_size = 0.15\n",
    "test_size = 0.15\n",
    "\n",
    "# load motif list from file\n",
    "motiflist = 'motif.pickle'\n",
    "f = open(motiflist, 'rb')\n",
    "motif_set = cPickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# generate regulatory grammar model\n",
    "print \"Generating motif grammars\"\n",
    "options = [motif_set, num_motif, num_grammar, \n",
    "        interaction_rate, distance_scale, offset, maxMotif]\n",
    "model = generate_grammar_model(options)\n",
    "\n",
    "# convert this to a sequence position weight matrix for each model\n",
    "seq_model = generate_sequence_model(seq_length, model)\n",
    "\n",
    "# simulate N sequences based on the position weight matrices\n",
    "print \"Generating synthetic data\"\n",
    "data, label = simulate_data(seq_model, num_seq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the grammars in PWM forms using sequence logos.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq_logo(pwm, height=100, width=200, norm=0, rna=1, filepath='.'):\n",
    "    \"\"\"generate a sequence logo from a pwm\"\"\"\n",
    "    \n",
    "    def load_alphabet(filepath, rna):\n",
    "        \"\"\"load images of nucleotide alphabet \"\"\"\n",
    "        df = pd.read_table(os.path.join(filepath, 'A.txt'), header=None);\n",
    "        A_img = df.as_matrix()\n",
    "        A_img = np.reshape(A_img, [72, 65, 3], order=\"F\").astype(np.uint8)\n",
    "\n",
    "        df = pd.read_table(os.path.join(filepath, 'C.txt'), header=None);\n",
    "        C_img = df.as_matrix()\n",
    "        C_img = np.reshape(C_img, [76, 64, 3], order=\"F\").astype(np.uint8)\n",
    "\n",
    "        df = pd.read_table(os.path.join(filepath, 'G.txt'), header=None);\n",
    "        G_img = df.as_matrix()\n",
    "        G_img = np.reshape(G_img, [76, 67, 3], order=\"F\").astype(np.uint8)\n",
    "\n",
    "        if rna == 1:\n",
    "            df = pd.read_table(os.path.join(filepath, 'U.txt'), header=None);\n",
    "            T_img = df.as_matrix()\n",
    "            T_img = np.reshape(T_img, [74, 57, 3], order=\"F\").astype(np.uint8)\n",
    "        else:\n",
    "            df = pd.read_table(os.path.join(filepath, 'T.txt'), header=None);\n",
    "            T_img = df.as_matrix()\n",
    "            T_img = np.reshape(T_img, [72, 59, 3], order=\"F\").astype(np.uint8)\n",
    "\n",
    "        return A_img, C_img, G_img, T_img\n",
    "\n",
    "\n",
    "    def get_nt_height(pwm, height, norm):\n",
    "        \"\"\"get the heights of each nucleotide\"\"\"\n",
    "\n",
    "        def entropy(p):\n",
    "            \"\"\"calculate entropy of each nucleotide\"\"\"\n",
    "            s = 0\n",
    "            for i in range(4):\n",
    "                if p[i] > 0:\n",
    "                    s -= p[i]*np.log(p[i])\n",
    "            return s\n",
    "\n",
    "        num_nt, num_seq = pwm.shape\n",
    "        heights = np.zeros((num_nt,num_seq));\n",
    "        for i in range(num_seq):\n",
    "            if norm == 1:\n",
    "                total_height = height\n",
    "            else:\n",
    "                total_height = (2 - entropy(pwm[:, i]))*height/2;\n",
    "            heights[:,i] = np.floor(pwm[:,i]*total_height);\n",
    "\n",
    "        return heights.astype(int)\n",
    "\n",
    "    \n",
    "    # get the alphabet images of each nucleotide\n",
    "    A_img, C_img, G_img, T_img = load_alphabet(filepath='.', rna=1)\n",
    "    \n",
    "    \n",
    "    # get the heights of each nucleotide\n",
    "    heights = get_nt_height(pwm, height, norm)\n",
    "\n",
    "    # resize nucleotide images for each base of sequence and stack\n",
    "    num_nt, num_seq = pwm.shape\n",
    "    nt_width = np.floor(width/num_seq).astype(int)\n",
    "    logo = np.ones((height, width, 3)).astype(int)*255;\n",
    "    for i in range(num_seq):\n",
    "        remaining_height = height;\n",
    "        nt_height = np.sort(heights[:,i]);\n",
    "        index = np.argsort(heights[:,i])\n",
    "\n",
    "        for j in range(num_nt):\n",
    "            # resized dimensions of image\n",
    "            resize = (nt_height[j],nt_width)\n",
    "            if index[j] == 0:\n",
    "                nt_img = imresize(A_img, resize)\n",
    "            elif index[j] == 1:\n",
    "                nt_img = imresize(C_img, resize)\n",
    "            elif index[j] == 2:\n",
    "                nt_img = imresize(G_img, resize)\n",
    "            elif index[j] == 3:\n",
    "                nt_img = imresize(T_img, resize)\n",
    "                \n",
    "            # determine location of image\n",
    "            height_range = range(remaining_height-nt_height[j], remaining_height)\n",
    "            width_range = range(i*nt_width, i*nt_width+nt_width)\n",
    "\n",
    "            # 'annoying' way to broadcast resized nucleotide image\n",
    "            for k in range(3):\n",
    "                for m in range(len(width_range)):\n",
    "                    logo[height_range, width_range[m],k] = nt_img[:,m,k];\n",
    "\n",
    "            remaining_height -= nt_height[j]\n",
    "\n",
    "    return logo.astype(np.uint8)\n",
    "\n",
    "\n",
    "def plot_conv_filter(plt, pwm, height=200, bp_width=100, norm=0, rna=1, adjust=-1, filepath='.', showbar=0):\n",
    "    num_seq = pwm.shape[1]\n",
    "    width = bp_width*num_seq\n",
    "\n",
    "    logo = seq_logo(pwm, height, width, norm, rna, filepath)\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1)\n",
    "    axes[0].imshow(logo, extent=[bp_width*2, width+bp_width, 0, height])\n",
    "    axes[0].set_axis_off()\n",
    "    im = axes[1].imshow(pwm, cmap='jet', vmin=0, vmax=1, interpolation='none') \n",
    "    axes[1].set_axis_off()\n",
    "    fig.subplots_adjust(bottom=adjust)\n",
    "    if showbar == 1:\n",
    "        cbar_ax = fig.add_axes([.85, 0.05, 0.05, 0.45])\n",
    "        cb = fig.colorbar(im, cax=cbar_ax, ticks=[0, 0.5, 1])\n",
    "        cb.ax.tick_params(labelsize=16)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f09b4068b10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABDAAAABqCAYAAABKzqGUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNzMt6QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f09b41c9b10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pwm = seq_model[5]\n",
    "seq_length = pwm.shape[1]\n",
    "plt.figure(figsize = (4,2))\n",
    "fig = plot_conv_filter(plt, pwm, height=600, bp_width=100, norm=0, rna=1, adjust=-.9, filepath='.', showbar=0)\n",
    "fig.set_size_inches(15,4, forward=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we generated some simulated data, we should shuffle the data, then split the data into training set, cross-validation set, and test set, while converting the sequence data into one-hot representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset into train, cross-validation, and test\n",
      "Saving dataset\n",
      "Saving model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get indices for each dataset\n",
    "print \"Splitting dataset into train, cross-validation, and test\"\n",
    "split_size = [train_size, cross_validation_size, test_size]\n",
    "train, cross_validation, test = split_data(data, label, split_size)\n",
    "\n",
    "# save training dataset in one-hot representation\n",
    "print \"Saving dataset\"\n",
    "f = open(filename+'_data.pickle', 'wb')\n",
    "cPickle.dump(train, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "cPickle.dump(cross_validation, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "cPickle.dump(test, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "f.close()\n",
    "\n",
    "# save training dataset in one-hot representation\n",
    "print \"Saving model\"\n",
    "f = open(filename+'_model.pickle', 'wb')\n",
    "cPickle.dump(options, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "cPickle.dump(model, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "cPickle.dump(seq_model, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'N=10000_S=200_M=50_G=100'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
