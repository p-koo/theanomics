{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating synthetic sequences with embedded regulatory grammars for RNA-binding proteins\n",
    "\n",
    "### Motivation-t of motifs implanted along the sequence in position specific locations.  The complexity of the number of motifs along a given sequence is determined by the modeled regulatory grammar.  There are only positive grammars in each sequence, which basically means there is no biological noise. This is an overly simplisitic data set with no biological noise and no positional noise, only sampling noise from the position weight matrices.  \n",
    "\n",
    "\n",
    "To generate regulatory grammars, we need to create a framework for the interactions of specific motifs across distinct spatial distances. \n",
    "\n",
    "First, we will assume there are only P proteins. Of these P proteins, we can generate G regulatory grammars, which sample combinations of the M motifs.  We can then generate arbitrary distances, sampled from an exponential distribution, between each motif. Once the motif distances and combinations have been set, these constitute the set of regulatory grammars.  \n",
    "\n",
    "We can also simulate negative results by simulating different motifs with the same distances or the same motifs with different distance or incomplete grammars.  First, we'll just assume a perfect dataset and see how it performs.  Then, we will systematically increase the complexity to see when exactly this model fails.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'figure.autolayout': True})\n",
    "import scipy.misc \n",
    "%matplotlib inline\n",
    "from scipy.misc import imresize\n",
    "from six.moves import cPickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a motif database for drosophila melanogaster\n",
    "\n",
    "The motifs comes from Ray et al. \"A compendium of RNA-binding motifs for decoding gene regulation\" (http://www.nature.com/nature/journal/v499/n7457/abs/nature12311.html). The link to the motifs I downloaded is here: \n",
    "\n",
    "!wget http://hugheslab.ccbr.utoronto.ca/supplementary-data/RNAcompete_eukarya/top10align_motifs.tar.gz\n",
    "\n",
    "!tar xzvf top10align_motifs.tar.gz\n",
    "\n",
    "Here, each file is a different RBP motif as a position frequency matrix.  So, the first step is to compile all of these files into a suitable database.  In particular, we can parse each motifs (position frequency matrix) from each file in motifpath (downloaded top10align_motifs folder), create a database (list of arrays), and save as binary format (motif.list):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load motifs\n",
    "motiflist = 'motif.pickle'  \n",
    "if os.path.isfile(motiflist):\n",
    "\n",
    "    # load motif list from file\n",
    "    f = open(motiflist, 'rb')\n",
    "    motif_set = cPickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "else:\n",
    "    # download motifs\n",
    "    motifpath = 'top10align_motifs/'   # directory where motif files are located\n",
    "\n",
    "    # get all motif files in motifpath directory\n",
    "    listdir = os.listdir(motifpath)\n",
    "\n",
    "    # parse motifs\n",
    "    motif_set = []\n",
    "    for files in listdir:\n",
    "        df = pd.read_table(os.path.join(motifpath,files))\n",
    "        motif_set.append(df.iloc[0::,1::].transpose())\n",
    "\n",
    "    # save motifs    \n",
    "    f = open(motiflist, 'wb')\n",
    "    cPickle.dump(motif_set, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "There are a total of 244 motifs. We can sample different sized subsets of these to create different levels of dataset complexity.  \n",
    "\n",
    "$N$: total number of sequences \n",
    "\n",
    "$M$: total number of motifs ($M \\leq 244$) \n",
    "\n",
    "$S$: size of each sequence (# nt per sequence)\n",
    "\n",
    "$w$: population fraction of each regulatory grammar \n",
    "\n",
    "For simplicity, we will use a uniform distribution for the PWFs for all 'non-motif' nucleotides. \n",
    "\n",
    "\n",
    "Let's set up a hierarchical interaction model for the motifs, a so-called \"regulatory grammar\". \n",
    "So, first, we will sample a regulatory grammar.  This will give us which motifs are present and how far apart they are separated with respect to each other.  Then, we can create a postiion frequency matrix of this regulatory grammar and simulate a set number based on the population fraction $w$. \n",
    "\n",
    "Now that we generated some simulated data, we should shuffle the data, then split the data into training set, cross-validation set, and test set, while converting the sequence data into one-hot representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from six.moves import cPickle\n",
    "import numpy as np\n",
    "\n",
    "def generate_grammar_model(options):\n",
    "    \"\"\"generate a regulatory grammar model: various numbers of motifs with \n",
    "    distinct separations.\"\"\"\n",
    "\n",
    "    # input options\n",
    "    motif_set = options[0]   # set of all motifs\n",
    "    num_motif = options[1]           # number of motifs for data set (to sample from motif_set)\n",
    "    num_grammar = options[2]           # number of regulatory grammars (combinations of motifs)\n",
    "    interaction_rate = options[3]  # exponential rate of number of motifs for each grammar\n",
    "    distance_scale = options[4]    # exponential rate of distance between motifs\n",
    "    distance_offset = options[5]   # offset addition between motif distances\n",
    "    max_motif = options[6]         # maximum number of motifs in a grammar\n",
    "\n",
    "    # select M random motifs from the complete list of motif_set\n",
    "    motifIndex = np.random.permutation(len(motif_set))[0:num_motif]\n",
    "\n",
    "    # build subset of motifs relevant for dataset\n",
    "    motifs = []\n",
    "    for index in motifIndex:\n",
    "        motifs.append(motif_set[index])\n",
    "    \n",
    "    # generate G regulatory grammars (combinations of motifs + distance between motifs)\n",
    "    Z = np.ceil(np.random.exponential(scale=interaction_rate, size=num_grammar)).astype(int)\n",
    "    num_interactions = np.minimum(Z, max_motif)\n",
    "    grammar = []\n",
    "    distance = []\n",
    "    for num in num_interactions:\n",
    "        index = motifIndex[np.random.randint(num_motif, size=num)]\n",
    "        grammar.append(index)\n",
    "        separation = np.ceil(np.random.exponential(scale=distance_scale, size=num)).astype(int) + distance_offset\n",
    "        distance.append(separation)\n",
    "\n",
    "    return [motifs, grammar, distance]\n",
    "\n",
    "\n",
    "def generate_sequence_pwm(seq_length, gram, dist, motifs):\n",
    "    \"\"\"generates the position weight matrix (pwm) for a given regulatory grammar \n",
    "    with a string length S\"\"\"\n",
    "    \n",
    "    # figure out offset after centering grammar\n",
    "    offset = np.round(np.random.uniform(1,(seq_length - np.sum(dist) - len(dist)*8 - 20)))\n",
    "\n",
    "    # build position weight matrix\n",
    "    sequence_pwm = np.ones((4,offset))/4\n",
    "    for i in xrange(len(gram)):\n",
    "        sequence_pwm = np.hstack((sequence_pwm, motifs[i]))\n",
    "        if i < len(dist):\n",
    "            sequence_pwm = np.hstack((sequence_pwm, np.ones((4,dist[i]))/4))\n",
    "\n",
    "    # fill in the rest of the sequence with uniform distribution to have length seq_length\n",
    "    sequence_pwm = np.hstack((sequence_pwm, np.ones((4,seq_length - sequence_pwm.shape[1]))/4))\n",
    "    \n",
    "    return sequence_pwm\n",
    "\n",
    "\n",
    "def generate_sequence_model(seq_length, model):\n",
    "    \"\"\"generate the sequence models (PWMs) for each regulatory grammars. \"\"\"\n",
    "    \n",
    "    motifs = model[0]      # set of motifs\n",
    "    grammar = model[1]     # set of combinations of motifs\n",
    "    distance = model[2]    # set of distances between motifs\n",
    "\n",
    "    # build a PWM for each regulatory grammar\n",
    "    seq_model = []\n",
    "    for j in xrange(len(grammar)):\n",
    "        seq_model.append(generate_sequence_pwm(seq_length, grammar[j], distance[j], motifs)) \n",
    "        \n",
    "    return seq_model\n",
    "\n",
    "\n",
    "def simulate_sequence(sequence_pwm):\n",
    "    \"\"\"simulate a sequence given a sequence model\"\"\"\n",
    "    \n",
    "    nucleotide = 'ACGU'\n",
    "\n",
    "    # sequence length\n",
    "    seq_length = sequence_pwm.shape[1]\n",
    "\n",
    "    # generate uniform random number for each nucleotide in sequence\n",
    "    Z = np.random.uniform(0,1,seq_length)\n",
    "    \n",
    "    # calculate cumulative sum of the probabilities\n",
    "    cum_prob = sequence_pwm.cumsum(axis=0)\n",
    "\n",
    "    # go through sequence and find bin where random number falls in cumulative \n",
    "    # probabilities for each nucleotide\n",
    "    sequence = ''\n",
    "    for i in xrange(seq_length):\n",
    "        index=[j for j in xrange(4) if Z[i] < cum_prob[j,i]][0]\n",
    "        sequence += nucleotide[index]\n",
    "\n",
    "    return sequence\n",
    "\n",
    "\n",
    "def simulate_data(seq_model, num_seq):\n",
    "    \"\"\"simulates N sequences with random population fractions for each sequence \n",
    "    model (PWM) of each regulatory grammar \"\"\"\n",
    "\n",
    "    # simulate random population fractions and scale to N sequences\n",
    "    w = np.random.uniform(0, 1, size=len(seq_model))\n",
    "    w = np.round(w/sum(w)*num_seq)\n",
    "    popFrac = w.astype(int)\n",
    "    \n",
    "    # create a popFrac weighted number of simulation for each regulatory grammar\n",
    "    label = []\n",
    "    data = []\n",
    "    for i in xrange(len(popFrac)):\n",
    "        for j in xrange(popFrac[i]):\n",
    "            sequence = simulate_sequence(seq_model[i])\n",
    "            data.append(sequence)\n",
    "            label.append(i)\n",
    "            \n",
    "    return data, label\n",
    "\n",
    "\n",
    "def convert_one_hot(seq):\n",
    "    \"\"\"convert a sequence into a 1-hot representation\"\"\"\n",
    "    \n",
    "    nucleotide = 'ACGU'\n",
    "    N = len(seq)\n",
    "    one_hot_seq = np.zeros((4,N))\n",
    "    for i in xrange(200):         \n",
    "        #for j in range(4):\n",
    "        #    if seq[i] == nucleotide[j]:\n",
    "        #        one_hot_seq[j,i] = 1\n",
    "        index = [j for j in xrange(4) if seq[i] == nucleotide[j]]\n",
    "        one_hot_seq[index,i] = 1\n",
    "        \n",
    "    return one_hot_seq\n",
    "\n",
    "\n",
    "def subset_data(data, label, sub_index):\n",
    "    \"\"\"returns a subset of the data and labels based on sub_index\"\"\"\n",
    "    \n",
    "    num_labels = len(np.unique(label))\n",
    "    num_sub = len(sub_index)\n",
    "    \n",
    "    sub_set = np.zeros((num_sub, 4, len(data[0])))\n",
    "    sub_set_label = np.zeros((num_sub, num_labels))\n",
    "    \n",
    "    k = 0;\n",
    "    for index in sub_index:\n",
    "        sub_set[k] = convert_one_hot(data[index])\n",
    "        sub_set_label[k,label[index]] = 1\n",
    "        k += 1\n",
    "\n",
    "    sub_set_label = sub_set_label.astype(np.uint8)\n",
    "    \n",
    "    return (sub_set, sub_set_label)\n",
    "\n",
    "\n",
    "def split_data(data, label, split_size):\n",
    "    \"\"\"split data into train set, cross-validation set, and test set\"\"\"\n",
    "    \n",
    "    # number of labels\n",
    "    num_labels = len(np.unique(label))\n",
    "\n",
    "    # determine indices of each dataset\n",
    "    N = len(data)\n",
    "    cum_index = np.cumsum(np.multiply([0, split_size[0], split_size[1], split_size[2]],N)).astype(int) \n",
    "\n",
    "    # shuffle data\n",
    "    shuffle = np.random.permutation(N)\n",
    "\n",
    "    # training dataset\n",
    "    train_index = shuffle[range(cum_index[0], cum_index[1])]\n",
    "    cross_validation_index = shuffle[range(cum_index[1], cum_index[2])]\n",
    "    test_index = shuffle[range(cum_index[2], cum_index[3])]\n",
    "\n",
    "    # create subsets of data based on indices \n",
    "    train = subset_data(data, label, train_index)\n",
    "    cross_validation = subset_data(data, label, cross_validation_index)\n",
    "    test = subset_data(data, label, test_index)\n",
    "    \n",
    "    return train, cross_validation, test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# dataset parameters\n",
    "num_seq = 100000       # number of sequences\n",
    "seq_length = 200      # length of sequence\n",
    "num_motif = 10        # number of motifs\n",
    "num_grammar = 20      # number of regulatory grammars\n",
    "filename =  'N=' + str(num_seq) + \\\n",
    "            '_S=' + str(seq_length) + \\\n",
    "            '_M=' + str(num_motif) + \\\n",
    "            '_G=' + str(num_grammar) # output filename\n",
    "\n",
    "# motif interaction parameters (grammars)\n",
    "interaction_rate = 1.5       # exponential rate of number of motifs for each grammar\n",
    "distance_scale = seq_length/15        # exponential rate of distance between motifs\n",
    "offset = 5                   # offset addition between motif distances\n",
    "maxMotif = 5                 # maximum number of motifs in a grammar\n",
    "\n",
    "# percentage for each dataset\n",
    "train_size = 0.7\n",
    "cross_validation_size = 0.15\n",
    "test_size = 0.15\n",
    "\n",
    "# load motif list from file\n",
    "motiflist = 'motif.pickle'\n",
    "f = open(motiflist, 'rb')\n",
    "motif_set = cPickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# generate regulatory grammar model\n",
    "print \"Generating motif grammars\"\n",
    "options = [motif_set, num_motif, num_grammar, \n",
    "        interaction_rate, distance_scale, offset, maxMotif]\n",
    "model = generate_grammar_model(options)\n",
    "\n",
    "# convert this to a sequence position weight matrix for each model\n",
    "seq_model = generate_sequence_model(seq_length, model)\n",
    "\n",
    "# simulate N sequences based on the position weight matrices\n",
    "print \"Generating synthetic data\"\n",
    "data, label = simulate_data(seq_model, num_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def seq_logo(pwm, height=100, width=200, norm=0, rna=1, filepath='.'):\n",
    "    \"\"\"generate a sequence logo from a pwm\"\"\"\n",
    "    \n",
    "    def load_alphabet(filepath, rna):\n",
    "        \"\"\"load images of nucleotide alphabet \"\"\"\n",
    "        df = pd.read_table(os.path.join(filepath, 'A.txt'), header=None);\n",
    "        A_img = df.as_matrix()\n",
    "        A_img = np.reshape(A_img, [72, 65, 3], order=\"F\").astype(np.uint8)\n",
    "\n",
    "        df = pd.read_table(os.path.join(filepath, 'C.txt'), header=None);\n",
    "        C_img = df.as_matrix()\n",
    "        C_img = np.reshape(C_img, [76, 64, 3], order=\"F\").astype(np.uint8)\n",
    "\n",
    "        df = pd.read_table(os.path.join(filepath, 'G.txt'), header=None);\n",
    "        G_img = df.as_matrix()\n",
    "        G_img = np.reshape(G_img, [76, 67, 3], order=\"F\").astype(np.uint8)\n",
    "\n",
    "        if rna == 1:\n",
    "            df = pd.read_table(os.path.join(filepath, 'U.txt'), header=None);\n",
    "            T_img = df.as_matrix()\n",
    "            T_img = np.reshape(T_img, [74, 57, 3], order=\"F\").astype(np.uint8)\n",
    "        else:\n",
    "            df = pd.read_table(os.path.join(filepath, 'T.txt'), header=None);\n",
    "            T_img = df.as_matrix()\n",
    "            T_img = np.reshape(T_img, [72, 59, 3], order=\"F\").astype(np.uint8)\n",
    "\n",
    "        return A_img, C_img, G_img, T_img\n",
    "\n",
    "\n",
    "    def get_nt_height(pwm, height, norm):\n",
    "        \"\"\"get the heights of each nucleotide\"\"\"\n",
    "\n",
    "        def entropy(p):\n",
    "            \"\"\"calculate entropy of each nucleotide\"\"\"\n",
    "            s = 0\n",
    "            for i in range(4):\n",
    "                if p[i] > 0:\n",
    "                    s -= p[i]*np.log(p[i])\n",
    "            return s\n",
    "\n",
    "        num_nt, num_seq = pwm.shape\n",
    "        heights = np.zeros((num_nt,num_seq));\n",
    "        for i in range(num_seq):\n",
    "            if norm == 1:\n",
    "                total_height = height\n",
    "            else:\n",
    "                total_height = (2 - entropy(pwm[:, i]))*height/2;\n",
    "            heights[:,i] = np.floor(pwm[:,i]*total_height);\n",
    "\n",
    "        return heights.astype(int)\n",
    "\n",
    "    \n",
    "    # get the alphabet images of each nucleotide\n",
    "    A_img, C_img, G_img, T_img = load_alphabet(filepath='.', rna=1)\n",
    "    \n",
    "    \n",
    "    # get the heights of each nucleotide\n",
    "    heights = get_nt_height(pwm, height, norm)\n",
    "\n",
    "    # resize nucleotide images for each base of sequence and stack\n",
    "    num_nt, num_seq = pwm.shape\n",
    "    nt_width = np.floor(width/num_seq).astype(int)\n",
    "    logo = np.ones((height, width, 3)).astype(int)*255;\n",
    "    for i in range(num_seq):\n",
    "        remaining_height = height;\n",
    "        nt_height = np.sort(heights[:,i]);\n",
    "        index = np.argsort(heights[:,i])\n",
    "\n",
    "        for j in range(num_nt):\n",
    "            # resized dimensions of image\n",
    "            resize = (nt_height[j],nt_width)\n",
    "            if index[j] == 0:\n",
    "                nt_img = imresize(A_img, resize)\n",
    "            elif index[j] == 1:\n",
    "                nt_img = imresize(C_img, resize)\n",
    "            elif index[j] == 2:\n",
    "                nt_img = imresize(G_img, resize)\n",
    "            elif index[j] == 3:\n",
    "                nt_img = imresize(T_img, resize)\n",
    "                \n",
    "            # determine location of image\n",
    "            height_range = range(remaining_height-nt_height[j], remaining_height)\n",
    "            width_range = range(i*nt_width, i*nt_width+nt_width)\n",
    "\n",
    "            # 'annoying' way to broadcast resized nucleotide image\n",
    "            for k in range(3):\n",
    "                for m in range(len(width_range)):\n",
    "                    logo[height_range, width_range[m],k] = nt_img[:,m,k];\n",
    "\n",
    "            remaining_height -= nt_height[j]\n",
    "\n",
    "    return logo.astype(np.uint8)\n",
    "\n",
    "\n",
    "def plot_conv_filter(plt, pwm, height=200, bp_width=100, norm=0, rna=1, adjust=-1, filepath='.', showbar=0):\n",
    "    num_seq = pwm.shape[1]\n",
    "    width = bp_width*num_seq\n",
    "\n",
    "    logo = seq_logo(pwm, height, width, norm, rna, filepath)\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1)\n",
    "    axes[0].imshow(logo, extent=[bp_width*2, width+bp_width, 0, height])\n",
    "    axes[0].set_axis_off()\n",
    "    im = axes[1].imshow(pwm, cmap='jet', vmin=0, vmax=1, interpolation='none') \n",
    "    axes[1].set_axis_off()\n",
    "    fig.subplots_adjust(bottom=adjust)\n",
    "    if showbar == 1:\n",
    "        cbar_ax = fig.add_axes([.85, 0.05, 0.05, 0.45])\n",
    "        cb = fig.colorbar(im, cax=cbar_ax, ticks=[0, 0.5, 1])\n",
    "        cb.ax.tick_params(labelsize=16)\n",
    "    return fig\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pwm = seq_model[12]\n",
    "\n",
    "seq_length = pwm.shape[1]\n",
    "plt.figure(figsize = (seq_length/20,1))\n",
    "fig = plot_conv_filter(plt, pwm, height=600, bp_width=100, norm=0, rna=1, adjust=-.9, filepath='.', showbar=0)\n",
    "fig.set_size_inches(seq_length/20,1, forward=True)\n",
    "plt.savefig('categorical_l1_activation.eps', format='eps', dpi=1000)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pwm[:,0:12].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "height=200\n",
    "bp_width=100\n",
    "norm=0\n",
    "rna=1\n",
    "adjust=-.7\n",
    "filepath='.'\n",
    "showbar=0\n",
    "num_seq = pwm.shape[1]\n",
    "width = bp_width*num_seq\n",
    "\n",
    "logo, heights = seq_logo(pwm, height, width, norm, rna, filepath)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1)\n",
    "axes[0].imshow(logo, extent=[0, width, 0, height])\n",
    "axes[0].set_axis_off()\n",
    "im = axes[1].matshow(pwm, cmap='jet', vmin=0, vmax=1) \n",
    "axes[1].set_axis_off()\n",
    "fig.subplots_adjust(bottom=adjust)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# get indices for each dataset\n",
    "print \"Splitting dataset into train, cross-validation, and test\"\n",
    "split_size = [train_size, cross_validation_size, test_size]\n",
    "train, cross_validation, test = split_data(data, label, split_size)\n",
    "\n",
    "# save training dataset in one-hot representation\n",
    "print \"Saving dataset\"\n",
    "f = open(filename+'_data.pickle', 'wb')\n",
    "cPickle.dump(train, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "cPickle.dump(cross_validation, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "cPickle.dump(test, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "f.close()\n",
    "\n",
    "# save training dataset in one-hot representation\n",
    "print \"Saving model\"\n",
    "f = open(filename+'_model.pickle', 'wb')\n",
    "cPickle.dump(options, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "cPickle.dump(model, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "cPickle.dump(seq_model, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
