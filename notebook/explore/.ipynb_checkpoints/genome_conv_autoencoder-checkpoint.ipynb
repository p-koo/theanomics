{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named src",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6d48b8003588>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'..'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msrc\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNeuralNet\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msrc\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msrc\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmake_directory\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named src"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "sys.path.append('..')\n",
    "from src import NeuralNet\n",
    "from src import train as fit\n",
    "from src import make_directory \n",
    "from models import load_model\n",
    "from data import load_data\n",
    "from six.moves import cPickle\n",
    "np.random.seed(247) # for reproducibility\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'figure.autolayout': True})\n",
    "%matplotlib inline\n",
    "from scipy.misc import imresize\n",
    "\n",
    "from lasagne import layers, nonlinearities, updates, objectives, init \n",
    "from lasagne.layers import get_output, get_output_shape, get_all_params\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "np.random.seed(247) # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from: /home/peter/Data/SequenceMotif/N=100000_S=200_M=10_G=20_data.pickle\n",
      "loading train data\n",
      "loading cross-validation data\n",
      "loading test data\n"
     ]
    }
   ],
   "source": [
    "name = 'MotifSimulation_categorical'\n",
    "datapath = '/home/peter/Data/SequenceMotif'\n",
    "filepath = os.path.join(datapath, 'N=100000_S=200_M=30_G=20_data.pickle')\n",
    "train, valid, test = load_data(name, filepath)\n",
    "shape = (None, train[0].shape[1], train[0].shape[2], train[0].shape[3])\n",
    "num_labels = np.max(train[1])+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = \"conv_autoencoder\"\n",
    "nnmodel = NeuralNet(model_name, shape, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputname = 'simple'\n",
    "datapath = make_directory(datapath, 'cae')\n",
    "filepath = os.path.join(datapath, outputname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nnmodel = fit.train_minibatch(nnmodel, train, valid, batch_size=100, num_epochs=500, \n",
    "                        patience=3, verbose=1, filepath=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_var = T.tensor4('input')\n",
    "target_var = T.dmatrix('output')\n",
    "\n",
    "net = []\n",
    "net = layers.InputLayer(shape=shape, input_var=input_var, name='input')\n",
    "net = layers.Conv2DLayer(net, num_filters=200, filter_size=(8,1), W=init.GlorotUniform(), \n",
    "                             nonlinearity=None, b=None, pad='valid', name='conv1')\n",
    "net = layers.BatchNormLayer(net)\n",
    "net = layers.BiasLayer(net, b=init.Constant(0.05))\n",
    "net = layers.NonlinearityLayer(net, nonlinearity=nonlinearities.rectify)\n",
    "net = layers.MaxPool2DLayer(net, pool_size=(4,1))\n",
    "\n",
    "net = layers.Conv2DLayer(net, num_filters=200, filter_size=(8,1), W=init.GlorotUniform(), \n",
    "                             nonlinearity=None, b=None, pad='valid', name='conv1')\n",
    "net = layers.BatchNormLayer(net)\n",
    "net = layers.BiasLayer(net, b=init.Constant(0.05))\n",
    "net = layers.NonlinearityLayer(net, nonlinearity=nonlinearities.rectify)\n",
    "net = layers.MaxPool2DLayer(net, pool_size=(4,1))\n",
    "#net = layers.DropoutLayer(net, p=0.3)\n",
    "\n",
    "net = layers.DenseLayer(net, num_units=200, W=init.GlorotUniform(), name='inter')\n",
    "net = layers.BatchNormLayer(net)\n",
    "net = layers.BiasLayer(net, init.Constant(0.05))\n",
    "net = layers.NonlinearityLayer(net, nonlinearity=nonlinearities.sigmoid)\n",
    "#net = layers.DropoutLayer(net, p=0.5)\n",
    "\n",
    "net = layers.DenseLayer(net, num_units=20, W=init.GlorotUniform(), name='inter')\n",
    "net = layers.BiasLayer(net, init.Constant(0.05))\n",
    "net = layers.NonlinearityLayer(net, nonlinearity=nonlinearities.sigmoid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set_all_param_values(net, all_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = get_output(net)\n",
    "train_loss = objectives.binary_crossentropy(prediction, target_var)\n",
    "train_loss = train_loss.mean()\n",
    "\n",
    "valid_prediction = get_output(net, deterministic=True)\n",
    "valid_loss = objectives.binary_crossentropy(valid_prediction, target_var)\n",
    "valid_loss = valid_loss.mean()\n",
    "\n",
    "valid_acc = objectives.binary_accuracy(valid_prediction, target_var)\n",
    "valid_acc = valid_acc.mean()\n",
    "\n",
    "params = get_all_params(net, trainable=True)\n",
    "update_op = updates.adam(train_loss, params)\n",
    "\n",
    "train_fn = theano.function([input_var, target_var], train_loss, updates=update_op, allow_input_downcast=True)\n",
    "val_fn = theano.function([input_var, target_var], [valid_loss, valid_acc], allow_input_downcast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 0.640928\n",
      "valid: 0.517883\n",
      "accuracy: 0.954063\n",
      "train: 0.449153\n",
      "valid: 0.390563\n",
      "accuracy: 0.954010\n",
      "train: 0.343769\n",
      "valid: 0.305715\n",
      "accuracy: 0.957979\n",
      "train: 0.277016\n",
      "valid: 0.254072\n",
      "accuracy: 0.958330\n",
      "train: 0.234525\n",
      "valid: 0.218843\n",
      "accuracy: 0.959589\n",
      "train: 0.205561\n",
      "valid: 0.196030\n",
      "accuracy: 0.960116\n",
      "train: 0.187256\n",
      "valid: 0.182145\n",
      "accuracy: 0.959959\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-1436d2569fbd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_train_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mave_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train: %f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mave_loss\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnum_train_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/peter/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "num_train_batches = len(train[0]) // batch_size\n",
    "train_batches = batch_generator(train[0], train[1], batch_size)\n",
    "\n",
    "num_valid_batches = len(valid[0]) // batch_size\n",
    "valid_batches = batch_generator(valid[0], valid[1], batch_size)\n",
    "\n",
    "n_epochs = 10\n",
    "for e in range(n_epochs):\n",
    "    ave_loss = 0\n",
    "    for index in range(num_train_batches):\n",
    "        X_batch, y_batch = next(train_batches)\n",
    "        train_loss = train_fn(X_batch, y_batch)\n",
    "        ave_loss += train_loss\n",
    "    print(\"train: %f\" % float(ave_loss/num_train_batches))\n",
    "\n",
    "    ave_loss = 0\n",
    "    ave_acc = 0\n",
    "    for index in range(num_valid_batches):\n",
    "        X_batch, y_batch = next(valid_batches)\n",
    "        valid_loss, valid_acc = val_fn(X_batch, y_batch)\n",
    "        ave_loss += valid_loss\n",
    "        ave_acc += valid_acc\n",
    "    print(\"valid: %f\" % float(ave_loss/num_valid_batches))\n",
    "    print(\"accuracy: %f\" % float(ave_acc/num_valid_batches))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 0.603613\n",
      "valid: 0.506383\n",
      "accuracy: 0.957385\n",
      "train: 0.437341\n",
      "valid: 0.378542\n",
      "accuracy: 0.957225\n",
      "train: 0.335457\n",
      "valid: 0.299383\n",
      "accuracy: 0.957348\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-b0abe48dbf59>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_train_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[0mave_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train: %f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mave_loss\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnum_train_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/peter/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_var = T.tensor4('input')\n",
    "target_var = T.dmatrix('output')\n",
    "\n",
    "net = []\n",
    "net = layers.InputLayer(shape=shape, input_var=input_var, name='input')\n",
    "net = layers.Conv2DLayer(net, num_filters=200, filter_size=(8,1), W=init.GlorotUniform(), \n",
    "                             nonlinearity=None, b=None, pad='valid', name='conv1')\n",
    "net = layers.BatchNormLayer(net)\n",
    "net = layers.BiasLayer(net, b=init.Constant(0.05))\n",
    "net = layers.NonlinearityLayer(net, nonlinearity=nonlinearities.rectify)\n",
    "net = layers.MaxPool2DLayer(net, pool_size=(4,1))\n",
    "\n",
    "net = layers.Conv2DLayer(net, num_filters=200, filter_size=(8,1), W=init.GlorotUniform(), \n",
    "                             nonlinearity=None, b=None, pad='valid', name='conv1')\n",
    "net = layers.BatchNormLayer(net)\n",
    "net = layers.BiasLayer(net, b=init.Constant(0.05))\n",
    "net = layers.NonlinearityLayer(net, nonlinearity=nonlinearities.rectify)\n",
    "net = layers.MaxPool2DLayer(net, pool_size=(4,1))\n",
    "#net = layers.DropoutLayer(net, p=0.3)\n",
    "\n",
    "net = layers.DenseLayer(net, num_units=200, W=init.GlorotUniform(), name='inter')\n",
    "net = layers.BatchNormLayer(net)\n",
    "net = layers.BiasLayer(net, init.Constant(0.05))\n",
    "net = layers.NonlinearityLayer(net, nonlinearity=nonlinearities.sigmoid)\n",
    "#net = layers.DropoutLayer(net, p=0.5)\n",
    "\n",
    "net = layers.DenseLayer(net, num_units=20, W=init.GlorotUniform(), name='inter')\n",
    "net = layers.BiasLayer(net, init.Constant(0.05))\n",
    "net = layers.NonlinearityLayer(net, nonlinearity=nonlinearities.sigmoid)\n",
    "\n",
    "prediction = get_output(net)\n",
    "train_loss = objectives.binary_crossentropy(prediction, target_var)\n",
    "train_loss = train_loss.mean()\n",
    "\n",
    "valid_prediction = get_output(net, deterministic=True)\n",
    "valid_loss = objectives.binary_crossentropy(valid_prediction, target_var)\n",
    "valid_loss = valid_loss.mean()\n",
    "\n",
    "valid_acc = objectives.binary_accuracy(valid_prediction, target_var)\n",
    "valid_acc = valid_acc.mean()\n",
    "\n",
    "params = get_all_params(net, trainable=True)\n",
    "update_op = updates.adam(train_loss, params)\n",
    "\n",
    "train_fn = theano.function([input_var, target_var], train_loss, updates=update_op, allow_input_downcast=True)\n",
    "val_fn = theano.function([input_var, target_var], [valid_loss, valid_acc], allow_input_downcast=True)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "num_train_batches = len(train[0]) // batch_size\n",
    "train_batches = batch_generator(train[0], train[1], batch_size)\n",
    "\n",
    "num_valid_batches = len(valid[0]) // batch_size\n",
    "valid_batches = batch_generator(valid[0], valid[1], batch_size)\n",
    "\n",
    "n_epochs = 10\n",
    "for e in range(n_epochs):\n",
    "    ave_loss = 0\n",
    "    for index in range(num_train_batches):\n",
    "        X_batch, y_batch = next(train_batches)\n",
    "        train_loss = train_fn(X_batch, y_batch)\n",
    "        ave_loss += train_loss\n",
    "    print(\"train: %f\" % float(ave_loss/num_train_batches))\n",
    "\n",
    "    ave_loss = 0\n",
    "    ave_acc = 0\n",
    "    for index in range(num_valid_batches):\n",
    "        X_batch, y_batch = next(valid_batches)\n",
    "        valid_loss, valid_acc = val_fn(X_batch, y_batch)\n",
    "        ave_loss += valid_loss\n",
    "        ave_acc += valid_acc\n",
    "    print(\"valid: %f\" % float(ave_loss/num_valid_batches))\n",
    "    print(\"accuracy: %f\" % float(ave_acc/num_valid_batches))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
