{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#/bin/python\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "sys.path.append('..')\n",
    "from src import NeuralNet\n",
    "from src import train as fit\n",
    "from src import make_directory \n",
    "from models import load_model\n",
    "from data import load_data\n",
    "np.random.seed(247) # for reproducibility\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# load data\n",
    "\n",
    "name = 'MotifSimulation_categorical'\n",
    "datapath = '/home/peter/Data/SequenceMotif'\n",
    "filepath = os.path.join(datapath, 'N=100000_S=200_M=10_G=20_data.pickle')\n",
    "train, valid, test = load_data(name, filepath)\n",
    "shape = (None, train[0].shape[1], train[0].shape[2], train[0].shape[3])\n",
    "num_labels = max(train[1])+1\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "# load model parameters\n",
    "model_name = \"categorical_genome_motif_model\"\n",
    "nnmodel = NeuralNet(model_name, shape, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from six.moves import cPickle\n",
    "outputname = 'new'\n",
    "filepath = os.path.join(datapath, 'Results', outputname)\n",
    "savepath = filepath + \"_best.pickle\"\n",
    "f = open(savepath, 'rb')\n",
    "best_parameters = cPickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nnmodel.reinitialize()\n",
    "\n",
    "# load model parameters for a given training epoch\n",
    "savepath = filepath + \"_epoch_\" + str(1) + \".pickle\"\n",
    "f = open(savepath, 'rb')\n",
    "best_parameters = cPickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# get test metrics \n",
    "nnmodel.set_model_parameters(best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_performance(savepath):\n",
    "    with open(savepath, 'rb') as f:\n",
    "        name = cPickle.load(f)\n",
    "        cost = cPickle.load(f)\n",
    "        metric = cPickle.load(f)\n",
    "        metric_std = cPickle.load(f)\n",
    "        roc = cPickle.load(f)\n",
    "        pr = cPickle.load(f)\n",
    "    return cost, metric, metric_std, roc, pr\n",
    "\n",
    "savepath = filepath + \"_train_performance.pickle\"\n",
    "train_cost, train_metric, train_metric_std, train_roc, trian_pr = get_performance(savepath)\n",
    "\n",
    "savepath = filepath + \"_cross-validation_performance.pickle\"\n",
    "valid_cost, valid_metric, valid_metric_std, valid_roc, valid_pr = get_performance(savepath)\n",
    "\n",
    "savepath = filepath + \"_test_performance.pickle\"\n",
    "best_cost, best_metric, best_metric_std, best_roc, best_pr = get_performance(savepath)\n",
    "\n",
    "savepath = filepath + \"_test_all_performance.pickle\"\n",
    "test_cost, test_metric, test_metric_std, test_roc, test_pr = get_performance(savepath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def plot_loss(loss):\n",
    "    \"\"\"Plot trainig/validation/test loss during training\"\"\"\n",
    "\n",
    "    fig = plt.figure()\n",
    "    num_data_types = len(loss)\n",
    "    if num_data_types == 2:\n",
    "        plt.plot(loss[0], label='train loss', linewidth=2)\n",
    "        plt.plot(loss[1], label='valid loss', linewidth=2)\n",
    "    elif num_data_types == 3:\n",
    "        plt.plot(loss[0], label='train loss', linewidth=2)\n",
    "        plt.plot(loss[1], label='valid loss', linewidth=2)\n",
    "        plt.plot(loss[2], label='test loss', linewidth=2)\n",
    "\n",
    "    plt.xlabel('epoch', fontsize=22)\n",
    "    plt.ylabel('loss', fontsize=22)\n",
    "    plt.legend(loc='best', frameon=False, fontsize=18)\n",
    "    return plt\n",
    "\n",
    "plt = plot_loss([train_cost, valid_cost, test_cost])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_conv_weights(layer, figsize=(6, 6)):\n",
    "    \"\"\"nolearn's plot the weights of a specific layer\"\"\"\n",
    "\n",
    "    fig = plt.figure()\n",
    "    W = layer.W.get_value()\n",
    "    shape = W.shape\n",
    "    nrows = np.ceil(np.sqrt(shape[0])).astype(int)\n",
    "    ncols = nrows\n",
    "\n",
    "    for feature_map in range(shape[1]):\n",
    "        figs, axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "\n",
    "        for ax in axes.flatten():\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.axis('off')\n",
    "\n",
    "        for i, (r, c) in enumerate(product(range(nrows), range(ncols))):\n",
    "            if i >= shape[0]:\n",
    "                break\n",
    "            axes[r, c].imshow(W[i, feature_map], cmap='gray',\n",
    "                              interpolation='nearest')\n",
    "    return plt\n",
    "\n",
    "\n",
    "plot_conv_weights(layer)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nnmodel.get_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import get_all_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my = get_all_layers(nnmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "network, input_var, target_var, optimization = load_model(model_name, shape, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "network.get_output_for('conv1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from lasagne.layers import DenseLayer\n",
    "from lasagne.layers import DropoutLayer\n",
    "from lasagne.layers import InputLayer\n",
    "from lasagne.layers import LocalResponseNormalization2DLayer as NormLayer\n",
    "from lasagne.layers import NonlinearityLayer\n",
    "from lasagne.layers.dnn import Conv2DDNNLayer as ConvLayer\n",
    "from lasagne.layers import MaxPool2DLayer as PoolLayer\n",
    "from lasagne.nonlinearities import softmax, sigmoid\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "input_var = T.tensor4('inputs')\n",
    "target_var = T.dmatrix('targets')\n",
    "\n",
    "net = {}\n",
    "net['input'] = InputLayer(input_var=input_var, shape=shape)\n",
    "net['conv1'] = ConvLayer(net['input'],\n",
    "                         num_filters=200,\n",
    "                         filter_size=(12, 1),\n",
    "                         stride=(1, 1))\n",
    "net['pool1'] = PoolLayer(net['conv1'],\n",
    "                         pool_size=(4, 1),\n",
    "                         stride=(4, 1))\n",
    "net['conv2'] = ConvLayer(net['pool1'],\n",
    "                         num_filters=200,\n",
    "                         filter_size=(8, 1))\n",
    "net['pool2'] = PoolLayer(net['conv2'],\n",
    "                         pool_size=(4,1),\n",
    "                         stride=(4,1),\n",
    "                         ignore_border=False)\n",
    "net['fc4'] = DenseLayer(net['pool2'], num_units=200)\n",
    "net['drop4'] = DropoutLayer(net['fc4'], p=0.5)\n",
    "net['fc5'] = DenseLayer(net['drop4'], num_units=num_labels, nonlinearity=None)\n",
    "net['prob'] = NonlinearityLayer(net['fc5'], sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.init import Constant, Normal, Uniform, GlorotNormal\n",
    "from lasagne.init import GlorotUniform, HeNormal, HeUniform\n",
    "\n",
    "input_var = T.tensor4('inputs')\n",
    "target_var = T.ivector('targets')\n",
    "\n",
    "# create model\n",
    "layer1 = {'layer': 'input',\n",
    "          'input_var': input_var,\n",
    "          'shape': shape,\n",
    "          'name': 'input'\n",
    "         }\n",
    "layer2 = {'layer': 'convolution', \n",
    "          'num_filters': 200, \n",
    "          'filter_size': (8, 1),\n",
    "          'W': GlorotUniform(),\n",
    "          'b': None,\n",
    "          'norm': 'batch', \n",
    "          'activation': 'prelu',\n",
    "          'pool_size': (4, 1),\n",
    "          'name': 'conv1'\n",
    "          }\n",
    "layer3 = {'layer': 'convolution', \n",
    "          'num_filters': 200, \n",
    "          'filter_size': (8, 1),\n",
    "          'W': GlorotUniform(),\n",
    "          'b': None,\n",
    "          'dropout': .2,\n",
    "          'norm': 'batch', \n",
    "          'activation': 'prelu',\n",
    "          'pool_size': (4, 1),\n",
    "          'name': 'conv2'\n",
    "          }\n",
    "layer4 = {'layer': 'dense', \n",
    "          'num_units': 200, \n",
    "          'default': True,\n",
    "          'W': GlorotUniform(),\n",
    "          'b': Constant(0.05), \n",
    "          'dropout': .5,\n",
    "          'norm': 'batch',\n",
    "          'activation': 'prelu',\n",
    "          'name': 'dense3'\n",
    "          }\n",
    "layer5 = {'layer': 'dense', \n",
    "          'num_units': num_labels, \n",
    "          'default': True,\n",
    "          'W': GlorotUniform(),\n",
    "          'b': Constant(0.05),\n",
    "          'activation': 'softmax',\n",
    "          'name': 'output'\n",
    "          }\n",
    "\n",
    "model_layers = [layer1, layer2, layer3, layer4, layer5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from build_network import build_network\n",
    "network = build_network(model_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "network['output'].W.get_value().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x= layers.get_all_params(network['output'], trainable=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grad = calculate_gradient(network, cost, params, weight_norm=optimization[\"weight_norm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "network.output_shape['input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 980 (CNMeM is disabled, CuDNN 4007)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from: /home/peter/Data/SequenceMotif/N=100000_S=200_M=10_G=20_data.pickle\n",
      "loading train data\n",
      "loading cross-validation data\n",
      "loading test data\n"
     ]
    }
   ],
   "source": [
    "#/bin/python\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "sys.path.append('..')\n",
    "from src import NeuralNet\n",
    "from src import train as fit\n",
    "from src import make_directory \n",
    "from models import load_model\n",
    "from data import load_data\n",
    "np.random.seed(247) # for reproducibility\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# load data\n",
    "\n",
    "name = 'MotifSimulation_categorical'\n",
    "datapath = '/home/peter/Data/SequenceMotif'\n",
    "filepath = os.path.join(datapath, 'N=100000_S=200_M=10_G=20_data.pickle')\n",
    "train, valid, test = load_data(name, filepath)\n",
    "shape = (None, train[0].shape[1], train[0].shape[2], train[0].shape[3])\n",
    "num_labels = max(train[1])+1\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "# load model parameters\n",
    "model_name = \"categorical_genome_motif_model\"\n",
    "nnmodel = NeuralNet(model_name, shape, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne import layers, objectives, updates, regularization\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "\n",
    "def build_cost(network, target_var, prediction, optimization):\n",
    "    \"\"\" setup cost function with weight decay regularization \"\"\"\n",
    "\n",
    "    if optimization[\"objective\"] == 'categorical':\n",
    "        cost = objectives.categorical_crossentropy(prediction, target_var)\n",
    "\n",
    "    elif optimization[\"objective\"] == 'binary':\n",
    "        cost = objectives.binary_crossentropy(prediction, target_var)\n",
    "\n",
    "    elif optimization[\"objective\"] == 'mse':\n",
    "        cost = objectives.squared_error(prediction, target_var)\n",
    "\n",
    "    #cost = cost.mean()\n",
    "    cost = objectives.aggregate(cost, mode='mean')\n",
    "\n",
    "    # weight-decay regularization\n",
    "    if \"l1\" in optimization:\n",
    "        l1_penalty = regularization.regularize_network_params(network, regularization.l1) * optimization[\"l1\"]\n",
    "        cost += l1_penalty\n",
    "    if \"l2\" in optimization:\n",
    "        l2_penalty = regularization.regularize_network_params(network, regularization.l2) * optimization[\"l2\"]        \n",
    "        cost += l2_penalty \n",
    "\n",
    "    return cost\n",
    "\n",
    "\n",
    "def calculate_gradient(cost, params, weight_norm=[]):\n",
    "    \"\"\" calculate gradients with option to clip norm \"\"\"\n",
    "\n",
    "    grad = T.grad(cost, params)\n",
    "\n",
    "    # gradient clipping option\n",
    "    if weight_norm:\n",
    "        grad = updates.total_norm_constraint(grad, weight_norm)\n",
    "\n",
    "    return grad\n",
    "\n",
    "\n",
    "def build_updates(grad, params, update_params):\n",
    "    \"\"\" setup optimization algorithm \"\"\"\n",
    "\n",
    "    if update_params['optimizer'] == 'sgd':\n",
    "        update_op = updates.sgd(grad, params, learning_rate=update_params['learning_rate']) \n",
    "\n",
    "    elif update_params['optimizer'] == 'nesterov_momentum':\n",
    "        update_op = updates.nesterov_momentum(grad, params, \n",
    "                                    learning_rate=update_params['learning_rate'], \n",
    "                                    momentum=update_params['momentum'])\n",
    "\n",
    "    elif update_params['optimizer'] == 'adagrad':\n",
    "        if \"learning_rate\" in update_params:\n",
    "            update_op = updates.adagrad(grad, params, \n",
    "                              learning_rate=update_params['learning_rate'], \n",
    "                              epsilon=update_params['epsilon'])\n",
    "        else:\n",
    "            update_op = updates.adagrad(grad, params)\n",
    "\n",
    "    elif update_params['optimizer'] == 'rmsprop':\n",
    "        if \"learning_rate\" in update_params:\n",
    "            update_op = updates.rmsprop(grad, params, \n",
    "                              learning_rate=update_params['learning_rate'], \n",
    "                              rho=update_params['rho'], \n",
    "                              epsilon=update_params['epsilon'])\n",
    "        else:\n",
    "            update_op = updates.rmsprop(grad, params)\n",
    "\n",
    "    elif update_params['optimizer'] == 'adam':\n",
    "        if \"learning_rate\" in update_params:\n",
    "            update_op = updates.adam(grad, params, \n",
    "                            learning_rate=update_params['learning_rate'], \n",
    "                            beta1=update_params['beta1'], \n",
    "                            beta2=update_params['beta2'], \n",
    "                            epsilon=update['epsilon'])\n",
    "        else:\n",
    "            update_op = updates.adam(grad, params)\n",
    "\n",
    "    return update_op\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "network, input_var, target_var, optimization = load_model(model_name, shape, num_labels)\n",
    "prediction = layers.get_output(network[\"output\"], deterministic=False)\n",
    "\n",
    "\n",
    "cost = build_cost(network, target_var, prediction, optimization)\n",
    "\n",
    "\n",
    "# calculate and clip gradients\n",
    "params = layers.get_all_params(network[\"output\"], trainable=True)    \n",
    "if \"weight_norm\" in optimization:\n",
    "    grad = calculate_gradient(cost, params, weight_norm=optimization[\"weight_norm\"])\n",
    "else:\n",
    "    grad = calculate_gradient(cost, params)\n",
    "\n",
    "# setup parameter updates\n",
    "update_op = build_updates(grad, params, optimization)\n",
    "\n",
    "# test/validation set \n",
    "test_prediction = layers.get_output(network[\"output\"], deterministic=True)\n",
    "test_cost = build_cost(network, target_var, test_prediction, optimization)\n",
    "\n",
    "# create theano function\n",
    "train_fun = theano.function([input_var, target_var], [cost, prediction], updates=update_op)\n",
    "test_fun = theano.function([input_var, target_var], [test_cost, test_prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(3.0125436782836914, dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost, prediction = test_fun(test[0].astype(np.float32), test[1].astype(np.int32)) \n",
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv1': <lasagne.layers.special.ParametricRectifierLayer at 0x7f2dabc59310>,\n",
       " 'conv1_pool': <lasagne.layers.pool.MaxPool2DLayer at 0x7f2dabf82610>,\n",
       " 'conv2': <lasagne.layers.special.ParametricRectifierLayer at 0x7f2dabff6210>,\n",
       " 'conv2_dropout': <lasagne.layers.noise.DropoutLayer at 0x7f2dabff6750>,\n",
       " 'conv2_pool': <lasagne.layers.pool.MaxPool2DLayer at 0x7f2dabff65d0>,\n",
       " 'dense3': <lasagne.layers.special.ParametricRectifierLayer at 0x7f2dabff6bd0>,\n",
       " 'dense3_dropout': <lasagne.layers.noise.DropoutLayer at 0x7f2dabf82750>,\n",
       " 'input': <lasagne.layers.input.InputLayer at 0x7f2dabff6a10>,\n",
       " 'output': <lasagne.layers.special.NonlinearityLayer at 0x7f2dabfd7f10>}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
